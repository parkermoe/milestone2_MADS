{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T01:27:28.202452Z",
     "start_time": "2023-10-07T01:27:28.196296Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 500K dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with 'Y' or 'Unknown': ['AFAMPROFLS', 'APP_CHILD', 'APP_MENBIG', 'APP_TODDLR', 'APP_WOMEN', 'APP_WOMPET', 'APP_WOMPLS', 'APP_YNGMEN', 'ARTS', 'AUTOACCES', 'AUTOWORK', 'BOATING', 'BROADERLIV', 'CARDUSER', 'CATOWNER', 'CH_0002FEM', 'CH_0002MAL', 'CH_0002UNK', 'CH_0305FEM', 'CH_0305MAL', 'CH_0305UNK', 'CH_0610FEM', 'CH_0610MAL', 'CH_0610UNK', 'CH_1115FEM', 'CH_1115MAL', 'CH_1115UNK', 'CH_1617FEM', 'CH_1617MAL', 'CH_1617UNK', 'CHRISTFAM', 'COL_ANTIQ', 'COL_ARTS', 'COL_COIN', 'COL_SPORT', 'COL_STAMP', 'COMPHOMOFC', 'COMPUTERS', 'COOK_GEN', 'CURRAFFAIR', 'DEPTSTCRD', 'DIETING', 'DIYLIV', 'DOGOWNER', 'DON_ANML', 'DON_ARTCUL', 'DON_CHARIT', 'DON_CHILD', 'DON_ENVIR', 'DON_ENVWLD', 'DON_HEALTH', 'DON_INTAID', 'DON_OTHER', 'DON_POLCONS', 'DON_POLIT', 'DON_POLLIB', 'DON_RELIG', 'DON_VET', 'DONATION', 'EDU_ONLINE', 'EQUESTRIAN', 'EXER_GROUP', 'GAMING', 'GARDENER', 'GOLF', 'GRANDCHLD', 'HEALTHBEAU', 'HEATHMED', 'HH_SENIOR', 'HH_VETERAN', 'HH_YOUNGAD', 'HIGHBROW', 'HIGHENDAPP', 'HISTMIL', 'HITECHLIV', 'HOME_DECOR', 'HOMEOFFICE', 'HUNTING', 'HUNTSHOOT', 'INV_ACTIVE', 'MAIL_DONOR', 'MAILORDBUY', 'MAILORDRSP', 'MOTORCYCLE', 'NASCAR', 'PARENTING', 'PETS', 'PRESENCHLD', 'PRFL_2NDAMEND', 'PRFL_ACTIVE_MIL', 'PRFL_AMZN_PRIME', 'PRFL_ANML_RIGHTS', 'PRFL_BIDEN_SUPPORT', 'PRFL_BLM_SUPPORT', 'PRFL_BORDER_SECURITY', 'PRFL_CLINTON_SUPPORT', 'PRFL_CONSERVATIVE_NEWS', 'PRFL_EDUCATION', 'PRFL_ENVIRONMENT', 'PRFL_EVANGELICAL', 'PRFL_FENCE_SITTER', 'PRFL_GUN_CONTROL', 'PRFL_HEALTHCARE', 'PRFL_IMMIGRATION_REFORM', 'PRFL_INFLUENCER', 'PRFL_INSURANCE', 'PRFL_LABOR', 'PRFL_LIBERAL_NEWS', 'PRFL_MARIJUANA_REFORM', 'PRFL_METOO_SUPPORT', 'PRFL_MIL_SUPPORT', 'PRFL_OBAMA', 'PRFL_PERSUADABLE_VOTER', 'PRFL_SANDERS_SUPPORT', 'PRFL_TAXES', 'PRFL_TEACHERS_UNION', 'PRFL_TRUMP_SUPPORT', 'PRFL_VETERAN', 'RD_FINNEWS', 'RD_GEN', 'RD_RELIG', 'RD_SCIFI', 'RELIGINSP', 'SCISPACE', 'SCUBADIVER', 'SELFIMP', 'SINGPARENT', 'SMOKING', 'SPEC_AUTO', 'SPEC_BASE', 'SPEC_BASK', 'SPEC_FOOT', 'SPEC_HOCK', 'SPEC_SOCC', 'SPORTLEIS', 'SWEEPSTAKE', 'TELECOM', 'TENNIS', 'THEATER', 'TRAVEL', 'WORKWOMAN'] 141\n",
      "Columns with more than two categories: ['ADD_TYPE', 'AI_COUNTY_NAME', 'AIRCOND', 'ASSMLCODE', 'BUS_OWNER', 'CENSUS_ST', 'CNS_MEDINC', 'CONG_DIST', 'COUNTY_ST', 'COUNTY_TYPE', 'CRD_RANGE', 'CREDRATE', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE', 'ETHNICCONF', 'ETHNICGRP', 'FUND_POLIT', 'GENDER_MIX', 'GENERATION', 'HH_NUMGEN', 'HH_SIZE', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT', 'INCOMESTHH', 'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'NETWORTH', 'NUMCHILD', 'OCCDETAIL', 'OCCUPATION', 'PARTY_CODE', 'PERSONS_HH', 'POOL', 'PRFL_POLITICAL_IDEOLOGY', 'PRFL_TEAPARTY', 'RELIGION', 'SEX', 'ST_LO_HOUS', 'ST_UP_HOUS', 'STATE', 'STATUS', 'TOD_PRES_DIFF_2016', 'TOD_PRES_DIFF_2016_PREC', 'TOD_PRES_DIFF_2020_PREC', 'VOTER_CNT', 'VOTER_TRLR', 'VTR_GEN00', 'VTR_GEN01', 'VTR_GEN02', 'VTR_GEN03', 'VTR_GEN04', 'VTR_GEN05', 'VTR_GEN06', 'VTR_GEN07', 'VTR_GEN08', 'VTR_GEN09', 'VTR_GEN10', 'VTR_GEN11', 'VTR_GEN12', 'VTR_GEN13', 'VTR_GEN14', 'VTR_GEN15', 'VTR_GEN16', 'VTR_GEN17', 'VTR_GEN18', 'VTR_GEN19', 'VTR_GEN20', 'VTR_GEN21', 'VTR_GEN22', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PPP08', 'VTR_PPP12', 'VTR_PPP16', 'VTR_PPP20', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI10', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI14', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI18', 'VTR_PRI19', 'VTR_PRI20', 'VTR_PRI21', 'VTR_PRI22', 'YEARBUILT'] 125\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning - Column Names\n",
    "data_500.columns = data_500.columns.str.strip()\n",
    "\n",
    "# Data Cleaning - Drop ZIP\n",
    "data_500.drop('ZIP', axis=1, inplace=True)\n",
    "\n",
    "# Data Cleaning - Drop Duplicates\n",
    "data_500.drop_duplicates(inplace=True)\n",
    "\n",
    "# Data Cleaning - Object Columns\n",
    "for col in data_500.columns:\n",
    "    if data_500[col].dtype == 'object':\n",
    "        data_500[col] = data_500[col].str.strip()\n",
    "\n",
    "# Data Cleaning - Empty Strings\n",
    "data_500.replace('', 'Unknown', inplace=True)\n",
    "\n",
    "# Data Cleaning - NaN for Object Types\n",
    "data_500.loc[:, data_500.dtypes == 'object'] = data_500.loc[:, data_500.dtypes == 'object'].fillna('Unknown')\n",
    "\n",
    "# Data Cleaning - Drop Columns and Rows with All NaNs\n",
    "data_500.dropna(axis=1, how='all', inplace=True)\n",
    "data_500.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = data_500.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = data_500.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Data Cleaning - Removing Non-Numeric Columns with More Than 90% Missing Data\n",
    "missing_data_percentage = data_500.isnull().mean() * 100\n",
    "non_numeric_cols_to_remove = missing_data_percentage[non_numeric_cols]\n",
    "non_numeric_cols_to_remove = non_numeric_cols_to_remove[non_numeric_cols_to_remove > 90].index.tolist()\n",
    "data_500_reduced = data_500.drop(columns=non_numeric_cols_to_remove)\n",
    "\n",
    "# Update the list of non-numeric columns after removal\n",
    "non_numeric_cols = data_500_reduced.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Identifying Specific Types of Non-Numeric Columns\n",
    "cols_with_Y_or_Unknown = [col for col in non_numeric_cols if set(data_500_reduced[col].unique()) <= {'Y', 'Unknown'}]\n",
    "cols_with_more_than_two_categories = [col for col in non_numeric_cols if len(data_500_reduced[col].unique()) > 2]\n",
    "\n",
    "# Print identified columns\n",
    "print(\"Columns with 'Y' or 'Unknown':\", cols_with_Y_or_Unknown, len(cols_with_Y_or_Unknown))\n",
    "print(\"Columns with more than two categories:\", cols_with_more_than_two_categories, len(cols_with_more_than_two_categories))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T02:34:25.637268Z",
     "start_time": "2023-10-06T02:31:31.976200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## time decorator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest run 1...\n",
      "Starting Random Forest run 2...\n",
      "Elapsed time: 32.87 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.9841  0.979634  0.9841   \n1    2    0.9871  0.983469  0.9871   \n\n                                      Top_N_Features  \n0  [CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...  \n1  [CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.9841</td>\n      <td>0.979634</td>\n      <td>0.9841</td>\n      <td>[CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.9871</td>\n      <td>0.983469</td>\n      <td>0.9871</td>\n      <td>[CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_random_forest(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Random Forest run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Drop rows with missing 'PARTY_CODE'\n",
    "        data_copy = data_copy.dropna(subset=['PARTY_CODE'])\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42+run)\n",
    "        \n",
    "        # Encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "        \n",
    "        # Select numerical columns\n",
    "        numerical_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        # Impute missing values with column means\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_sample[numerical_cols] = imputer.fit_transform(data_sample[numerical_cols])\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample[numerical_cols]\n",
    "        y = data_sample['PARTY_CODE']\n",
    "        \n",
    "        # Train-test split without stratified sampling\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Initialize Random Forest Classifier\n",
    "        rf = RandomForestClassifier(random_state=42+run)\n",
    "        \n",
    "        # Fit the model\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Get top N features using SelectKBest with chi-squared\n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 is your dataset, you can call the function like this:\n",
    "result = run_random_forest(data_500, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T21:10:12.808746Z",
     "start_time": "2023-10-07T21:09:39.854193Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['CENSUS_TRK',\n 'RECORD_ID',\n 'VP_PPP',\n 'CNSUS_PCTB',\n 'PARTY_CODE',\n 'TOD_PRES_R_2016_PREC',\n 'TOD_PRES_D_2016_PREC',\n 'CNSUS_PCTW',\n 'VP_PRI',\n 'TOD_PRES_R_2020_PREC']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Top_N_Features'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T21:10:30.049641Z",
     "start_time": "2023-10-07T21:10:30.040741Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gradient boosting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradient Boosting run 1...\n",
      "Starting Gradient Boosting run 2...\n",
      "Elapsed time: 456.50 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.9992    0.9992  0.9992   \n1    2    0.9992    0.9992  0.9992   \n\n                                      Top_N_Features  \n0  [CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...  \n1  [CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.9992</td>\n      <td>0.9992</td>\n      <td>0.9992</td>\n      <td>[CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.9992</td>\n      <td>0.9992</td>\n      <td>0.9992</td>\n      <td>[CENSUS_TRK, RECORD_ID, VP_PPP, CNSUS_PCTB, PA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_gradient_boosting(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Gradient Boosting run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Drop rows with missing 'PARTY_CODE'\n",
    "        data_copy = data_copy.dropna(subset=['PARTY_CODE'])\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "        \n",
    "        # Check for classes with only one instance after sampling and encoding\n",
    "        class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        # Select numerical columns\n",
    "        numerical_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        # Impute missing values with column means\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_sample[numerical_cols] = imputer.fit_transform(data_sample[numerical_cols])\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample[numerical_cols]\n",
    "        y = data_sample['PARTY_CODE']\n",
    "        \n",
    "        # Train-test split\n",
    "        if min_class_count > 1:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Initialize Gradient Boosting Classifier\n",
    "        gb = GradientBoostingClassifier(random_state=42)\n",
    "        \n",
    "        # Fit the model\n",
    "        gb.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = gb.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division='warn')\n",
    "        \n",
    "        # Get top N features using SelectKBest with chi-squared\n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 is your dataset, you can call the function like this:\n",
    "result = run_gradient_boosting(data_500, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T21:31:13.647611Z",
     "start_time": "2023-10-07T21:23:37.139398Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['CENSUS_TRK',\n 'RECORD_ID',\n 'VP_PPP',\n 'CNSUS_PCTB',\n 'PARTY_CODE',\n 'TOD_PRES_R_2016_PREC',\n 'TOD_PRES_D_2016_PREC',\n 'CNSUS_PCTW',\n 'VP_PRI',\n 'TOD_PRES_R_2020_PREC']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Top_N_Features'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T21:40:47.107082Z",
     "start_time": "2023-10-07T21:40:47.094736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['CENSUS_TRK', 'RECORD_ID', 'VP_PPP', 'CNSUS_PCTB', 'PARTY_CODE']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_gradient_boosting(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Gradient Boosting run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Check class distribution\n",
    "        print(\"Class distribution before dropping NaNs: \", data_copy['PARTY_CODE'].value_counts())\n",
    "        \n",
    "        # Drop rows with missing 'PARTY_CODE'\n",
    "        data_copy = data_copy.dropna(subset=['PARTY_CODE'])\n",
    "        \n",
    "        # Check class distribution\n",
    "        print(\"Class distribution after dropping NaNs: \", data_copy['PARTY_CODE'].value_counts())\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42+run)\n",
    "        \n",
    "        # Check class distribution\n",
    "        print(\"Class distribution after sampling: \", data_sample['PARTY_CODE'].value_counts())\n",
    "        \n",
    "        # Encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample.drop('PARTY_CODE', axis=1)\n",
    "        y = data_sample['PARTY_CODE']\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42+run, stratify=y)\n",
    "        \n",
    "        # Initialize Gradient Boosting Classifier\n",
    "        gb = GradientBoostingClassifier(random_state=42+run)\n",
    "        \n",
    "        # Fit the model\n",
    "        gb.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = gb.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)  # Changed zero_division to 1\n",
    "        \n",
    "        # Get top N features using SelectKBest with chi-squared\n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 is your dataset, you can call the function like this:\n",
    "result = run_gradient_boosting(data_500, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result\n",
    "result['Top_N_Features'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T17:28:33.950273Z",
     "start_time": "2023-10-07T17:28:33.937697Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Logistic Regression run 1...\n",
      "Starting Logistic Regression run 2...\n",
      "Starting Logistic Regression run 3...\n",
      "Starting Logistic Regression run 4...\n",
      "Starting Logistic Regression run 5...\n",
      "Starting Logistic Regression run 6...\n",
      "Starting Logistic Regression run 7...\n",
      "Starting Logistic Regression run 8...\n",
      "Starting Logistic Regression run 9...\n",
      "Starting Logistic Regression run 10...\n",
      "Elapsed time: 363.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.7568    0.7438  0.7568   \n1    2    0.7568    0.7438  0.7568   \n2    3    0.7568    0.7438  0.7568   \n3    4    0.7568    0.7438  0.7568   \n4    5    0.7568    0.7438  0.7568   \n5    6    0.7568    0.7438  0.7568   \n6    7    0.7568    0.7438  0.7568   \n7    8    0.7568    0.7438  0.7568   \n8    9    0.7568    0.7438  0.7568   \n9   10    0.7568    0.7438  0.7568   \n\n                                      Top_N_Features  \n0  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n1  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n2  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n3  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n4  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n5  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n6  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n7  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n8  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  \n9  [PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0.7568</td>\n      <td>0.7438</td>\n      <td>0.7568</td>\n      <td>[PARTY_CODE, PARTY_MIX, VP_PPP, CNSUS_PCTB, TO...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_logistic_regression(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Logistic Regression run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Drop rows with missing 'PARTY_CODE'\n",
    "        data_copy = data_copy.dropna(subset=['PARTY_CODE'])\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42+run)\n",
    "        \n",
    "        # Encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "        \n",
    "        # Check for classes with only one instance after sampling and encoding\n",
    "        class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        # Select numerical columns\n",
    "        numerical_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        # Impute missing values with column means\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_sample[numerical_cols] = imputer.fit_transform(data_sample[numerical_cols])\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample[numerical_cols]\n",
    "        y = data_sample['PARTY_CODE']\n",
    "        \n",
    "        # Train-test split\n",
    "        if min_class_count > 1:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        \n",
    "        # Initialize Logistic Regression model\n",
    "        lr = LogisticRegression(C=0.1, max_iter=10000, random_state=42+run)\n",
    "    \n",
    "        # Fit the model\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = lr.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division='warn')\n",
    "        \n",
    "        # Get top N features using SelectKBest with chi-squared\n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 is your dataset, you can call the function like this:\n",
    "result = run_logistic_regression(data_500, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T01:34:10.502096Z",
     "start_time": "2023-10-07T01:28:07.039940Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.420523Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.421145Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.421370Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.422136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.422312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.422573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T01:42:30.423620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T19:19:29.875664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
