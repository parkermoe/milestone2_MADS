{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:09:29.456166Z",
     "start_time": "2023-10-22T18:09:28.973546Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 500K dataset cleaning (with riley's code)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# def clean_party_create_infer():\n",
    "#     corrected_data  = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False).sample(20000)\n",
    "#     #Replace blanks and spaces with NaN\n",
    "#     corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "#     # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "#     corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "#     # Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "#     states_to_update = ['DC', 'LA', 'RI']\n",
    "#     corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "# \n",
    "#     # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "#     corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "#     \n",
    "#     # print(\"N's after corrections\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['N'])]))\n",
    "#     # \n",
    "#     # print(\"U's after corrections\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['U'])]))\n",
    "#     # \n",
    "#     # print(\"inferred Rs:\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['S'])]))\n",
    "#     # \n",
    "#     # print(\"inferred Ds:\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['E'])]))\n",
    "#     # \n",
    "#     # print(\"new inferred Rs:\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['S'])]))\n",
    "#     # \n",
    "#     # print(\"new inferred Ds:\")\n",
    "#     # print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['E'])]))\n",
    "#           \n",
    "#     # Create party_mapping using defaultdict.Lambda sets default (all other non-D and non-R party registrations to NaN.\n",
    "#     infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "#     infer_party_mapping.update({\n",
    "#         'D': 'D',  # Registered Democrats are mapped to Democrat\n",
    "#         'E': 'D',  # E (Inferred Democrats) are mapped to Democrat\n",
    "#         'R': 'R',  # Registered Republicans are mapped to Republican \n",
    "#         'S': 'R',\n",
    "#         'L' :'R' # S (Inferred Republicans) are mapped to Republican\n",
    "#     })\n",
    "#     # Map PARTY_CODE to INFER_PARTY\n",
    "#     corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "# \n",
    "#     # Define the mapping for PARTY_CODE modifications\n",
    "#     party_code_mapping = {\n",
    "#         'E': float('nan'),\n",
    "#         'S': float('nan'),\n",
    "#         'U': float('nan'),\n",
    "#         'A': 'O',\n",
    "#         'B': 'O',\n",
    "#         'C': 'O',\n",
    "#         'F': 'O',\n",
    "#         'G': 'O',\n",
    "#         'H': 'O',\n",
    "#         'I': 'O',\n",
    "#         'J': 'O',\n",
    "#         'K': 'O',\n",
    "#       #  'L': 'L',\n",
    "#         'P': 'O',\n",
    "#         'Q': 'O',\n",
    "#         'T': 'O',\n",
    "#         'V': 'O',\n",
    "#         'W': 'O',\n",
    "#         'Y': 'O',\n",
    "#         'Z': 'O'\n",
    "#     }\n",
    "#     # Apply the mapping to the PARTY_CODE column\n",
    "#     corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "# \n",
    "#     # print(\"Final PARTY_CODE values:\")\n",
    "#     # print(corrected_data['PARTY_CODE'].unique())\n",
    "#     # print('Final INFER_PARTY values:')\n",
    "#     # print(corrected_data['INFER_PARTY'].unique())\n",
    "#     # print('INFER_PARTY Ds and Rs:')\n",
    "#     # print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "#     # print('INFER_PARTY NaNs:')\n",
    "#     # print(corrected_data['INFER_PARTY'].isna().sum())\n",
    "#     \n",
    "#     voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "#     #Sum Democrat and Republican totals\n",
    "#     corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "#     corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "# \n",
    "#     #Create conditions that evaluate whether someone has cast at least two more votes for Democrats or Republicans: \n",
    "#     conditions = [\n",
    "#         corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2,\n",
    "#         corrected_data['VTR_TOTAL_RVOTES'] - corrected_data['VTR_TOTAL_DVOTES'] >= 2\n",
    "#     ]\n",
    "#     choices = ['D', 'R']\n",
    "# \n",
    "#     corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "#     corrected_data['VTR_INFER_PARTY'].replace('nan', np.nan, inplace=True)\n",
    "# \n",
    "#     # Assign a 'D' or an 'R' to INFER_PARTY if either condition is true:\n",
    "#     corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D') , 'INFER_PARTY'] = 'D'\n",
    "#     corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R') , 'INFER_PARTY'] = 'R'\n",
    "#         \n",
    "#     # If anyone has voted Dem or Repub in a more recent election, change their INFER_PARTY label to that party\n",
    "#     columns_to_check = ['VTR_PPP16', 'VTR_GEN16', 'VTR_GEN18', 'VTR_PRI20', 'VTR_PPP20', 'VTR_GEN20', 'VTR_PRI22', 'VTR_GEN22']\n",
    "#     democrat_values = ['D', 'M', 'Z']\n",
    "#     republican_values = ['R', 'P', 'X']\n",
    "# \n",
    "#     # Loop through the columns and set the 'INFER_PARTY' value based on the mappings\n",
    "#     for column in columns_to_check:\n",
    "#         corrected_data.loc[corrected_data[column].isin(democrat_values), 'INFER_PARTY'] = 'D'\n",
    "#         corrected_data.loc[corrected_data[column].isin(republican_values), 'INFER_PARTY'] = 'R'\n",
    "#     \n",
    "#     corrected_data.drop(columns=[\"VTR_INFER_PARTY\", \"VTR_TOTAL_DVOTES\", \"VTR_TOTAL_RVOTES\"], inplace=True)\n",
    "#        \n",
    "#     # print('Final INFER_PARTY Ds and Rs:')\n",
    "#     # print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "#     # print('Final INFER_PARTY NaNs:')\n",
    "#     # print(corrected_data['INFER_PARTY'].isna().sum())\n",
    "#     \n",
    "#     return corrected_data\n",
    "# engineered_data = clean_party_create_infer()\n",
    "# print(engineered_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:55:02.968281Z",
     "start_time": "2023-10-19T18:55:02.965160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499999, 299)\n"
     ]
    }
   ],
   "source": [
    "def clean_party_create_infer():\n",
    "    corrected_data  = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False).sample(499999)\n",
    "\n",
    "    corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "    states_to_update = ['DC', 'LA', 'RI']\n",
    "    corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "    corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "    infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "    infer_party_mapping.update({'D': 'D', 'E': 'D', 'R': 'R', 'S': 'R', 'L': 'R'})\n",
    "\n",
    "    new_columns_df = pd.DataFrame()\n",
    "    new_columns_df['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "    \n",
    "    party_code_mapping = {\n",
    "        'E': float('nan'),\n",
    "        'S': float('nan'),\n",
    "        'U': float('nan'),\n",
    "        'A': 'O',\n",
    "        'B': 'O',\n",
    "        'C': 'O',\n",
    "        'F': 'O',\n",
    "        'G': 'O',\n",
    "        'H': 'O',\n",
    "        'I': 'O',\n",
    "        'J': 'O',\n",
    "        'K': 'O',\n",
    "        'P': 'O',\n",
    "        'Q': 'O',\n",
    "        'T': 'O',\n",
    "        'V': 'O',\n",
    "        'W': 'O',\n",
    "        'Y': 'O',\n",
    "        'Z': 'O'\n",
    "    }\n",
    "    \n",
    "    corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "    \n",
    "    voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "\n",
    "    new_columns_df['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "    new_columns_df['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "\n",
    "    conditions = [\n",
    "        new_columns_df['VTR_TOTAL_DVOTES'] - new_columns_df['VTR_TOTAL_RVOTES'] >= 2,\n",
    "        new_columns_df['VTR_TOTAL_RVOTES'] - new_columns_df['VTR_TOTAL_DVOTES'] >= 2\n",
    "    ]\n",
    "    choices = ['D', 'R']\n",
    "\n",
    "    new_columns_df['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "    corrected_data = pd.concat([corrected_data, new_columns_df], axis=1)\n",
    "    \n",
    "    corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D') , 'INFER_PARTY'] = 'D'\n",
    "    corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R') , 'INFER_PARTY'] = 'R'\n",
    "\n",
    "    columns_to_check = ['VTR_PPP16', 'VTR_GEN16', 'VTR_GEN18', 'VTR_PRI20', 'VTR_PPP20', 'VTR_GEN20', 'VTR_PRI22', 'VTR_GEN22']\n",
    "    democrat_values = ['D', 'M', 'Z']\n",
    "    republican_values = ['R', 'P', 'X']\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        corrected_data.loc[corrected_data[column].isin(democrat_values), 'INFER_PARTY'] = 'D'\n",
    "        corrected_data.loc[corrected_data[column].isin(republican_values), 'INFER_PARTY'] = 'R'\n",
    "    \n",
    "    corrected_data.drop(columns=[\"VTR_INFER_PARTY\", \"VTR_TOTAL_DVOTES\", \"VTR_TOTAL_RVOTES\"], inplace=True)\n",
    "    \n",
    "    return corrected_data\n",
    "\n",
    "engineered_data = clean_party_create_infer()\n",
    "print(engineered_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:16.389298Z",
     "start_time": "2023-10-19T18:55:02.971244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AFAMPROFLS', 'APP_CHILD', 'APP_MENBIG', 'APP_TODDLR', 'APP_WOMEN', 'APP_WOMPET', 'APP_WOMPLS', 'APP_YNGMEN', 'ARTS', 'AUTOACCES', 'BOATING', 'BUS_OWNER', 'CATOWNER', 'CH_0002FEM', 'CH_0002MAL', 'CH_0002UNK', 'CH_0305FEM', 'CH_0305MAL', 'CH_0305UNK', 'CH_0610FEM', 'CH_0610MAL', 'CH_0610UNK', 'CH_1115FEM', 'CH_1115MAL', 'CH_1115UNK', 'CH_1617FEM', 'CH_1617MAL', 'CH_1617UNK', 'CHRISTFAM', 'COL_ANTIQ', 'COL_ARTS', 'COL_COIN', 'COL_SPORT', 'COL_STAMP', 'COMPHOMOFC', 'CURRAFFAIR', 'DIYLIV', 'DON_ANML', 'DON_ARTCUL', 'DON_CHILD', 'DON_ENVIR', 'DON_ENVWLD', 'DON_INTAID', 'DON_OTHER', 'DON_POLCONS', 'DON_POLLIB', 'DON_RELIG', 'DON_VET', 'DONATION', 'EDU_ONLINE', 'EQUESTRIAN', 'ETHNICCONF', 'GAMING', 'GRANDCHLD', 'HH_SENIOR', 'HH_VETERAN', 'HH_YOUNGAD', 'HIGHBROW', 'HIGHENDAPP', 'HISTMIL', 'HOMEOFFICE', 'HUNTING', 'HUNTSHOOT', 'INV_ACTIVE', 'MAIL_DONOR', 'MOTORCYCLE', 'NASCAR', 'PARENTING', 'POOL', 'PRFL_ACTIVE_MIL', 'PRFL_BLM_SUPPORT', 'PRFL_CHOICELIFE', 'PRFL_CLINTON_SUPPORT', 'PRFL_EDUCATION', 'PRFL_EVANGELICAL', 'PRFL_GUN_CONTROL', 'PRFL_HEALTHCARE', 'PRFL_HEALTHCARE_REFORM', 'PRFL_INFLUENCER', 'PRFL_LABOR', 'PRFL_LGBT_SUPPORT', 'PRFL_MARIJUANA_REFORM', 'PRFL_MARRIAGE_EQUALITY', 'PRFL_MIL_SUPPORT', 'PRFL_OBAMA', 'PRFL_TEACHERS_UNION', 'PRFL_TEAPARTY', 'PRFL_VETERAN', 'RD_FINNEWS', 'RD_SCIFI', 'RELIGINSP', 'SCISPACE', 'SCUBADIVER', 'SELFIMP', 'SINGPARENT', 'SMOKING', 'SPEC_AUTO', 'SPEC_BASE', 'SPEC_BASK', 'SPEC_HOCK', 'SPEC_SOCC', 'TELECOM', 'TENNIS', 'THEATER', 'VTR_GEN01', 'VTR_GEN03', 'VTR_GEN05', 'VTR_GEN07', 'VTR_GEN09', 'VTR_GEN11', 'VTR_GEN13', 'VTR_GEN15', 'VTR_GEN17', 'VTR_GEN19', 'VTR_GEN21', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI19', 'VTR_PRI21']\n",
      "(499999, 141)\n"
     ]
    }
   ],
   "source": [
    "def additional_cleaning(data):\n",
    "    \n",
    "    # Calculate the percentage of missing data for each column\n",
    "    missing_data_percentage = data.isnull().mean() * 100\n",
    "\n",
    "    # Identify columns with more than 90% missing data\n",
    "    cols_to_remove = missing_data_percentage[missing_data_percentage > 90].index.tolist()\n",
    "    print(cols_to_remove)\n",
    "   \n",
    "    # Drop the identified columns\n",
    "    data = data.drop(columns=cols_to_remove)\n",
    "\n",
    "    def convert_with_leading_zero(value):\n",
    "        if isinstance(value, int) and 0 <= value < 10:\n",
    "            return '0' + str(value)\n",
    "        else:\n",
    "            return str(value)\n",
    "\n",
    "    data['CONG_DIST'] = data['CONG_DIST'].apply(convert_with_leading_zero)\n",
    "        \n",
    "    data['CNS_MEDINC'] = data['CNS_MEDINC'].astype('Int64')\n",
    "    data['YEARBUILT'] = data['YEARBUILT'].astype('Int64')\n",
    "    data['VOTER_CNT'] = data['VOTER_CNT'].str.strip().astype('Int64')\n",
    "    data['VOTER_TRLR'] = data['VOTER_TRLR'].astype(str)\n",
    "    \n",
    "    return data\n",
    "engineered_data = additional_cleaning(engineered_data)\n",
    "print(engineered_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:18.694561Z",
     "start_time": "2023-10-19T18:56:16.397961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499999, 94)\n"
     ]
    }
   ],
   "source": [
    "remove_columns = [\n",
    "    'PRFL_LGBT_SUPPORT',\n",
    "    'PRFL_LIBERAL_NEWS',\n",
    "    'PRFL_MARIJUANA_REFORM',\n",
    "    'PRFL_BIDEN_SUPPORT',\n",
    "    'PRFL_BORDER_SECURITY',\n",
    "    'PRFL_CONSERVATIVE_NEWS',\n",
    "    'PRFL_IMMIGRATION_REFORM',\n",
    "    'PRFL_OBAMA',\n",
    "    'PRFL_PERSUADABLE_VOTER',\n",
    "    'PRFL_POLITICAL_IDEOLOGY',\n",
    "    'PRFL_SANDERS_SUPPORT',\n",
    "    'PRFL_TRUMP_SUPPORT',\n",
    "    'ZIP',\n",
    "    \n",
    "    'VTR_GEN00', 'VTR_GEN01', 'VTR_GEN02', 'VTR_GEN03', 'VTR_GEN04', 'VTR_GEN05', 'VTR_GEN06', 'VTR_GEN07', 'VTR_GEN08', 'VTR_GEN09', 'VTR_GEN10', 'VTR_GEN11', 'VTR_GEN12', 'VTR_GEN13', 'VTR_GEN14', 'VTR_GEN15', 'VTR_GEN16', 'VTR_GEN17', 'VTR_GEN18', 'VTR_GEN19', 'VTR_GEN20', 'VTR_GEN21', 'VTR_GEN22', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PPP08', 'VTR_PPP12', 'VTR_PPP16', 'VTR_PPP20', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI10', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI14', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI18', 'VTR_PRI19', 'VTR_PRI20', 'VTR_PRI21', 'VTR_PRI22',\n",
    "    \n",
    "      'PRFL_CHOICELIFE', \"PRFL_GUN_CONTROL\",'PRFL_MARRIAGE_EQUALITY', 'PRFL_TEAPARTY',\n",
    "    'VP_PPP', \"AI_COUNTY_NAME\", 'PRFL_CLINTON_SUPPORT',\n",
    "     'CNSUS_PCTW', 'PARTY_MIX', 'PRFL_MINWAGE', 'PRFL_FENCE_SITTER', \"RECORD_ID\", 'CNSUS_PCTA',\n",
    "    'PRFL_BLM_SUPPORT', 'PRFL_METOO_SUPPORT', 'PRFL_HEALTHCARE_REFORM','DON_POLLIB', 'PRFL_2NDAMEND', 'FUND_POLIT', 'DON_POLCONS', \n",
    "    \n",
    "   'PARTY_CODE','ST_LO_HOUS', 'ST_UP_HOUS', 'CENSUS_ST', 'COUNTY_ST',\n",
    "    #'AGE'\n",
    "]\n",
    "# Drop the list of columns from the dataset\n",
    "engineered_data.drop(columns=remove_columns, errors='ignore', inplace=True)\n",
    "print(engineered_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:19.004588Z",
     "start_time": "2023-10-19T18:56:18.712587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Looking at what we have done so far"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ADD_TYPE', 'AGE', 'AIRCOND', 'ASSMLCODE', 'AUTOWORK', 'BROADERLIV',\n",
      "       'CARDUSER', 'CENSUS_TRK', 'CNS_MEDINC', 'CNSUS_PCTB', 'CNSUS_PCTH',\n",
      "       'CNSUS_PCTI', 'CNSUS_PCTM', 'CNSUS_PCTO', 'CNSUS_PCTP', 'COMPUTERS',\n",
      "       'CONG_DIST', 'COOK_GEN', 'COUNTY_TYPE', 'CRD_RANGE', 'CREDRATE',\n",
      "       'DEPTSTCRD', 'DIETING', 'DOGOWNER', 'DON_CHARIT', 'DON_HEALTH',\n",
      "       'DON_POLIT', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE', 'ETHNICGRP',\n",
      "       'EXER_GROUP', 'GARDENER', 'GENDER_MIX', 'GENERATION', 'GOLF',\n",
      "       'HEALTHBEAU', 'HEATHMED', 'HH_NUMGEN', 'HH_SIZE', 'HITECHLIV',\n",
      "       'HOME_DECOR', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT', 'INCOMESTHH',\n",
      "       'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'MAILORDBUY',\n",
      "       'MAILORDRSP', 'NETWORTH', 'NUMCHILD', 'OCCDETAIL', 'OCCUPATION',\n",
      "       'PERSONS_HH', 'PETS', 'PRESENCHLD', 'PRFL_AMZN_PRIME',\n",
      "       'PRFL_ANML_RIGHTS', 'PRFL_ENVIRONMENT', 'PRFL_INSURANCE', 'PRFL_TAXES',\n",
      "       'RD_GEN', 'RD_RELIG', 'RELIGION', 'SEX', 'SPEC_FOOT', 'SPORTLEIS',\n",
      "       'STATE', 'STATUS', 'SWEEPSTAKE', 'TOD_PRES_D_2016',\n",
      "       'TOD_PRES_D_2016_PREC', 'TOD_PRES_D_2020_PREC', 'TOD_PRES_DIFF_2016',\n",
      "       'TOD_PRES_DIFF_2016_PREC', 'TOD_PRES_DIFF_2020_PREC', 'TOD_PRES_O_2016',\n",
      "       'TOD_PRES_O_2016_PREC', 'TOD_PRES_O_2020_PREC', 'TOD_PRES_R_2016',\n",
      "       'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'TRAIL_CNT', 'TRAVEL',\n",
      "       'VOTER_CNT', 'VOTER_TRLR', 'VP_GEN', 'VP_OTH', 'VP_PRI', 'WORKWOMAN',\n",
      "       'YEARBUILT', 'INFER_PARTY'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(engineered_data.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:19.005273Z",
     "start_time": "2023-10-19T18:56:19.002230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "       ADD_TYPE   AGE AIRCOND ASSMLCODE AUTOWORK BROADERLIV CARDUSER  \\\n380550        S  74.0     NaN       NaN      NaN        NaN        Y   \n177284      NaN   NaN     NaN       NaN      NaN        NaN      NaN   \n149189        S  36.0       F       NaN        Y          Y        Y   \n374160        H  55.0     NaN       NaN      NaN        NaN      NaN   \n471852        S  55.0     NaN       NaN      NaN          Y        Y   \n498584        S  38.0     NaN       NaN      NaN        NaN      NaN   \n446111        S  40.0     NaN       NaN      NaN        NaN      NaN   \n388712        S  64.0       A       NaN      NaN        NaN        Y   \n377215        S  58.0     NaN       NaN      NaN        NaN      NaN   \n473583        S  63.0     NaN       NaN        Y          Y        Y   \n\n        CENSUS_TRK  CNS_MEDINC  CNSUS_PCTB  ...  TRAIL_CNT  TRAVEL  VOTER_CNT  \\\n380550      2103.0         723    1.212121  ...          1     NaN          2   \n177284         NaN        <NA>         NaN  ...          1     NaN          6   \n149189    617400.0        <NA>    4.285714  ...          1     NaN          6   \n374160       600.0        <NA>   69.465649  ...          1     NaN          4   \n471852    202304.0         543    4.081633  ...          7       Y         32   \n498584     22005.0        <NA>   39.449541  ...          1     NaN          2   \n446111     20104.0        <NA>    0.980392  ...          3     NaN          1   \n388712     50405.0        <NA>    1.379310  ...          2     NaN          7   \n377215     10803.0         436    1.587302  ...          1     NaN         15   \n473583      1104.0         866    0.000000  ...          2       Y          8   \n\n        VOTER_TRLR  VP_GEN VP_OTH VP_PRI WORKWOMAN YEARBUILT INFER_PARTY  \n380550           1   33.33    0.0  33.33         Y      2014           R  \n177284           0   40.00    0.0   0.00       NaN      <NA>           R  \n149189           0   40.00    0.0  10.00         Y      2007         NaN  \n374160           0   20.00    0.0  10.00       NaN      <NA>           R  \n471852           7   50.00   80.0  50.00       NaN      1910           R  \n498584           0   12.50    0.0   0.00       NaN      <NA>           D  \n446111           2   14.29    0.0   0.00       NaN      <NA>           R  \n388712           1   40.00    0.0  10.00       NaN      2002           D  \n377215           0   50.00    0.0   0.00       NaN      1955           D  \n473583           1   20.00    0.0   0.00         Y      <NA>           R  \n\n[10 rows x 94 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ADD_TYPE</th>\n      <th>AGE</th>\n      <th>AIRCOND</th>\n      <th>ASSMLCODE</th>\n      <th>AUTOWORK</th>\n      <th>BROADERLIV</th>\n      <th>CARDUSER</th>\n      <th>CENSUS_TRK</th>\n      <th>CNS_MEDINC</th>\n      <th>CNSUS_PCTB</th>\n      <th>...</th>\n      <th>TRAIL_CNT</th>\n      <th>TRAVEL</th>\n      <th>VOTER_CNT</th>\n      <th>VOTER_TRLR</th>\n      <th>VP_GEN</th>\n      <th>VP_OTH</th>\n      <th>VP_PRI</th>\n      <th>WORKWOMAN</th>\n      <th>YEARBUILT</th>\n      <th>INFER_PARTY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>380550</th>\n      <td>S</td>\n      <td>74.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>2103.0</td>\n      <td>723</td>\n      <td>1.212121</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n      <td>33.33</td>\n      <td>0.0</td>\n      <td>33.33</td>\n      <td>Y</td>\n      <td>2014</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>177284</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>0</td>\n      <td>40.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>149189</th>\n      <td>S</td>\n      <td>36.0</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>617400.0</td>\n      <td>&lt;NA&gt;</td>\n      <td>4.285714</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>0</td>\n      <td>40.00</td>\n      <td>0.0</td>\n      <td>10.00</td>\n      <td>Y</td>\n      <td>2007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>374160</th>\n      <td>H</td>\n      <td>55.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>600.0</td>\n      <td>&lt;NA&gt;</td>\n      <td>69.465649</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0</td>\n      <td>20.00</td>\n      <td>0.0</td>\n      <td>10.00</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>471852</th>\n      <td>S</td>\n      <td>55.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>202304.0</td>\n      <td>543</td>\n      <td>4.081633</td>\n      <td>...</td>\n      <td>7</td>\n      <td>Y</td>\n      <td>32</td>\n      <td>7</td>\n      <td>50.00</td>\n      <td>80.0</td>\n      <td>50.00</td>\n      <td>NaN</td>\n      <td>1910</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>498584</th>\n      <td>S</td>\n      <td>38.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22005.0</td>\n      <td>&lt;NA&gt;</td>\n      <td>39.449541</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>12.50</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>446111</th>\n      <td>S</td>\n      <td>40.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20104.0</td>\n      <td>&lt;NA&gt;</td>\n      <td>0.980392</td>\n      <td>...</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>14.29</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>&lt;NA&gt;</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>388712</th>\n      <td>S</td>\n      <td>64.0</td>\n      <td>A</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>50405.0</td>\n      <td>&lt;NA&gt;</td>\n      <td>1.379310</td>\n      <td>...</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>1</td>\n      <td>40.00</td>\n      <td>0.0</td>\n      <td>10.00</td>\n      <td>NaN</td>\n      <td>2002</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>377215</th>\n      <td>S</td>\n      <td>58.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10803.0</td>\n      <td>436</td>\n      <td>1.587302</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>15</td>\n      <td>0</td>\n      <td>50.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>1955</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>473583</th>\n      <td>S</td>\n      <td>63.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1104.0</td>\n      <td>866</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>2</td>\n      <td>Y</td>\n      <td>8</td>\n      <td>1</td>\n      <td>20.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>Y</td>\n      <td>&lt;NA&gt;</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 94 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_data.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:19.032661Z",
     "start_time": "2023-10-19T18:56:19.004956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Numerical Columns Count: 24\n",
      "Initial Non-Numerical Columns Count: 70\n",
      "Column ADD_TYPE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column AIRCOND couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column ASSMLCODE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column AUTOWORK couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column BROADERLIV couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column CARDUSER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column COMPUTERS couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column COOK_GEN couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column COUNTY_TYPE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column CREDRATE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DEPTSTCRD couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DIETING couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DOGOWNER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DON_CHARIT couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DON_HEALTH couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column DON_POLIT couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column EDUCATION couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column ETHNIC_INFER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column ETHNICCODE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column ETHNICGRP couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column EXER_GROUP couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column GARDENER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column GENERATION couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column GOLF couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HEALTHBEAU couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HEATHMED couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HITECHLIV couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HOME_DECOR couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HOMEMKTVAL couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HOMEOWNER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column HOMEOWNRNT couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column INCOMESTHH couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column LANGUAGE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column LIFESTAGE_CLUSTER couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column MAILORDBUY couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column MAILORDRSP couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column NETWORTH couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column OCCDETAIL couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column OCCUPATION couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PETS couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRESENCHLD couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRFL_AMZN_PRIME couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRFL_ANML_RIGHTS couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRFL_ENVIRONMENT couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRFL_INSURANCE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column PRFL_TAXES couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column RD_GEN couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column RD_RELIG couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column RELIGION couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column SEX couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column SPEC_FOOT couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column SPORTLEIS couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column STATE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column STATUS couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column SWEEPSTAKE couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column TOD_PRES_DIFF_2016 couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column TOD_PRES_DIFF_2016_PREC couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column TOD_PRES_DIFF_2020_PREC couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column TRAVEL couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column VOTER_TRLR couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column WORKWOMAN couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "Column INFER_PARTY couldn't be converted to 'int64' or 'float64'. It is non-numeric.\n",
      "\n",
      "Data types after conversion:\n",
      "ADD_TYPE        object\n",
      "AGE            float64\n",
      "AIRCOND         object\n",
      "ASSMLCODE       object\n",
      "AUTOWORK        object\n",
      "                ...   \n",
      "VP_OTH         float64\n",
      "VP_PRI         float64\n",
      "WORKWOMAN       object\n",
      "YEARBUILT      float64\n",
      "INFER_PARTY     object\n",
      "Length: 94, dtype: object\n",
      "\n",
      "Total Numerical Columns after conversion: 32\n",
      "Total Non-Numerical Columns after conversion: 62\n",
      "(499999, 94)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    # Initial count of numerical and non-numerical columns\n",
    "    initial_numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    initial_non_numerical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(f\"Initial Numerical Columns Count: {len(initial_numerical_cols)}\")\n",
    "    print(f\"Initial Non-Numerical Columns Count: {len(initial_non_numerical_cols)}\")\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    converted_data = df.copy()\n",
    "\n",
    "    # Initialize counters for numerical and non-numerical columns\n",
    "    num_cols_count = 0\n",
    "    non_num_cols_count = 0\n",
    "\n",
    "    # Loop through each column and try to convert it to numerical data types\n",
    "    for col in converted_data.columns:\n",
    "        try:\n",
    "            # Try converting to 'int64'\n",
    "            converted_data[col] = converted_data[col].astype('int64')\n",
    "            num_cols_count += 1\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # If that fails, try converting to 'float64'\n",
    "                converted_data[col] = converted_data[col].astype('float64')\n",
    "                num_cols_count += 1\n",
    "            except ValueError:\n",
    "                # If both fail, the column is genuinely non-numeric\n",
    "                non_num_cols_count += 1\n",
    "                print(f\"Column {col} couldn't be converted to 'int64' or 'float64'. It is non-numeric.\")\n",
    "\n",
    "    # Display the data types after the conversion\n",
    "    print(\"\\nData types after conversion:\")\n",
    "    print(converted_data.dtypes)\n",
    "\n",
    "    # Display count of numerical and non-numerical columns\n",
    "    print(f\"\\nTotal Numerical Columns after conversion: {num_cols_count}\")\n",
    "    print(f\"Total Non-Numerical Columns after conversion: {non_num_cols_count}\")\n",
    "\n",
    "    return converted_data\n",
    "\n",
    "# Assuming your DataFrame is named 'engineered_data'\n",
    "processed_data = preprocess_dataframe(engineered_data)\n",
    "print(processed_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:20.410055Z",
     "start_time": "2023-10-19T18:56:19.024948Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## time decorator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:20.414774Z",
     "start_time": "2023-10-19T18:56:20.412268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection with Random Forest Generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest run 1...\n",
      "Starting Random Forest run 2...\n",
      "Elapsed time: 16.63 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score    Recall  \\\n0    1    0.7364  0.724152  0.734295   \n1    2    0.7347  0.729203  0.749318   \n\n                                      Top_N_Features  \n0  [TOD_PRES_R_2016_PREC, TOD_PRES_D_2016_PREC, T...  \n1  [TOD_PRES_D_2016_PREC, TOD_PRES_R_2016_PREC, T...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.7364</td>\n      <td>0.724152</td>\n      <td>0.734295</td>\n      <td>[TOD_PRES_R_2016_PREC, TOD_PRES_D_2016_PREC, T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.7347</td>\n      <td>0.729203</td>\n      <td>0.749318</td>\n      <td>[TOD_PRES_D_2016_PREC, TOD_PRES_R_2016_PREC, T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_random_forest(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Random Forest run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Drop rows with missing 'INFER_CODE'\n",
    "        data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42+run)\n",
    "        \n",
    "        # Encode all object type columns\n",
    "        le = LabelEncoder()\n",
    "        for col in data_sample.select_dtypes(include=['object']).columns:\n",
    "            data_sample[col] = le.fit_transform(data_sample[col].astype(str))\n",
    "        \n",
    "        # Impute missing values with column means\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_sample = imputer.fit_transform(data_sample)\n",
    "        \n",
    "        # Convert back to DataFrame after imputation\n",
    "        data_sample = pd.DataFrame(data_sample, columns=data_copy.columns)\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample.drop(columns=['INFER_PARTY'])\n",
    "        y = data_sample['INFER_PARTY']\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Initialize Random Forest Classifier\n",
    "        rf = RandomForestClassifier(random_state=42+run)\n",
    "        \n",
    "        # Fit the model\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        # Get top N features using feature_importances_\n",
    "        top_feature_indices = np.argsort(rf.feature_importances_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "result = run_random_forest(processed_data, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:37.058553Z",
     "start_time": "2023-10-19T18:56:20.416945Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot for Random Forest Generator for Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest run for feature importance...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_feature_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 47\u001B[0m\n\u001B[1;32m     44\u001B[0m     plot_feature_importance(rf\u001B[38;5;241m.\u001B[39mfeature_importances_[top_feature_indices], top_features, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRandom Forest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Assuming processed_data is your DataFrame\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m \u001B[43mrun_random_forest_for_feature_importance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessed_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_N_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_feature_importance\u001B[39m(importance, names, model_type):\n\u001B[1;32m     49\u001B[0m     feature_importance \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: names, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m: importance})\n",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m, in \u001B[0;36mtimer_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      3\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 4\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      6\u001B[0m     elapsed_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time\n",
      "Cell \u001B[0;32mIn[11], line 44\u001B[0m, in \u001B[0;36mrun_random_forest_for_feature_importance\u001B[0;34m(data, sample_size, top_N_features)\u001B[0m\n\u001B[1;32m     41\u001B[0m top_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mcolumns[top_feature_indices]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Plot feature importance\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m \u001B[43mplot_feature_importance\u001B[49m(rf\u001B[38;5;241m.\u001B[39mfeature_importances_[top_feature_indices], top_features, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRandom Forest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plot_feature_importance' is not defined"
     ]
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_random_forest_for_feature_importance(data, sample_size=50000, top_N_features=10):\n",
    "    print(\"Starting Random Forest run for feature importance...\")\n",
    "\n",
    "    # Copy the original data to work with\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Drop rows with missing 'INFER_CODE'\n",
    "    data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "\n",
    "    # Sample data\n",
    "    data_sample = data_copy.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Encode all object type columns\n",
    "    le = LabelEncoder()\n",
    "    for col in data_sample.select_dtypes(include=['object']).columns:\n",
    "        data_sample[col] = le.fit_transform(data_sample[col].astype(str))\n",
    "\n",
    "    # Impute missing values with column means\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_sample = imputer.fit_transform(data_sample)\n",
    "\n",
    "    # Convert back to DataFrame after imputation\n",
    "    data_sample = pd.DataFrame(data_sample, columns=data_copy.columns)\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = data_sample.drop(columns=['INFER_PARTY'])\n",
    "    y = data_sample['INFER_PARTY']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Random Forest Classifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Get top N features using feature_importances_\n",
    "    top_feature_indices = np.argsort(rf.feature_importances_)[::-1][:top_N_features]\n",
    "    top_features = X.columns[top_feature_indices].tolist()\n",
    "\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(rf.feature_importances_[top_feature_indices], top_features, 'Random Forest')\n",
    "\n",
    "# Assuming processed_data is your DataFrame\n",
    "run_random_forest_for_feature_importance(processed_data, sample_size=50000, top_N_features=10)\n",
    "def plot_feature_importance(importance, names, model_type):\n",
    "    feature_importance = pd.DataFrame({'feature': names, 'importance': importance})\n",
    "    feature_importance = feature_importance.sort_values(by='importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    cubehelix_palette = sns.cubehelix_palette(start=.5, rot=-.5)\n",
    "    \n",
    "    ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance, palette=cubehelix_palette)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_width():.3f}', \n",
    "                    (p.get_width(), p.get_y() + p.get_height() / 2.), \n",
    "                    ha='left', \n",
    "                    va='center', \n",
    "                    fontsize=12, \n",
    "                    color='black',\n",
    "                    xytext=(5, 0), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.title(f'{model_type} - Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:45.562014Z",
     "start_time": "2023-10-19T18:56:37.056436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result['Top_N_Features'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.560001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection with Gradient boosting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradient Boosting run 1...\n",
      "Starting Gradient Boosting run 2...\n",
      "Elapsed time: 83.39 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score   Recall  \\\n0    1    0.7408  0.743061  0.71911   \n1    2    0.7408  0.743061  0.71911   \n\n                                      Top_N_Features  \n0  [OCCDETAIL_L419, OCCDETAIL_L400, OCCDETAIL_L46...  \n1  [OCCDETAIL_L419, OCCDETAIL_L400, OCCDETAIL_L46...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.7408</td>\n      <td>0.743061</td>\n      <td>0.71911</td>\n      <td>[OCCDETAIL_L419, OCCDETAIL_L400, OCCDETAIL_L46...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.7408</td>\n      <td>0.743061</td>\n      <td>0.71911</td>\n      <td>[OCCDETAIL_L419, OCCDETAIL_L400, OCCDETAIL_L46...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_gradient_boosting(data, sample_size=100000, num_runs=1, top_N_features=10, pos_label='D'):\n",
    "    results_list = []\n",
    "    \n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Gradient Boosting run {run}...\")\n",
    "        \n",
    "        data_copy = data.copy()\n",
    "        data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        y = data_sample['INFER_PARTY']\n",
    "        X = data_sample.drop('INFER_PARTY', axis=1)\n",
    "        \n",
    "        X = pd.get_dummies(X, drop_first=True)\n",
    "        \n",
    "        class_counts = y.value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        if min_class_count > 1:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        gb = GradientBoostingClassifier(random_state=42)\n",
    "        gb.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = gb.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=pos_label)\n",
    "        \n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "result = run_gradient_boosting(processed_data, sample_size=50000, num_runs=2, top_N_features=10, pos_label='D')\n",
    "#result = run_gradient_boosting(processed_data, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:58:43.420067Z",
     "start_time": "2023-10-19T18:57:19.982950Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot for Gradient Boosting for Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bar plot with plotly\n",
    "@timer_decorator\n",
    "def run_gradient_boosting_for_feature_importance(data, sample_size=100000, top_N_features=10):\n",
    "    print(f\"Starting Gradient Boosting...\")\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "    data_sample = data_copy.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    y = data_sample['INFER_PARTY']\n",
    "    X = data_sample.drop('INFER_PARTY', axis=1)\n",
    "    \n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "    k_best.fit(X_train, y_train)\n",
    "    top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "    top_features = X.columns[top_feature_indices].tolist()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    feature_importances = gb.feature_importances_[top_feature_indices]\n",
    "    plot_feature_importance(feature_importances, top_features, 'Gradient Boosting')\n",
    "\n",
    "def plot_feature_importance(importance, names, model_type):\n",
    "    feature_importance = pd.DataFrame({'feature': names, 'importance': importance})\n",
    "    feature_importance = feature_importance.sort_values(by='importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    cubehelix_palette = sns.cubehelix_palette(start=.5, rot=-.5)\n",
    "    \n",
    "    ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance, palette=cubehelix_palette)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_width():.3f}', \n",
    "                    (p.get_width(), p.get_y() + p.get_height() / 2.), \n",
    "                    ha='left', \n",
    "                    va='center', \n",
    "                    fontsize=12, \n",
    "                    color='black',\n",
    "                    xytext=(5, 0), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.title(f'{model_type} - Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming processed_data is your dataset, you can call the function like this:\n",
    "run_gradient_boosting_for_feature_importance(processed_data, sample_size=50000, top_N_features=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result['Top_N_Features'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:45.565425Z",
     "start_time": "2023-10-19T18:56:45.563685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.564873Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection with Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Logistic Regression run 1...\n",
      "Starting Logistic Regression run 2...\n",
      "Starting Logistic Regression run 3...\n",
      "Starting Logistic Regression run 4...\n",
      "Starting Logistic Regression run 5...\n",
      "Starting Logistic Regression run 6...\n",
      "Starting Logistic Regression run 7...\n",
      "Starting Logistic Regression run 8...\n",
      "Starting Logistic Regression run 9...\n",
      "Starting Logistic Regression run 10...\n",
      "Elapsed time: 10.89 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score    Recall  \\\n0    1    0.7076  0.699671  0.715847   \n1    2    0.7058  0.697263  0.712363   \n2    3    0.7212  0.714344  0.731431   \n3    4    0.7044  0.690406  0.700382   \n4    5    0.7186  0.709717  0.719967   \n5    6    0.7122  0.701266  0.713863   \n6    7    0.7162  0.705663  0.717722   \n7    8    0.7192  0.708593  0.719343   \n8    9    0.7278  0.717107  0.726316   \n9   10    0.7152  0.704319  0.719864   \n\n                                      Top_N_Features  \n0  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n1  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n2  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n3  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n4  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n5  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n6  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n7  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n8  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  \n9  [CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.7076</td>\n      <td>0.699671</td>\n      <td>0.715847</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.7058</td>\n      <td>0.697263</td>\n      <td>0.712363</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.7212</td>\n      <td>0.714344</td>\n      <td>0.731431</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.7044</td>\n      <td>0.690406</td>\n      <td>0.700382</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.7186</td>\n      <td>0.709717</td>\n      <td>0.719967</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0.7122</td>\n      <td>0.701266</td>\n      <td>0.713863</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0.7162</td>\n      <td>0.705663</td>\n      <td>0.717722</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0.7192</td>\n      <td>0.708593</td>\n      <td>0.719343</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0.7278</td>\n      <td>0.717107</td>\n      <td>0.726316</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0.7152</td>\n      <td>0.704319</td>\n      <td>0.719864</td>\n      <td>[CNSUS_PCTB, TOD_PRES_R_2016_PREC, TOD_PRES_D_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_logistic_regression(data, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting Logistic Regression run {run}...\")\n",
    "        \n",
    "        # Copy the original data to work with\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Drop rows with missing 'INFER_PARTY'\n",
    "        data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "        \n",
    "        # Sample data\n",
    "        data_sample = data_copy.sample(n=sample_size, random_state=42+run)\n",
    "        \n",
    "        # Encode 'INFER_PARTY' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "        \n",
    "        # Check for classes with only one instance after sampling and encoding\n",
    "        class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        # Select numerical columns\n",
    "        numerical_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        # Drop 'INFER_PARTY' from the numerical columns as it is the target variable\n",
    "        numerical_cols = numerical_cols.drop('INFER_PARTY')\n",
    "\n",
    "        # Impute missing values with column means\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_sample[numerical_cols] = imputer.fit_transform(data_sample[numerical_cols])\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X = data_sample[numerical_cols]\n",
    "        y = data_sample['INFER_PARTY']\n",
    "        \n",
    "        # Train-test split\n",
    "        if min_class_count > 1:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize Logistic Regression model\n",
    "        lr = LogisticRegression(C=0.1, max_iter=10000, random_state=42+run)\n",
    "    \n",
    "        # Fit the model\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = lr.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred, zero_division='warn')\n",
    "        \n",
    "        # Get top N features using SelectKBest with chi-squared\n",
    "        k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "        top_features = X.columns[top_feature_indices].tolist()\n",
    "        \n",
    "        # Append results to the list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 is your dataset, you can call the function like this:\n",
    "result = run_logistic_regression(processed_data, sample_size=50000, num_runs=10, top_N_features=30)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T19:01:09.014165Z",
     "start_time": "2023-10-19T19:00:58.098948Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot for Logistic Regression for Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance, names, model_type):\n",
    "    feature_importance = pd.DataFrame({'feature': names, 'importance': importance})\n",
    "    feature_importance = feature_importance.sort_values(by='importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    cubehelix_palette = sns.cubehelix_palette(start=.5, rot=-.5)\n",
    "    \n",
    "    ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance, palette=cubehelix_palette)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_width():.3f}', \n",
    "                    (p.get_width(), p.get_y() + p.get_height() / 2.), \n",
    "                    ha='left', \n",
    "                    va='center', \n",
    "                    fontsize=12, \n",
    "                    color='black',\n",
    "                    xytext=(5, 0), \n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.title(f'{model_type} - Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_logistic_regression_for_feature_importance(data, sample_size=100000, top_N_features=10):\n",
    "    # Copy the original data to work with\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Drop rows with missing 'INFER_PARTY'\n",
    "    data_copy = data_copy.dropna(subset=['INFER_PARTY'])\n",
    "    \n",
    "    # Sample data\n",
    "    data_sample = data_copy.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Encode 'INFER_PARTY' column\n",
    "    le = LabelEncoder()\n",
    "    data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "    \n",
    "    # Select numerical columns\n",
    "    numerical_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Drop 'INFER_PARTY' from the numerical columns as it is the target variable\n",
    "    numerical_cols = numerical_cols.drop('INFER_PARTY')\n",
    "\n",
    "    # Impute missing values with column means\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_sample[numerical_cols] = imputer.fit_transform(data_sample[numerical_cols])\n",
    "    \n",
    "    # Extract features and target variable\n",
    "    X = data_sample[numerical_cols]\n",
    "    y = data_sample['INFER_PARTY']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize Logistic Regression model\n",
    "    lr = LogisticRegression(C=0.1, max_iter=10000, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Get top N features using SelectKBest with chi-squared\n",
    "    k_best = SelectKBest(score_func=chi2, k=top_N_features)\n",
    "    k_best.fit(X_train, y_train)\n",
    "    top_feature_indices = np.argsort(k_best.scores_)[::-1][:top_N_features]\n",
    "    top_features = X.columns[top_feature_indices].tolist()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    feature_importances = k_best.scores_[top_feature_indices]\n",
    "    plot_feature_importance(feature_importances, top_features, 'Logistic Regression')\n",
    "\n",
    "# Assuming processed_data is your dataset, you can call the function like this:\n",
    "run_logistic_regression_for_feature_importance(processed_data, sample_size=50000, top_N_features=20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.567126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.568179Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGradient Boosting for Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@timer_decorator\n",
    "def run_xgboost_all_columns(data, sample_size=50000, num_runs=2, top_N_features=50):\n",
    "    results_list = []\n",
    "    le = LabelEncoder()  # Initialize the label encoder\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting run {run}...\")\n",
    "        \n",
    "        data_with_all_columns = data.copy()\n",
    "        data_with_all_columns = data_with_all_columns[data_with_all_columns['INFER_PARTY'].notna()]\n",
    "        \n",
    "        print(\"Number of columns in the dataset:\", len(data_with_all_columns.columns))\n",
    "\n",
    "        data_sample = data_with_all_columns.sample(n=sample_size, random_state=42)\n",
    "\n",
    "        # Convert object columns to category type\n",
    "        for col in data_sample.select_dtypes(['object']).columns:\n",
    "            data_sample[col] = data_sample[col].astype('category')\n",
    "\n",
    "        # Splitting the data into features and target without encoding feature columns\n",
    "        X = data_sample.drop('INFER_PARTY', axis=1)\n",
    "        \n",
    "        # Encoding the target column 'INFER_PARTY'\n",
    "        y = le.fit_transform(data_sample['INFER_PARTY'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        xgb = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)), random_state=42, enable_categorical=True)\n",
    "        xgb.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = xgb.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred, zero_division='warn')\n",
    "\n",
    "        print(f\"Run {run} - Baseline Metrics: Accuracy: {accuracy}, F1 Score: {f1}, Recall: {recall}\")\n",
    "\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "        print(top_features)\n",
    "        \n",
    "        # Re-train and evaluate the model using top 50 features\n",
    "        X_train_top_50 = X_train[top_features]\n",
    "        X_test_top_50 = X_test[top_features]\n",
    "        \n",
    "        xgb_top_50 = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)), random_state=42, enable_categorical=True)\n",
    "        xgb_top_50.fit(X_train_top_50, y_train)\n",
    "        \n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "        top_importances = feature_importances[sorted_idx]\n",
    "    \n",
    "        # Plot the feature importances\n",
    "        plt.figure(figsize=(7, 15))\n",
    "        plt.barh(top_features, top_importances, align='center')\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature Name')\n",
    "        plt.title('Top 50 Features Based on Importance')\n",
    "        plt.gca().invert_yaxis()  # Show the most important feature at the top\n",
    "        plt.show()\n",
    "        \n",
    "        y_pred_top_50 = xgb_top_50.predict(X_test_top_50)\n",
    "        accuracy_top_50 = accuracy_score(y_test, y_pred_top_50)\n",
    "        f1_top_50 = f1_score(y_test, y_pred_top_50)\n",
    "        recall_top_50 = recall_score(y_test, y_pred_top_50)\n",
    "\n",
    "        print(f\"Run {run} - Model performance with top 50 features:\")\n",
    "        print(f\"Accuracy: {accuracy_top_50}\")\n",
    "        print(f\"F1 Score: {f1_top_50}\")\n",
    "        print(f\"Recall: {recall_top_50}\")\n",
    "\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'Recall': recall,\n",
    "            'F1_Score': f1,\n",
    "            'Accuracy_Top_50': accuracy_top_50,\n",
    "            'F1_Score_Top_50': f1_top_50,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(results_list)\n",
    "    return result_df\n",
    "\n",
    "# Assuming engineered_data is already defined\n",
    "result = run_xgboost_all_columns(engineered_data, sample_size=50000, num_runs=2, top_N_features=20)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.569329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in result['Top_N_Features']:\n",
    "    print(n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.570405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.571570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.572716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.573952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.574895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.575594Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.576374Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.577286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.578089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.578822Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T18:56:45.585680Z",
     "start_time": "2023-10-19T18:56:45.579543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.580249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.580974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.581667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T18:56:45.582370Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
