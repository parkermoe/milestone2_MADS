{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:12:07.475333Z",
     "start_time": "2023-10-13T19:12:07.192447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 500K dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "        RECORD_ID ADD_TYPE AFAMPROFLS   AGE        AI_COUNTY_NAME AIRCOND  \\\n0          403390        S             21.0  Fairbanks North Star           \n1           62285        H              NaN             Anchorage           \n2          331355                      91.0       Kenai Peninsula           \n3          206320        H             65.0             Anchorage           \n4          188078        S             76.0                Juneau           \n...           ...      ...        ...   ...                   ...     ...   \n499995     349635        H             20.0                  BIBB           \n499996     420654        S             50.0                COWETA       A   \n499997     131262        S             19.0              ROCKDALE           \n499998     315673        H             21.0                BARROW           \n499999     467477        S             19.0               CHATHAM           \n\n       APP_CHILD APP_MENBIG APP_TODDLR APP_WOMEN  ... VTR_PRI16 VTR_PRI17  \\\n0                                                 ...                       \n1                                                 ...                       \n2                                                 ...                       \n3                                                 ...                       \n4                                                 ...         Y             \n...          ...        ...        ...       ...  ...       ...       ...   \n499995                                            ...                       \n499996                                            ...                       \n499997                                            ...                       \n499998                                            ...                       \n499999                                            ...                       \n\n       VTR_PRI18 VTR_PRI19 VTR_PRI20 VTR_PRI21 VTR_PRI22 WORKWOMAN YEARBUILT  \\\n0                                                                        NaN   \n1                                                                        NaN   \n2                                                                        NaN   \n3                                                      Y                       \n4              Y                                       Y         Y      1985   \n...          ...       ...       ...       ...       ...       ...       ...   \n499995                                                                   NaN   \n499996                                                                  2003   \n499997                                                                         \n499998                                                                   NaN   \n499999                                                                   NaN   \n\n          ZIP  \n0       99705  \n1       99506  \n2       99603  \n3       99567  \n4       99801  \n...       ...  \n499995  31204  \n499996  30263  \n499997  30013  \n499998  30680  \n499999  31407  \n\n[500000 rows x 298 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RECORD_ID</th>\n      <th>ADD_TYPE</th>\n      <th>AFAMPROFLS</th>\n      <th>AGE</th>\n      <th>AI_COUNTY_NAME</th>\n      <th>AIRCOND</th>\n      <th>APP_CHILD</th>\n      <th>APP_MENBIG</th>\n      <th>APP_TODDLR</th>\n      <th>APP_WOMEN</th>\n      <th>...</th>\n      <th>VTR_PRI16</th>\n      <th>VTR_PRI17</th>\n      <th>VTR_PRI18</th>\n      <th>VTR_PRI19</th>\n      <th>VTR_PRI20</th>\n      <th>VTR_PRI21</th>\n      <th>VTR_PRI22</th>\n      <th>WORKWOMAN</th>\n      <th>YEARBUILT</th>\n      <th>ZIP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>403390</td>\n      <td>S</td>\n      <td></td>\n      <td>21.0</td>\n      <td>Fairbanks North Star</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99705</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62285</td>\n      <td>H</td>\n      <td></td>\n      <td>NaN</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99506</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>331355</td>\n      <td></td>\n      <td></td>\n      <td>91.0</td>\n      <td>Kenai Peninsula</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99603</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>206320</td>\n      <td>H</td>\n      <td></td>\n      <td>65.0</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td>99567</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188078</td>\n      <td>S</td>\n      <td></td>\n      <td>76.0</td>\n      <td>Juneau</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>Y</td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1985</td>\n      <td>99801</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>349635</td>\n      <td>H</td>\n      <td></td>\n      <td>20.0</td>\n      <td>BIBB</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31204</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>420654</td>\n      <td>S</td>\n      <td></td>\n      <td>50.0</td>\n      <td>COWETA</td>\n      <td>A</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>2003</td>\n      <td>30263</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>131262</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>ROCKDALE</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>30013</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>315673</td>\n      <td>H</td>\n      <td></td>\n      <td>21.0</td>\n      <td>BARROW</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>30680</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>467477</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>CHATHAM</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31407</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows Ã— 298 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "data_500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:12:27.687795Z",
     "start_time": "2023-10-13T19:12:07.804053Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "remove_columns = [\n",
    "    'PRFL_LGBT_SUPPORT',\n",
    "    'PRFL_LIBERAL_NEWS',\n",
    "    'PRFL_MARIJUANA_REFORM',\n",
    "    'PRFL_BIDEN_SUPPORT',\n",
    "    'PRFL_BORDER_SECURITY',\n",
    "    'PRFL_CONSERVATIVE_NEWS',\n",
    "    'PRFL_IMMIGRATION_REFORM',\n",
    "    'PRFL_OBAMA',\n",
    "    'PRFL_PERSUADABLE_VOTER',\n",
    "    'PRFL_POLITICAL_IDEOLOGY',\n",
    "    'PRFL_SANDERS_SUPPORT',\n",
    "    'PRFL_TRUMP_SUPPORT',\n",
    "    'ZIP',\n",
    "    \n",
    "    'VTR_GEN00', 'VTR_GEN01', 'VTR_GEN02', 'VTR_GEN03', 'VTR_GEN04', 'VTR_GEN05', 'VTR_GEN06', 'VTR_GEN07', 'VTR_GEN08', 'VTR_GEN09', 'VTR_GEN10', 'VTR_GEN11', 'VTR_GEN12', 'VTR_GEN13', 'VTR_GEN14', 'VTR_GEN15', 'VTR_GEN16', 'VTR_GEN17', 'VTR_GEN18', 'VTR_GEN19', 'VTR_GEN20', 'VTR_GEN21', 'VTR_GEN22', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PPP08', 'VTR_PPP12', 'VTR_PPP16', 'VTR_PPP20', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI10', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI14', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI18', 'VTR_PRI19', 'VTR_PRI20', 'VTR_PRI21', 'VTR_PRI22',\n",
    "    \n",
    "        \n",
    "      'PRFL_CHOICELIFE', 'TOD_PRES_D_2016_PREC', 'TOD_PRES_O_2016',\n",
    "    'TOD_PRES_R_2016', 'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'VP_PPP',\n",
    "    'AGE', 'CNSUS_PCTW',\n",
    "    \n",
    "    'PARTY_MIX', 'PRFL_MINWAGE', 'PRFL_FENCE_SITTER'\n",
    "]\n",
    "# Drop the list of columns from the dataset\n",
    "data_500.drop(columns=remove_columns, errors='ignore', inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:12:29.748097Z",
     "start_time": "2023-10-13T19:12:29.746060Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with 'Y' or 'Unknown': ['AFAMPROFLS', 'APP_CHILD', 'APP_MENBIG', 'APP_TODDLR', 'APP_WOMEN', 'APP_WOMPET', 'APP_WOMPLS', 'APP_YNGMEN', 'ARTS', 'AUTOACCES', 'AUTOWORK', 'BOATING', 'BROADERLIV', 'CARDUSER', 'CATOWNER', 'CH_0002FEM', 'CH_0002MAL', 'CH_0002UNK', 'CH_0305FEM', 'CH_0305MAL', 'CH_0305UNK', 'CH_0610FEM', 'CH_0610MAL', 'CH_0610UNK', 'CH_1115FEM', 'CH_1115MAL', 'CH_1115UNK', 'CH_1617FEM', 'CH_1617MAL', 'CH_1617UNK', 'CHRISTFAM', 'COL_ANTIQ', 'COL_ARTS', 'COL_COIN', 'COL_SPORT', 'COL_STAMP', 'COMPHOMOFC', 'COMPUTERS', 'COOK_GEN', 'CURRAFFAIR', 'DEPTSTCRD', 'DIETING', 'DIYLIV', 'DOGOWNER', 'DON_ANML', 'DON_ARTCUL', 'DON_CHARIT', 'DON_CHILD', 'DON_ENVIR', 'DON_ENVWLD', 'DON_HEALTH', 'DON_INTAID', 'DON_OTHER', 'DON_POLCONS', 'DON_POLIT', 'DON_POLLIB', 'DON_RELIG', 'DON_VET', 'DONATION', 'EDU_ONLINE', 'EQUESTRIAN', 'EXER_GROUP', 'GAMING', 'GARDENER', 'GOLF', 'GRANDCHLD', 'HEALTHBEAU', 'HEATHMED', 'HH_SENIOR', 'HH_VETERAN', 'HH_YOUNGAD', 'HIGHBROW', 'HIGHENDAPP', 'HISTMIL', 'HITECHLIV', 'HOME_DECOR', 'HOMEOFFICE', 'HUNTING', 'HUNTSHOOT', 'INV_ACTIVE', 'MAIL_DONOR', 'MAILORDBUY', 'MAILORDRSP', 'MOTORCYCLE', 'NASCAR', 'PARENTING', 'PETS', 'PRESENCHLD', 'PRFL_2NDAMEND', 'PRFL_ACTIVE_MIL', 'PRFL_AMZN_PRIME', 'PRFL_ANML_RIGHTS', 'PRFL_BLM_SUPPORT', 'PRFL_CLINTON_SUPPORT', 'PRFL_EDUCATION', 'PRFL_ENVIRONMENT', 'PRFL_EVANGELICAL', 'PRFL_FENCE_SITTER', 'PRFL_GUN_CONTROL', 'PRFL_HEALTHCARE', 'PRFL_INFLUENCER', 'PRFL_INSURANCE', 'PRFL_LABOR', 'PRFL_METOO_SUPPORT', 'PRFL_MIL_SUPPORT', 'PRFL_TAXES', 'PRFL_TEACHERS_UNION', 'PRFL_VETERAN', 'RD_FINNEWS', 'RD_GEN', 'RD_RELIG', 'RD_SCIFI', 'RELIGINSP', 'SCISPACE', 'SCUBADIVER', 'SELFIMP', 'SINGPARENT', 'SMOKING', 'SPEC_AUTO', 'SPEC_BASE', 'SPEC_BASK', 'SPEC_FOOT', 'SPEC_HOCK', 'SPEC_SOCC', 'SPORTLEIS', 'SWEEPSTAKE', 'TELECOM', 'TENNIS', 'THEATER', 'TRAVEL', 'WORKWOMAN'] 131\n",
      "Columns with more than two categories: ['ADD_TYPE', 'AI_COUNTY_NAME', 'AIRCOND', 'ASSMLCODE', 'BUS_OWNER', 'CENSUS_ST', 'CNS_MEDINC', 'CONG_DIST', 'COUNTY_ST', 'COUNTY_TYPE', 'CRD_RANGE', 'CREDRATE', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE', 'ETHNICCONF', 'ETHNICGRP', 'FUND_POLIT', 'GENDER_MIX', 'GENERATION', 'HH_NUMGEN', 'HH_SIZE', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT', 'INCOMESTHH', 'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'NETWORTH', 'NUMCHILD', 'OCCDETAIL', 'OCCUPATION', 'PARTY_CODE', 'PERSONS_HH', 'POOL', 'PRFL_TEAPARTY', 'RELIGION', 'SEX', 'ST_LO_HOUS', 'ST_UP_HOUS', 'STATE', 'STATUS', 'TOD_PRES_DIFF_2016', 'TOD_PRES_DIFF_2016_PREC', 'TOD_PRES_DIFF_2020_PREC', 'VOTER_CNT', 'VOTER_TRLR', 'YEARBUILT'] 49\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "#data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning - Column Names\n",
    "data_500.columns = data_500.columns.str.strip()\n",
    "\n",
    "# Data Cleaning - Drop Duplicates\n",
    "data_500.drop_duplicates(inplace=True)\n",
    "\n",
    "# Data Cleaning - Object Columns\n",
    "for col in data_500.columns:\n",
    "    if data_500[col].dtype == 'object':\n",
    "        data_500[col] = data_500[col].str.strip()\n",
    "\n",
    "# Data Cleaning - Empty Strings\n",
    "data_500.replace('', 'Unknown', inplace=True)\n",
    "\n",
    "# Data Cleaning - NaN for Object Types\n",
    "data_500.loc[:, data_500.dtypes == 'object'] = data_500.loc[:, data_500.dtypes == 'object'].fillna('Unknown')\n",
    "\n",
    "# Data Cleaning - Drop Columns and Rows with All NaNs\n",
    "data_500.dropna(axis=1, how='all', inplace=True)\n",
    "data_500.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = data_500.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = data_500.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Data Cleaning - Removing Non-Numeric Columns with More Than 90% Missing Data\n",
    "missing_data_percentage = data_500.isnull().mean() * 100\n",
    "non_numeric_cols_to_remove = missing_data_percentage[non_numeric_cols]\n",
    "non_numeric_cols_to_remove = non_numeric_cols_to_remove[non_numeric_cols_to_remove > 90].index.tolist()\n",
    "data_500_reduced = data_500.drop(columns=non_numeric_cols_to_remove)\n",
    "\n",
    "# Update the list of non-numeric columns after removal\n",
    "non_numeric_cols = data_500_reduced.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Identifying Specific Types of Non-Numeric Columns\n",
    "cols_with_Y_or_Unknown = [col for col in non_numeric_cols if set(data_500_reduced[col].unique()) <= {'Y', 'Unknown'}]\n",
    "cols_with_more_than_two_categories = [col for col in non_numeric_cols if len(data_500_reduced[col].unique()) > 2]\n",
    "\n",
    "# Print identified columns\n",
    "print(\"Columns with 'Y' or 'Unknown':\", cols_with_Y_or_Unknown, len(cols_with_Y_or_Unknown))\n",
    "print(\"Columns with more than two categories:\", cols_with_more_than_two_categories, len(cols_with_more_than_two_categories))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:13:00.901626Z",
     "start_time": "2023-10-13T19:12:29.752265Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'R' 'O' 'D' 'A' 'F' 'P' 'G' 'L' 'U' 'W' 'B' 'I' 'Y' 'V' 'H' 'Unknown'\n",
      " 'S' 'E' 'Q' 'Z']\n",
      "unique_infer_party:\n",
      "[nan 'R' 'D']\n",
      "unique_party_code_after_modifications:\n",
      "['N' 'R' 'O' 'D' 'L' nan 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again\n",
    "#data = pd.read_csv('data/surveydata.csv')\n",
    "\n",
    "corrected_data = data_500.copy()\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "states_to_update = ['DC', 'LA', 'RI']\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "\n",
    "#categorical_columns\n",
    "# Create a mapping dictionary for PARTY_CODE to INFER_PARTY values\n",
    "party_mapping = {\n",
    "    'D': 'D',\n",
    "    'E': 'D',\n",
    "    'R': 'R',\n",
    "    'S': 'R',\n",
    "    'N': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': float('nan'),\n",
    "    'B': float('nan'),\n",
    "    'C': float('nan'),\n",
    "    'F': float('nan'),\n",
    "    'G': float('nan'),\n",
    "    'H': float('nan'),\n",
    "    'I': float('nan'),\n",
    "    'J': float('nan'),\n",
    "    'K': float('nan'),\n",
    "    'L': float('nan'),\n",
    "    'P': float('nan'),\n",
    "    'Q': float('nan'),\n",
    "    'T': float('nan'),\n",
    "    'V': float('nan'),\n",
    "    'W': float('nan'),\n",
    "    'Y': float('nan'),\n",
    "    'Z': float('nan'),\n",
    "    'O': float('nan'),\n",
    "}\n",
    "\n",
    "# Create the INFER_PARTY column using the mapping\n",
    "corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(party_mapping)\n",
    "\n",
    "print(corrected_data['PARTY_CODE'].unique())\n",
    "\n",
    "\n",
    "# Display the unique values in the INFER_PARTY column to ensure correctness\n",
    "unique_infer_party = corrected_data['INFER_PARTY'].unique()\n",
    "\n",
    "print('unique_infer_party:')\n",
    "\n",
    "print(unique_infer_party)\n",
    "\n",
    "# Define the mapping for PARTY_CODE modifications\n",
    "party_code_mapping = {\n",
    "    'E': float('nan'),\n",
    "    'S': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': 'O',\n",
    "    'B': 'O',\n",
    "    'C': 'O',\n",
    "    'F': 'O',\n",
    "    'G': 'O',\n",
    "    'H': 'O',\n",
    "    'I': 'O',\n",
    "    'J': 'O',\n",
    "    'K': 'O',\n",
    "    'L': 'L',\n",
    "    'P': 'O',\n",
    "    'Q': 'O',\n",
    "    'T': 'O',\n",
    "    'V': 'O',\n",
    "    'W': 'O',\n",
    "    'Y': 'O',\n",
    "    'Z': 'O'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the PARTY_CODE column\n",
    "corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "# Check the unique values of PARTY_CODE after the modifications\n",
    "unique_party_code_after_modifications = corrected_data['PARTY_CODE'].unique()\n",
    "\n",
    "print(\"unique_party_code_after_modifications:\")\n",
    "\n",
    "print(unique_party_code_after_modifications)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:13:01.825651Z",
     "start_time": "2023-10-13T19:13:00.905647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154352\n",
      "154352\n"
     ]
    }
   ],
   "source": [
    "engineered_data = corrected_data.copy()\n",
    "voter_columns = [col for col in engineered_data.columns if col.startswith(\"VTR\")]\n",
    "#Create column with total number of votes in voter_columns per row\n",
    "engineered_data['VTR_TOTAL_VOTES'] = engineered_data[voter_columns].notnull().sum(axis=1)\n",
    "#Sum Democrat and Republican totals\n",
    "engineered_data['VTR_TOTAL_DVOTES'] = engineered_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "engineered_data['VTR_TOTAL_RVOTES'] = engineered_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "#Create conditions that evaluate whether more votes have been cast for Democrats or Republicans, and assign 'D' and 'R' to new column based on conditions\n",
    "conditions = [\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] > engineered_data['VTR_TOTAL_RVOTES'],\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] < engineered_data['VTR_TOTAL_RVOTES']\n",
    "]\n",
    "choices = ['D', 'R']\n",
    "engineered_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "#Create conditions that evaluate whether a voter is a swing voter or not\n",
    "conditions_swing = [\n",
    "    (engineered_data['VTR_TOTAL_DVOTES'] > 2) & (engineered_data['VTR_TOTAL_RVOTES'] > 2),\n",
    "    ((engineered_data['VTR_TOTAL_DVOTES'] > 1) & (engineered_data['VTR_TOTAL_RVOTES'] == 0)) | ((engineered_data['VTR_TOTAL_RVOTES'] > 1) & (engineered_data['VTR_TOTAL_DVOTES'] == 0))\n",
    "]\n",
    "choices_swing = ['Y', 'N']\n",
    "engineered_data['VTR_INFER_SWING'] = np.select(conditions_swing, choices_swing, default=np.nan)\n",
    "#sampledf = engineered_data[['PARTY_CODE','INFER_PARTY','VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY','VTR_INFER_SWING']]\n",
    "#print(sampledf.head(50))\n",
    "#Add values to INFER_PARTY and correct any other INFER_PARTY values that don't meet the conditions above:\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'D'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'R'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "#Also delete any 'D' or 'R' INFER_PARTY labels for anyone deemed a \"swing voter\" based on criteria above of voting for both parties at least 3 times each:\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_SWING'] == 'Y') , 'INFER_PARTY'] = float('nan')\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "#Drop auxiliary columns used for math, keeping 'VTR_TOTAL_VOTES', 'VTR_INFER_SWING', and the updated 'INFER_PARTY' columns as features:\n",
    "engineered_data = engineered_data.drop(columns=['VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:13:04.892677Z",
     "start_time": "2023-10-13T19:13:01.827837Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:13:04.897952Z",
     "start_time": "2023-10-13T19:13:04.894176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Just the columns with Yes or Unknown non_numeric_cols "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(500000, 131)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced_with_Y_or_Unknown = data_500[cols_with_Y_or_Unknown].copy()\n",
    "data_reduced_with_Y_or_Unknown.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T03:30:37.264427Z",
     "start_time": "2023-10-09T03:30:37.119142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test INFER_PARTY\n",
      "0    1061\n",
      "1     939\n",
      "Name: count, dtype: int64\n",
      "y_pred_value_counts Counter({0: 1266, 1: 734})\n",
      "0.5047923322683706\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score    Recall  \\\n0    1    0.6375  0.566647  0.504792   \n\n                                      Top_N_Features  \n0  [PRFL_BLM_SUPPORT_Y, PRFL_2NDAMEND_Y, PRFL_MET...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.6375</td>\n      <td>0.566647</td>\n      <td>0.504792</td>\n      <td>[PRFL_BLM_SUPPORT_Y, PRFL_2NDAMEND_Y, PRFL_MET...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_xgboost(sample_size, num_runs, top_N_features):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "        data_reduced_with_Y_or_Unknown = engineered_data[cols_with_Y_or_Unknown].copy()\n",
    "\n",
    "        # Adding the target column to this data\n",
    "        data_reduced_with_Y_or_Unknown['INFER_PARTY'] = engineered_data['INFER_PARTY']\n",
    "\n",
    "        # Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "        data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['INFER_PARTY'].notna()]\n",
    "\n",
    "        # Sample data\n",
    "        data_sample = data_reduced_with_Y_or_Unknown.sample(n=sample_size, random_state=run)\n",
    "\n",
    "        # Dynamic Class Handling\n",
    "        class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 1].index.tolist()\n",
    "        data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "\n",
    "        # Label-encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "\n",
    "        # One-Hot Encoding\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "\n",
    "        # Splitting the Data into Training and Test Sets\n",
    "        X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "        y = data_one_hot['INFER_PARTY']\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        print(\"y_test\", y_test.value_counts())\n",
    "        \n",
    "        \n",
    "        # Initialize and Train XGBoost Classifier\n",
    "        xgb = XGBClassifier(objective='multi:softmax', num_class=len(valid_classes), random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        # Make Predictions and Evaluate the Model\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        y_pred_value_counts = Counter(y_pred)\n",
    "        print(\"y_pred_value_counts\",y_pred_value_counts)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test.to_list(), y_pred)\n",
    "        \n",
    "        true_positive = sum(1 for a, b in zip(y_test.to_list(), y_pred) if a == 1 and b == 1)\n",
    "        new_recall = true_positive/ (939)\n",
    "        print(new_recall)\n",
    "\n",
    "        # Get top N features\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        # Append to results list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example of how to call this function\n",
    "result = run_xgboost(sample_size=10000, num_runs=1, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:27:58.244847Z",
     "start_time": "2023-10-13T19:27:56.385830Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Unique Features: ['PRFL_HEALTHCARE_Y', 'CH_1115MAL_Y', 'CH_1115FEM_Y', 'AUTOWORK_Y', 'HOME_DECOR_Y', 'MOTORCYCLE_Y', 'CHRISTFAM_Y', 'HUNTSHOOT_Y']\n",
      "Most Common Features: ['PRFL_BLM_SUPPORT_Y', 'PRFL_2NDAMEND_Y', 'PRFL_METOO_SUPPORT_Y', 'PRFL_GUN_CONTROL_Y', 'DON_POLLIB_Y', 'DON_POLCONS_Y', 'PRFL_HEALTHCARE_Y', 'CH_1115MAL_Y', 'CH_1115FEM_Y', 'AUTOWORK_Y', 'HOME_DECOR_Y', 'MOTORCYCLE_Y', 'CHRISTFAM_Y', 'HUNTSHOOT_Y']\n"
     ]
    }
   ],
   "source": [
    "# Assuming result is your DataFrame and 'Top_N_Features' is the column with the lists of top features\n",
    "all_features = [feature for sublist in result['Top_N_Features'].tolist() for feature in sublist]\n",
    "\n",
    "# Count the frequency of each feature\n",
    "feature_counts = Counter(all_features)\n",
    "\n",
    "# Find the most unique features (those that appear only once across all runs)\n",
    "most_unique_features = [feature for feature, count in feature_counts.items() if count == 1]\n",
    "\n",
    "# Find the most common features (those that appear the most across all runs)\n",
    "most_common_features = [feature for feature, count in feature_counts.most_common()]\n",
    "\n",
    "print(\"Most Unique Features:\", most_unique_features)\n",
    "print(\"Most Common Features:\", most_common_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T03:31:10.281815Z",
     "start_time": "2023-10-09T03:31:10.264332Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Just the non_numeric_cols with more than 2 values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(500000, 49)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced_with_more_than_two_categories = data_500[cols_with_more_than_two_categories].copy()\n",
    "data_reduced_with_more_than_two_categories.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T03:31:19.359617Z",
     "start_time": "2023-10-09T03:31:19.353181Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engineered_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 77\u001B[0m\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results_df\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Assuming engineered_data and cols_with_more_than_two_categories are already defined\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m result \u001B[38;5;241m=\u001B[39m run_xgboost(\u001B[43mengineered_data\u001B[49m, cols_with_more_than_two_categories, sample_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50000\u001B[39m, num_runs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, top_N_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'engineered_data' is not defined"
     ]
    }
   ],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer_decorator\n",
    "def run_xgboost(data, cols_with_more_than_two_categories, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting run {run}...\")\n",
    "        \n",
    "        data_reduced_with_more_than_two_categories = data[cols_with_more_than_two_categories].copy()\n",
    "        \n",
    "        # Using 'INFER_PARTY' as the target variable\n",
    "        data_reduced_with_more_than_two_categories['INFER_PARTY'] = data['INFER_PARTY']\n",
    "        \n",
    "        # Remove rows where 'INFER_PARTY' is missing, as it's our target variable\n",
    "        data_reduced_with_more_than_two_categories = data_reduced_with_more_than_two_categories[data_reduced_with_more_than_two_categories['INFER_PARTY'].notna()]\n",
    "\n",
    "        # Sample data\n",
    "        data_sample = data_reduced_with_more_than_two_categories.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Dynamic Class Handling\n",
    "        class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 1].index.tolist()\n",
    "        data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "        \n",
    "        cols_to_encode = [col for col in cols_with_more_than_two_categories if col in data_sample.columns and col != 'INFER_PARTY']\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_to_encode, drop_first=True)\n",
    "        \n",
    "        X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "        y = data_one_hot['INFER_PARTY']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        print(\"Training set class distribution:\\n\", y_train.value_counts())\n",
    "        print(\"Testing set class distribution:\\n\", y_test.value_counts())\n",
    "        \n",
    "        num_classes = len(np.unique(y_train))\n",
    "\n",
    "        xgb = XGBClassifier(objective='multi:softmax', num_class=num_classes, random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Run {run} - Accuracy: {accuracy}, F1 Score: {f1}, Recall: {recall}\")\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Assuming engineered_data and cols_with_more_than_two_categories are already defined\n",
    "result = run_xgboost(engineered_data, cols_with_more_than_two_categories, sample_size=50000, num_runs=2, top_N_features=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:12:41.606476Z",
     "start_time": "2023-10-10T03:12:41.227703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1     0.911  0.911058   0.911   \n1    2     0.911  0.911058   0.911   \n\n                                      Top_N_Features  \n0  [PARTY_CODE_R, ETHNIC_INFER_C, TOD_PRES_DIFF_2...  \n1  [PARTY_CODE_R, ETHNIC_INFER_C, TOD_PRES_DIFF_2...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.911</td>\n      <td>0.911058</td>\n      <td>0.911</td>\n      <td>[PARTY_CODE_R, ETHNIC_INFER_C, TOD_PRES_DIFF_2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.911</td>\n      <td>0.911058</td>\n      <td>0.911</td>\n      <td>[PARTY_CODE_R, ETHNIC_INFER_C, TOD_PRES_DIFF_2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result['Top_N_Features'][0]\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T03:32:50.093701Z",
     "start_time": "2023-10-09T03:32:50.084507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Unique Features: []\n",
      "Most Common Features: ['PARTY_CODE_R', 'ETHNIC_INFER_C', 'TOD_PRES_DIFF_2020_PREC_Unknown', 'CENSUS_ST_12', 'CENSUS_ST_42', 'ADD_TYPE_Unknown', 'CENSUS_ST_13', 'CENSUS_ST_34', 'FUND_POLIT_D', 'CENSUS_ST_06']\n"
     ]
    }
   ],
   "source": [
    "# Assuming result is your DataFrame and 'Top_N_Features' is the column with the lists of top features\n",
    "all_features = [feature for sublist in result['Top_N_Features'].tolist() for feature in sublist]\n",
    "\n",
    "# Count the frequency of each feature\n",
    "feature_counts = Counter(all_features)\n",
    "\n",
    "# Find the most unique features (those that appear only once across all runs)\n",
    "most_unique_features = [feature for feature, count in feature_counts.items() if count == 1]\n",
    "\n",
    "# Find the most common features (those that appear the most across all runs)\n",
    "most_common_features = [feature for feature, count in feature_counts.most_common()]\n",
    "\n",
    "print(\"Most Unique Features:\", most_unique_features)\n",
    "print(\"Most Common Features:\", most_common_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T03:32:58.739526Z",
     "start_time": "2023-10-09T03:32:58.729443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'PARTY_CODE' as 'N' or 'U': 273\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/Users/nirugidla/Documents/GitHub/milestone2_MADS/surveydata.csv'\n",
    "survey_data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a copy for corrections\n",
    "corrected_data = survey_data.copy()\n",
    "\n",
    "# Replace blanks and spaces with NaN\n",
    "corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# Update 'PARTY_CODE' based on certain conditions in 'STATE'\n",
    "states_to_update_for_U = ['DC', 'LA', 'RI']\n",
    "states_to_update_for_I = ['SD', 'OK']\n",
    "\n",
    "# Set 'PARTY_CODE' to 'N' for rows where 'STATE' is in the predefined lists and 'PARTY_CODE' is 'U' or 'I'\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update_for_U)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update_for_I)) & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Count the number of rows where 'PARTY_CODE' is 'N' or 'U'\n",
    "count_N_U = len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['N', 'U'])])\n",
    "\n",
    "print(f\"Number of rows with 'PARTY_CODE' as 'N' or 'U': {count_N_U}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T00:01:03.871298Z",
     "start_time": "2023-10-14T00:01:03.645992Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PARTY_CODE values:\n",
      "[nan 'R' 'D' 'N' 'O' 'L']\n",
      "Final INFER_PARTY values:\n",
      "['D' 'R' nan]\n",
      "Total INFER_PARTY Ds and Rs:\n",
      "1269\n",
      "INFER_PARTY NaNs:\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a copy to de-fragment the DataFrame\n",
    "corrected_data = corrected_data.copy()\n",
    "\n",
    "# Create party_mapping using defaultdict. Lambda sets default (all other non-D and non-R party registrations to NaN).\n",
    "infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "infer_party_mapping.update({\n",
    "    'D': 'D',\n",
    "    'E': 'D',\n",
    "    'R': 'R',\n",
    "    'S': 'R'\n",
    "})\n",
    "\n",
    "# Map PARTY_CODE to INFER_PARTY\n",
    "corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "\n",
    "# Define the mapping for PARTY_CODE modifications\n",
    "party_code_mapping = {\n",
    "    'E': float('nan'),\n",
    "    'S': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': 'O',\n",
    "    'B': 'O',\n",
    "    'C': 'O',\n",
    "    'F': 'O',\n",
    "    'G': 'O',\n",
    "    'H': 'O',\n",
    "    'I': 'O',\n",
    "    'J': 'O',\n",
    "    'K': 'O',\n",
    "    'L': 'L',\n",
    "    'P': 'O',\n",
    "    'Q': 'O',\n",
    "    'T': 'O',\n",
    "    'V': 'O',\n",
    "    'W': 'O',\n",
    "    'Y': 'O',\n",
    "    'Z': 'O'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the PARTY_CODE column\n",
    "corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "# The rest of your code\n",
    "print(\"Final PARTY_CODE values:\")\n",
    "print(corrected_data['PARTY_CODE'].unique())\n",
    "print('Final INFER_PARTY values:')\n",
    "print(corrected_data['INFER_PARTY'].unique())\n",
    "print('Total INFER_PARTY Ds and Rs:')\n",
    "print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "print('INFER_PARTY NaNs:')\n",
    "print(corrected_data['INFER_PARTY'].isna().sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T00:01:05.531032Z",
     "start_time": "2023-10-14T00:01:05.500680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of D's and R's in INFER_PARTY column: 1291\n",
      "Total number of NaNs in INFER_PARTY column: 269\n"
     ]
    }
   ],
   "source": [
    "# Create a de-fragmented copy of the DataFrame to avoid performance issues\n",
    "corrected_data = corrected_data.copy()\n",
    "\n",
    "# Identify columns starting with \"VTR\"\n",
    "voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "\n",
    "# Calculate total votes, Democrat votes, and Republican votes\n",
    "corrected_data['VTR_TOTAL_VOTES'] = corrected_data[voter_columns].notnull().sum(axis=1)\n",
    "corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "\n",
    "# Create conditions to infer party based on vote counts\n",
    "conditions = [\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2,\n",
    "    corrected_data['VTR_TOTAL_RVOTES'] - corrected_data['VTR_TOTAL_DVOTES'] >= 2,\n",
    "]\n",
    "choices = ['D', 'R']\n",
    "\n",
    "# Apply conditions to create a new column\n",
    "corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "corrected_data['VTR_INFER_PARTY'].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "# Update the INFER_PARTY column based on VTR_INFER_PARTY\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D'), 'INFER_PARTY'] = 'D'\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R'), 'INFER_PARTY'] = 'R'\n",
    "\n",
    "# Display statistics\n",
    "print(\"Total number of D's and R's in INFER_PARTY column:\", len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "print(\"Total number of NaNs in INFER_PARTY column:\", corrected_data['INFER_PARTY'].isna().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T00:03:58.804399Z",
     "start_time": "2023-10-14T00:03:58.770577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again\n",
    "survey_data = pd.read_csv('/Users/nirugidla/Documents/GitHub/milestone2_MADS/surveydata.csv')\n",
    "\n",
    "corrected_data=survey_data\n",
    "\n",
    "#Replace blanks and spaces with NaN\n",
    "corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "states_to_update = ['DC', 'LA', 'RI']\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['N', 'U'])]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T23:59:26.982804Z",
     "start_time": "2023-10-13T23:59:26.756011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PARTY_CODE values:\n",
      "[nan 'R' 'D' 'N' 'O' 'L']\n",
      "Final INFER_PARTY values:\n",
      "['D' 'R' nan]\n",
      "Total INFER_PARTY Ds and Rs:\n",
      "1269\n",
      "INFER_PARTY NaNs:\n",
      "291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/j0bcgw097y7f7wy0j1r4m67c0000gp/T/ipykernel_16138/495648273.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# Create party_mapping using defaultdict.Lambda sets default (all other non-D and non-R party registrations to NaN).\n",
    "infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "infer_party_mapping.update({\n",
    "    'D': 'D',  # Registered Democrats are mapped to Democrat\n",
    "    'E': 'D',  # E (Inferred Democrats) are mapped to Democrat\n",
    "    'R': 'R',  # Registered Republicans are mapped to Republican \n",
    "    'S': 'R'   # S (Inferred Republicans) are mapped to Republican\n",
    "})\n",
    "# Map PARTY_CODE to INFER_PARTY\n",
    "corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "\n",
    "# Define the mapping for PARTY_CODE modifications\n",
    "party_code_mapping = {\n",
    "    'E': float('nan'),\n",
    "    'S': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': 'O',\n",
    "    'B': 'O',\n",
    "    'C': 'O',\n",
    "    'F': 'O',\n",
    "    'G': 'O',\n",
    "    'H': 'O',\n",
    "    'I': 'O',\n",
    "    'J': 'O',\n",
    "    'K': 'O',\n",
    "    'L': 'L',\n",
    "    'P': 'O',\n",
    "    'Q': 'O',\n",
    "    'T': 'O',\n",
    "    'V': 'O',\n",
    "    'W': 'O',\n",
    "    'Y': 'O',\n",
    "    'Z': 'O'\n",
    "}\n",
    "# Apply the mapping to the PARTY_CODE column\n",
    "corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "print(\"Final PARTY_CODE values:\")\n",
    "print(corrected_data['PARTY_CODE'].unique())\n",
    "print('Final INFER_PARTY values:')\n",
    "print(corrected_data['INFER_PARTY'].unique())\n",
    "print('Total INFER_PARTY Ds and Rs:')\n",
    "print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "print('INFER_PARTY NaNs:')\n",
    "print(corrected_data['INFER_PARTY'].isna().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T23:59:27.705711Z",
     "start_time": "2023-10-13T23:59:27.681453Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of D's and R's in INFER_PARTY column:\n",
      "1280\n",
      "Total number of NaNs in INFER_PARTY column:\n",
      "280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/j0bcgw097y7f7wy0j1r4m67c0000gp/T/ipykernel_16138/3016435579.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  corrected_data['VTR_TOTAL_VOTES'] = corrected_data[voter_columns].notnull().sum(axis=1)\n",
      "/var/folders/gh/j0bcgw097y7f7wy0j1r4m67c0000gp/T/ipykernel_16138/3016435579.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
      "/var/folders/gh/j0bcgw097y7f7wy0j1r4m67c0000gp/T/ipykernel_16138/3016435579.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
      "/var/folders/gh/j0bcgw097y7f7wy0j1r4m67c0000gp/T/ipykernel_16138/3016435579.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n"
     ]
    }
   ],
   "source": [
    "voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "\n",
    "#Create column with total number of votes in voter_columns per row\n",
    "corrected_data['VTR_TOTAL_VOTES'] = corrected_data[voter_columns].notnull().sum(axis=1)\n",
    "\n",
    "#Sum Democrat and Republican totals\n",
    "corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "\n",
    "#Create conditions that evaluate whether someone has cast at least two more votes for Democrats or Republicans: \n",
    "conditions = [\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2,\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2\n",
    "]\n",
    "\n",
    "choices = ['D', 'R']\n",
    "\n",
    "corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "corrected_data['VTR_INFER_PARTY'].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "# Assign a 'D' or an 'R' to INFER_PARTY if either condition is true:\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D') , 'INFER_PARTY'] = 'D'\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R') , 'INFER_PARTY'] = 'R'\n",
    "\n",
    "print(\"Total number of D's and R's in INFER_PARTY column:\")\n",
    "print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D','R'])])) \n",
    "print(\"Total number of NaNs in INFER_PARTY column:\")\n",
    "print(sum(corrected_data['INFER_PARTY'].isna()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:54:16.318612Z",
     "start_time": "2023-10-13T20:54:16.285112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset again\n",
    "data = pd.read_csv('data/surveydata.csv')\n",
    "\n",
    "corrected_data=data\n",
    "\n",
    "#Replace blanks and spaces with NaN\n",
    "corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "states_to_update = ['DC', 'LA', 'RI']\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['N', 'U'])]))\n",
    "\n",
    "# Clean PARTY_CODE and create INFER_PARTY column indicating D or R for all rows, both inferred and real\n",
    "[543]:\n",
    "from collections import defaultdict\n",
    "# Create party_mapping using defaultdict.Lambda sets default (all other non-D and non-R party registrations to NaN).\n",
    "infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "infer_party_mapping.update({\n",
    "    'D': 'D',  # Registered Democrats are mapped to Democrat\n",
    "    'E': 'D',  # E (Inferred Democrats) are mapped to Democrat\n",
    "    'R': 'R',  # Registered Republicans are mapped to Republican \n",
    "    'S': 'R'   # S (Inferred Republicans) are mapped to Republican\n",
    "})\n",
    "# Map PARTY_CODE to INFER_PARTY\n",
    "corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "\n",
    "# Define the mapping for PARTY_CODE modifications\n",
    "party_code_mapping = {\n",
    "    'E': float('nan'),\n",
    "    'S': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': 'O',\n",
    "    'B': 'O',\n",
    "    'C': 'O',\n",
    "    'F': 'O',\n",
    "    'G': 'O',\n",
    "    'H': 'O',\n",
    "    'I': 'O',\n",
    "    'J': 'O',\n",
    "    'K': 'O',\n",
    "    'L': 'L',\n",
    "    'P': 'O',\n",
    "    'Q': 'O',\n",
    "    'T': 'O',\n",
    "    'V': 'O',\n",
    "    'W': 'O',\n",
    "    'Y': 'O',\n",
    "    'Z': 'O'\n",
    "}\n",
    "# Apply the mapping to the PARTY_CODE column\n",
    "corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "print(\"Final PARTY_CODE values:\")\n",
    "print(corrected_data['PARTY_CODE'].unique())\n",
    "print('Final INFER_PARTY values:')\n",
    "print(corrected_data['INFER_PARTY'].unique())\n",
    "print('Total INFER_PARTY Ds and Rs:')\n",
    "print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "print('INFER_PARTY NaNs:')\n",
    "print(corrected_data['INFER_PARTY'].isna().sum())\n",
    "\n",
    "voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "\n",
    "#Create column with total number of votes in voter_columns per row\n",
    "corrected_data['VTR_TOTAL_VOTES'] = corrected_data[voter_columns].notnull().sum(axis=1)\n",
    "\n",
    "#Sum Democrat and Republican totals\n",
    "corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "\n",
    "#Create conditions that evaluate whether someone has cast at least two more votes for Democrats or Republicans: \n",
    "conditions = [\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2,\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2\n",
    "]\n",
    "\n",
    "choices = ['D', 'R']\n",
    "\n",
    "corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "corrected_data['VTR_INFER_PARTY'].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "# Assign a 'D' or an 'R' to INFER_PARTY if either condition is true:\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D') , 'INFER_PARTY'] = 'D'\n",
    "corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R') , 'INFER_PARTY'] = 'R'\n",
    "\n",
    "print(\"Total number of D's and R's in INFER_PARTY column:\")\n",
    "print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D','R'])])) \n",
    "print(\"Total number of NaNs in INFER_PARTY column:\")\n",
    "print(sum(corrected_data['INFER_PARTY'].isna()))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
