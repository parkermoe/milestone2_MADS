{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T21:08:47.051826Z",
     "start_time": "2023-10-13T21:08:47.037430Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 500K dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "        RECORD_ID ADD_TYPE AFAMPROFLS   AGE        AI_COUNTY_NAME AIRCOND  \\\n0          403390        S             21.0  Fairbanks North Star           \n1           62285        H              NaN             Anchorage           \n2          331355                      91.0       Kenai Peninsula           \n3          206320        H             65.0             Anchorage           \n4          188078        S             76.0                Juneau           \n...           ...      ...        ...   ...                   ...     ...   \n499995     349635        H             20.0                  BIBB           \n499996     420654        S             50.0                COWETA       A   \n499997     131262        S             19.0              ROCKDALE           \n499998     315673        H             21.0                BARROW           \n499999     467477        S             19.0               CHATHAM           \n\n       APP_CHILD APP_MENBIG APP_TODDLR APP_WOMEN  ... VTR_PRI16 VTR_PRI17  \\\n0                                                 ...                       \n1                                                 ...                       \n2                                                 ...                       \n3                                                 ...                       \n4                                                 ...         Y             \n...          ...        ...        ...       ...  ...       ...       ...   \n499995                                            ...                       \n499996                                            ...                       \n499997                                            ...                       \n499998                                            ...                       \n499999                                            ...                       \n\n       VTR_PRI18 VTR_PRI19 VTR_PRI20 VTR_PRI21 VTR_PRI22 WORKWOMAN YEARBUILT  \\\n0                                                                        NaN   \n1                                                                        NaN   \n2                                                                        NaN   \n3                                                      Y                       \n4              Y                                       Y         Y      1985   \n...          ...       ...       ...       ...       ...       ...       ...   \n499995                                                                   NaN   \n499996                                                                  2003   \n499997                                                                         \n499998                                                                   NaN   \n499999                                                                   NaN   \n\n          ZIP  \n0       99705  \n1       99506  \n2       99603  \n3       99567  \n4       99801  \n...       ...  \n499995  31204  \n499996  30263  \n499997  30013  \n499998  30680  \n499999  31407  \n\n[500000 rows x 298 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RECORD_ID</th>\n      <th>ADD_TYPE</th>\n      <th>AFAMPROFLS</th>\n      <th>AGE</th>\n      <th>AI_COUNTY_NAME</th>\n      <th>AIRCOND</th>\n      <th>APP_CHILD</th>\n      <th>APP_MENBIG</th>\n      <th>APP_TODDLR</th>\n      <th>APP_WOMEN</th>\n      <th>...</th>\n      <th>VTR_PRI16</th>\n      <th>VTR_PRI17</th>\n      <th>VTR_PRI18</th>\n      <th>VTR_PRI19</th>\n      <th>VTR_PRI20</th>\n      <th>VTR_PRI21</th>\n      <th>VTR_PRI22</th>\n      <th>WORKWOMAN</th>\n      <th>YEARBUILT</th>\n      <th>ZIP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>403390</td>\n      <td>S</td>\n      <td></td>\n      <td>21.0</td>\n      <td>Fairbanks North Star</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99705</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62285</td>\n      <td>H</td>\n      <td></td>\n      <td>NaN</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99506</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>331355</td>\n      <td></td>\n      <td></td>\n      <td>91.0</td>\n      <td>Kenai Peninsula</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99603</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>206320</td>\n      <td>H</td>\n      <td></td>\n      <td>65.0</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td>99567</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188078</td>\n      <td>S</td>\n      <td></td>\n      <td>76.0</td>\n      <td>Juneau</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>Y</td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1985</td>\n      <td>99801</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>349635</td>\n      <td>H</td>\n      <td></td>\n      <td>20.0</td>\n      <td>BIBB</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31204</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>420654</td>\n      <td>S</td>\n      <td></td>\n      <td>50.0</td>\n      <td>COWETA</td>\n      <td>A</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>2003</td>\n      <td>30263</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>131262</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>ROCKDALE</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>30013</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>315673</td>\n      <td>H</td>\n      <td></td>\n      <td>21.0</td>\n      <td>BARROW</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>30680</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>467477</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>CHATHAM</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31407</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows Ã— 298 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "data_500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:05.929422Z",
     "start_time": "2023-10-13T21:08:47.434173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "remove_columns = [\n",
    "    'PRFL_LGBT_SUPPORT',\n",
    "    'PRFL_LIBERAL_NEWS',\n",
    "    'PRFL_MARIJUANA_REFORM',\n",
    "    'PRFL_BIDEN_SUPPORT',\n",
    "    'PRFL_BORDER_SECURITY',\n",
    "    'PRFL_CONSERVATIVE_NEWS',\n",
    "    'PRFL_IMMIGRATION_REFORM',\n",
    "    'PRFL_OBAMA',\n",
    "    'PRFL_PERSUADABLE_VOTER',\n",
    "    'PRFL_POLITICAL_IDEOLOGY',\n",
    "    'PRFL_SANDERS_SUPPORT',\n",
    "    'PRFL_TRUMP_SUPPORT',\n",
    "    'ZIP',\n",
    "    \n",
    "    'VTR_GEN00', 'VTR_GEN01', 'VTR_GEN02', 'VTR_GEN03', 'VTR_GEN04', 'VTR_GEN05', 'VTR_GEN06', 'VTR_GEN07', 'VTR_GEN08', 'VTR_GEN09', 'VTR_GEN10', 'VTR_GEN11', 'VTR_GEN12', 'VTR_GEN13', 'VTR_GEN14', 'VTR_GEN15', 'VTR_GEN16', 'VTR_GEN17', 'VTR_GEN18', 'VTR_GEN19', 'VTR_GEN20', 'VTR_GEN21', 'VTR_GEN22', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PPP08', 'VTR_PPP12', 'VTR_PPP16', 'VTR_PPP20', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI10', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI14', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI18', 'VTR_PRI19', 'VTR_PRI20', 'VTR_PRI21', 'VTR_PRI22',\n",
    "    \n",
    "        \n",
    "      'PRFL_CHOICELIFE', 'TOD_PRES_D_2016_PREC', 'TOD_PRES_O_2016',\n",
    "    'TOD_PRES_R_2016', 'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'VP_PPP',\n",
    "    'AGE', 'CNSUS_PCTW',\n",
    "    \n",
    "    'PARTY_MIX', 'PRFL_MINWAGE', 'PRFL_FENCE_SITTER', \n",
    "]\n",
    "# Drop the list of columns from the dataset\n",
    "data_500.drop(columns=remove_columns, errors='ignore', inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:07.223581Z",
     "start_time": "2023-10-13T21:09:06.779591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'R' 'O' 'D' 'A' 'F' 'P' 'G' 'L' 'U' 'W' 'B' 'I' 'Y' 'V' 'H' ' ' 'S'\n",
      " 'E' 'Q' 'Z']\n",
      "unique_infer_party:\n",
      "[nan 'R' 'D']\n",
      "unique_party_code_after_modifications:\n",
      "['N' 'R' 'O' 'D' 'L' nan ' ']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again\n",
    "#data = pd.read_csv('data/surveydata.csv')\n",
    "\n",
    "corrected_data = data_500.copy()\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "states_to_update = ['DC', 'LA', 'RI']\n",
    "corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "# Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "\n",
    "#categorical_columns\n",
    "# Create a mapping dictionary for PARTY_CODE to INFER_PARTY values\n",
    "party_mapping = {\n",
    "    'D': 'D',\n",
    "    'E': 'D',\n",
    "    'R': 'R',\n",
    "    'S': 'R',\n",
    "    'N': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': float('nan'),\n",
    "    'B': float('nan'),\n",
    "    'C': float('nan'),\n",
    "    'F': float('nan'),\n",
    "    'G': float('nan'),\n",
    "    'H': float('nan'),\n",
    "    'I': float('nan'),\n",
    "    'J': float('nan'),\n",
    "    'K': float('nan'),\n",
    "    'L': float('nan'),\n",
    "    'P': float('nan'),\n",
    "    'Q': float('nan'),\n",
    "    'T': float('nan'),\n",
    "    'V': float('nan'),\n",
    "    'W': float('nan'),\n",
    "    'Y': float('nan'),\n",
    "    'Z': float('nan'),\n",
    "    'O': float('nan'),\n",
    "}\n",
    "\n",
    "# Create the INFER_PARTY column using the mapping\n",
    "corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(party_mapping)\n",
    "\n",
    "print(corrected_data['PARTY_CODE'].unique())\n",
    "\n",
    "\n",
    "# Display the unique values in the INFER_PARTY column to ensure correctness\n",
    "unique_infer_party = corrected_data['INFER_PARTY'].unique()\n",
    "\n",
    "print('unique_infer_party:')\n",
    "\n",
    "print(unique_infer_party)\n",
    "\n",
    "# Define the mapping for PARTY_CODE modifications\n",
    "party_code_mapping = {\n",
    "    'E': float('nan'),\n",
    "    'S': float('nan'),\n",
    "    'U': float('nan'),\n",
    "    'A': 'O',\n",
    "    'B': 'O',\n",
    "    'C': 'O',\n",
    "    'F': 'O',\n",
    "    'G': 'O',\n",
    "    'H': 'O',\n",
    "    'I': 'O',\n",
    "    'J': 'O',\n",
    "    'K': 'O',\n",
    "    'L': 'L',\n",
    "    'P': 'O',\n",
    "    'Q': 'O',\n",
    "    'T': 'O',\n",
    "    'V': 'O',\n",
    "    'W': 'O',\n",
    "    'Y': 'O',\n",
    "    'Z': 'O'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the PARTY_CODE column\n",
    "corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "# Check the unique values of PARTY_CODE after the modifications\n",
    "unique_party_code_after_modifications = corrected_data['PARTY_CODE'].unique()\n",
    "\n",
    "print(\"unique_party_code_after_modifications:\")\n",
    "\n",
    "print(unique_party_code_after_modifications)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:08.964567Z",
     "start_time": "2023-10-13T21:09:07.227778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['N', 'R', 'O', 'D', 'A', 'F', 'P', 'G', 'L', 'U', 'W', 'B', 'I',\n       'Y', 'V', 'H', ' ', 'S', 'E', 'Q', 'Z'], dtype=object)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500['PARTY_CODE'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:08.978957Z",
     "start_time": "2023-10-13T21:09:08.971114Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154352\n",
      "154352\n"
     ]
    }
   ],
   "source": [
    "engineered_data = corrected_data.copy()\n",
    "voter_columns = [col for col in engineered_data.columns if col.startswith(\"VTR\")]\n",
    "#Create column with total number of votes in voter_columns per row\n",
    "engineered_data['VTR_TOTAL_VOTES'] = engineered_data[voter_columns].notnull().sum(axis=1)\n",
    "#Sum Democrat and Republican totals\n",
    "engineered_data['VTR_TOTAL_DVOTES'] = engineered_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "engineered_data['VTR_TOTAL_RVOTES'] = engineered_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "#Create conditions that evaluate whether more votes have been cast for Democrats or Republicans, and assign 'D' and 'R' to new column based on conditions\n",
    "conditions = [\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] > engineered_data['VTR_TOTAL_RVOTES'],\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] < engineered_data['VTR_TOTAL_RVOTES']\n",
    "]\n",
    "choices = ['D', 'R']\n",
    "engineered_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "#Create conditions that evaluate whether a voter is a swing voter or not\n",
    "conditions_swing = [\n",
    "    (engineered_data['VTR_TOTAL_DVOTES'] > 2) & (engineered_data['VTR_TOTAL_RVOTES'] > 2),\n",
    "    ((engineered_data['VTR_TOTAL_DVOTES'] > 1) & (engineered_data['VTR_TOTAL_RVOTES'] == 0)) | ((engineered_data['VTR_TOTAL_RVOTES'] > 1) & (engineered_data['VTR_TOTAL_DVOTES'] == 0))\n",
    "]\n",
    "choices_swing = ['Y', 'N']\n",
    "engineered_data['VTR_INFER_SWING'] = np.select(conditions_swing, choices_swing, default=np.nan)\n",
    "#sampledf = engineered_data[['PARTY_CODE','INFER_PARTY','VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY','VTR_INFER_SWING']]\n",
    "#print(sampledf.head(50))\n",
    "#Add values to INFER_PARTY and correct any other INFER_PARTY values that don't meet the conditions above:\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'D'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'R'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "#Also delete any 'D' or 'R' INFER_PARTY labels for anyone deemed a \"swing voter\" based on criteria above of voting for both parties at least 3 times each:\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_SWING'] == 'Y') , 'INFER_PARTY'] = float('nan')\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "#Drop auxiliary columns used for math, keeping 'VTR_TOTAL_VOTES', 'VTR_INFER_SWING', and the updated 'INFER_PARTY' columns as features:\n",
    "engineered_data = engineered_data.drop(columns=['VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY'])\n",
    "# Drop the list of columns from the dataset\n",
    "#engineered_data_cleaned = engineered_data.drop(columns=remove_columns, errors='ignore')\n",
    "\n",
    "features_to_remove = [\n",
    "    'PRFL_CHOICELIFE', 'TOD_PRES_D_2016_PREC', 'TOD_PRES_O_2016',\n",
    "    'TOD_PRES_R_2016', 'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'VP_PPP',\n",
    "    'AGE', 'CNSUS_PCTW'\n",
    "]\n",
    "# Assuming 'engineered_data' is your DataFrame, remove the less important features\n",
    "engineered_data = engineered_data.drop(columns=features_to_remove, errors='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:12.431697Z",
     "start_time": "2023-10-13T21:09:08.976540Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "#data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning - Column Names\n",
    "data_500.columns = data_500.columns.str.strip()\n",
    "\n",
    "# Data Cleaning - Drop ZIP\n",
    "#data_500.drop('ZIP', axis=1, inplace=True)\n",
    "# Drop the list of columns from the dataset\n",
    "data_500_cleaned = data_500.drop(columns=remove_columns, errors='ignore')\n",
    "\n",
    "# Data Cleaning - Drop Duplicates\n",
    "data_500.drop_duplicates(inplace=True)\n",
    "\n",
    "# Data Cleaning - Object Columns\n",
    "for col in data_500.columns:\n",
    "    if data_500[col].dtype == 'object':\n",
    "        data_500[col] = data_500[col].str.strip()\n",
    "\n",
    "# Data Cleaning - Empty Strings\n",
    "data_500.replace('', 'Unknown', inplace=True)\n",
    "\n",
    "# Data Cleaning - NaN for Object Types\n",
    "data_500.loc[:, data_500.dtypes == 'object'] = data_500.loc[:, data_500.dtypes == 'object'].fillna('Unknown')\n",
    "\n",
    "# Data Cleaning - Drop Columns and Rows with All NaNs\n",
    "data_500.dropna(axis=1, how='all', inplace=True)\n",
    "data_500.dropna(axis=0, how='all', inplace=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:36.135648Z",
     "start_time": "2023-10-13T21:09:14.136354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with 'Y' or 'Unknown': ['AFAMPROFLS', 'APP_CHILD', 'APP_MENBIG', 'APP_TODDLR', 'APP_WOMEN', 'APP_WOMPET', 'APP_WOMPLS', 'APP_YNGMEN', 'ARTS', 'AUTOACCES', 'AUTOWORK', 'BOATING', 'BROADERLIV', 'CARDUSER', 'CATOWNER', 'CH_0002FEM', 'CH_0002MAL', 'CH_0002UNK', 'CH_0305FEM', 'CH_0305MAL', 'CH_0305UNK', 'CH_0610FEM', 'CH_0610MAL', 'CH_0610UNK', 'CH_1115FEM', 'CH_1115MAL', 'CH_1115UNK', 'CH_1617FEM', 'CH_1617MAL', 'CH_1617UNK', 'CHRISTFAM', 'COL_ANTIQ', 'COL_ARTS', 'COL_COIN', 'COL_SPORT', 'COL_STAMP', 'COMPHOMOFC', 'COMPUTERS', 'COOK_GEN', 'CURRAFFAIR', 'DEPTSTCRD', 'DIETING', 'DIYLIV', 'DOGOWNER', 'DON_ANML', 'DON_ARTCUL', 'DON_CHARIT', 'DON_CHILD', 'DON_ENVIR', 'DON_ENVWLD', 'DON_HEALTH', 'DON_INTAID', 'DON_OTHER', 'DON_POLCONS', 'DON_POLIT', 'DON_POLLIB', 'DON_RELIG', 'DON_VET', 'DONATION', 'EDU_ONLINE', 'EQUESTRIAN', 'EXER_GROUP', 'GAMING', 'GARDENER', 'GOLF', 'GRANDCHLD', 'HEALTHBEAU', 'HEATHMED', 'HH_SENIOR', 'HH_VETERAN', 'HH_YOUNGAD', 'HIGHBROW', 'HIGHENDAPP', 'HISTMIL', 'HITECHLIV', 'HOME_DECOR', 'HOMEOFFICE', 'HUNTING', 'HUNTSHOOT', 'INV_ACTIVE', 'MAIL_DONOR', 'MAILORDBUY', 'MAILORDRSP', 'MOTORCYCLE', 'NASCAR', 'PARENTING', 'PETS', 'PRESENCHLD', 'PRFL_2NDAMEND', 'PRFL_ACTIVE_MIL', 'PRFL_AMZN_PRIME', 'PRFL_ANML_RIGHTS', 'PRFL_BLM_SUPPORT', 'PRFL_CLINTON_SUPPORT', 'PRFL_EDUCATION', 'PRFL_ENVIRONMENT', 'PRFL_EVANGELICAL', 'PRFL_FENCE_SITTER', 'PRFL_GUN_CONTROL', 'PRFL_HEALTHCARE', 'PRFL_INFLUENCER', 'PRFL_INSURANCE', 'PRFL_LABOR', 'PRFL_METOO_SUPPORT', 'PRFL_MIL_SUPPORT', 'PRFL_TAXES', 'PRFL_TEACHERS_UNION', 'PRFL_VETERAN', 'RD_FINNEWS', 'RD_GEN', 'RD_RELIG', 'RD_SCIFI', 'RELIGINSP', 'SCISPACE', 'SCUBADIVER', 'SELFIMP', 'SINGPARENT', 'SMOKING', 'SPEC_AUTO', 'SPEC_BASE', 'SPEC_BASK', 'SPEC_FOOT', 'SPEC_HOCK', 'SPEC_SOCC', 'SPORTLEIS', 'SWEEPSTAKE', 'TELECOM', 'TENNIS', 'THEATER', 'TRAVEL', 'WORKWOMAN'] 131\n",
      "Columns with more than two categories: ['ADD_TYPE', 'AI_COUNTY_NAME', 'AIRCOND', 'ASSMLCODE', 'BUS_OWNER', 'CENSUS_ST', 'CNS_MEDINC', 'CONG_DIST', 'COUNTY_ST', 'COUNTY_TYPE', 'CRD_RANGE', 'CREDRATE', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE', 'ETHNICCONF', 'ETHNICGRP', 'FUND_POLIT', 'GENDER_MIX', 'GENERATION', 'HH_NUMGEN', 'HH_SIZE', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT', 'INCOMESTHH', 'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'NETWORTH', 'NUMCHILD', 'OCCDETAIL', 'OCCUPATION', 'PARTY_CODE', 'PERSONS_HH', 'POOL', 'PRFL_TEAPARTY', 'RELIGION', 'SEX', 'ST_LO_HOUS', 'ST_UP_HOUS', 'STATE', 'STATUS', 'TOD_PRES_DIFF_2016', 'TOD_PRES_DIFF_2016_PREC', 'TOD_PRES_DIFF_2020_PREC', 'VOTER_CNT', 'VOTER_TRLR', 'YEARBUILT'] 49\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = data_500.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = data_500.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Data Cleaning - Removing Non-Numeric Columns with More Than 90% Missing Data\n",
    "missing_data_percentage = data_500.isnull().mean() * 100\n",
    "non_numeric_cols_to_remove = missing_data_percentage[non_numeric_cols]\n",
    "non_numeric_cols_to_remove = non_numeric_cols_to_remove[non_numeric_cols_to_remove > 90].index.tolist()\n",
    "data_500_reduced = data_500.drop(columns=non_numeric_cols_to_remove)\n",
    "\n",
    "# Update the list of non-numeric columns after removal\n",
    "non_numeric_cols = data_500_reduced.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Identifying Specific Types of Non-Numeric Columns\n",
    "cols_with_Y_or_Unknown = [col for col in non_numeric_cols if set(data_500_reduced[col].unique()) <= {'Y', 'Unknown'}]\n",
    "cols_with_more_than_two_categories = [col for col in non_numeric_cols if len(data_500_reduced[col].unique()) > 2]\n",
    "\n",
    "# Print identified columns\n",
    "print(\"Columns with 'Y' or 'Unknown':\", cols_with_Y_or_Unknown, len(cols_with_Y_or_Unknown))\n",
    "print(\"Columns with more than two categories:\", cols_with_more_than_two_categories, len(cols_with_more_than_two_categories))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:43.592092Z",
     "start_time": "2023-10-13T21:09:36.134801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:43.595639Z",
     "start_time": "2023-10-13T21:09:43.593416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTY_CODE\n",
      "D    110365\n",
      "R     85218\n",
      "N     75028\n",
      "O      8121\n",
      "L      1621\n",
      "         45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(engineered_data['PARTY_CODE'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:43.607276Z",
     "start_time": "2023-10-13T21:09:43.603410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class Distribution for PARTY_CODE: Counter({'D': 110365, 'R': 85218, nan: 78296, 'N': 75028, nan: 71769, nan: 69537, 'O': 8121, 'L': 1621})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 837, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 873, in partial_fit\n    X = self._validate_data(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'S'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 837, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 873, in partial_fit\n    X = self._validate_data(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'H'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 23\u001B[0m\n\u001B[1;32m     17\u001B[0m pipeline_party \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[1;32m     18\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscaler\u001B[39m\u001B[38;5;124m'\u001B[39m, StandardScaler()),\n\u001B[1;32m     19\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxgboost\u001B[39m\u001B[38;5;124m'\u001B[39m, XGBClassifier())\n\u001B[1;32m     20\u001B[0m ])\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Cross-Validation for 'PARTY_CODE'\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m scores_party \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline_party\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_party\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_party\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCross-Validation Scores for PARTY_CODE:\u001B[39m\u001B[38;5;124m\"\u001B[39m, scores_party)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Fit the model for 'PARTY_CODE'\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py:562\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    560\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 562\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    209\u001B[0m         )\n\u001B[1;32m    210\u001B[0m     ):\n\u001B[0;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    221\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py:328\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    308\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m    309\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    310\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    311\u001B[0m         clone(estimator),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[1;32m    326\u001B[0m )\n\u001B[0;32m--> 328\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(scoring):\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py:414\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[0;34m(results, error_score)\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[1;32m    408\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    409\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    412\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    413\u001B[0m     )\n\u001B[0;32m--> 414\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    417\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 837, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 873, in partial_fit\n    X = self._validate_data(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'S'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 837, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/preprocessing/_data.py\", line 873, in partial_fit\n    X = self._validate_data(\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/base.py\", line 604, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/Users/nirugidla/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'H'\n"
     ]
    }
   ],
   "source": [
    "# Remove or replace records where 'PARTY_CODE' is \" \" (white spaces)\n",
    "engineered_data = engineered_data[engineered_data['PARTY_CODE'] != \" \"]\n",
    "\n",
    "# Class distribution for 'PARTY_CODE'\n",
    "print(\"Original Class Distribution for PARTY_CODE:\", Counter(engineered_data['PARTY_CODE']))\n",
    "\n",
    "# Features and Target for 'PARTY_CODE'\n",
    "X_party = engineered_data.drop(columns=['PARTY_CODE', 'INFER_PARTY'])\n",
    "y_party = engineered_data['PARTY_CODE']\n",
    "\n",
    "# Train-Test Split for 'PARTY_CODE'\n",
    "X_train_party, X_test_party, y_train_party, y_test_party = train_test_split(\n",
    "    X_party, y_party, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model Pipeline for 'PARTY_CODE'\n",
    "pipeline_party = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgboost', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Cross-Validation for 'PARTY_CODE'\n",
    "scores_party = cross_val_score(pipeline_party, X_train_party, y_train_party, cv=5)\n",
    "print(\"Cross-Validation Scores for PARTY_CODE:\", scores_party)\n",
    "\n",
    "# Fit the model for 'PARTY_CODE'\n",
    "pipeline_party.fit(X_train_party, y_train_party)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:09:57.888700Z",
     "start_time": "2023-10-13T21:09:43.619030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_party.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-13T21:09:57.887651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Columns - Dealing with Imbalanced Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 'PARTY_CODE' as target variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def preprocess_data(engineered_data, sample_size=10000):\n",
    "    # Copy the data\n",
    "    data_with_all_columns = engineered_data.copy()\n",
    "    \n",
    "    # Remove rows where the target column 'PARTY_CODE' is NaN\n",
    "    data_with_all_columns = data_with_all_columns[data_with_all_columns['PARTY_CODE'].notna()]\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data_with_all_columns.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Keep only the classes that have at least 2 samples\n",
    "    class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "    data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "    \n",
    "    # Label encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "    \n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns.difference(['PARTY_CODE'])\n",
    "    non_numeric_cols = data_sample.select_dtypes(exclude=['int64', 'float64']).columns.difference(['PARTY_CODE'])\n",
    "    \n",
    "    # One-hot encode non-numeric columns\n",
    "    data_one_hot = pd.get_dummies(data_sample, columns=non_numeric_cols, drop_first=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "    y = data_one_hot['PARTY_CODE']\n",
    "    \n",
    "    # Handle special characters in column names that XGBoost doesn't like\n",
    "    X.columns = X.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Usage\n",
    "# Assuming engineered_data is already defined\n",
    "X_train, X_test, y_train, y_test = preprocess_data(engineered_data, sample_size=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T04:52:17.315040Z",
     "start_time": "2023-10-13T04:52:07.376600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 'INFER_PARTY' as target variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(engineered_data, sample_size=10000):\n",
    "    # Copy the data\n",
    "    data_with_all_columns = engineered_data.copy()\n",
    "    \n",
    "    # Remove rows where the target column 'INFER_PARTY' is NaN\n",
    "    data_with_all_columns = data_with_all_columns[data_with_all_columns['INFER_PARTY'].notna()]\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data_with_all_columns.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Keep only the classes that have at least 2 samples\n",
    "    class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "    data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "    \n",
    "    # Label encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "    \n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns.difference(['INFER_PARTY'])\n",
    "    non_numeric_cols = data_sample.select_dtypes(exclude=['int64', 'float64']).columns.difference(['INFER_PARTY'])\n",
    "    \n",
    "    # One-hot encode non-numeric columns\n",
    "    data_one_hot = pd.get_dummies(data_sample, columns=non_numeric_cols, drop_first=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "    y = data_one_hot['INFER_PARTY']\n",
    "    \n",
    "    # Handle special characters in column names that XGBoost doesn't like\n",
    "    X.columns = X.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Usage\n",
    "# Assuming engineered_data is already defined\n",
    "X_train, X_test, y_train, y_test = preprocess_data(engineered_data, sample_size=10000)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resampling Techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.925\n",
      "Recall: 0.925\n",
      "F1 Score: 0.9128656777493607\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Determine the smallest class count\n",
    "min_class_count = np.min(np.bincount(y_train))\n",
    "\n",
    "# Apply SMOTE to the imputed training data\n",
    "# Set n_neighbors to min_class_count - 1 or a default small number\n",
    "n_neighbors = min(min_class_count - 1, 5)\n",
    "smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "X_res, y_res = smote.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "# Make predictions on the imputed test set\n",
    "y_pred = xgb.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:02:38.164714Z",
     "start_time": "2023-10-13T05:00:22.327485Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.413\n",
      "Recall: 0.413\n",
      "F1 Score: 0.484446515639784\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Apply RandomUnderSampler to the imputed training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "# Make predictions on the imputed test set\n",
    "y_pred = xgb.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:13:47.516746Z",
     "start_time": "2023-10-13T05:13:44.438174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Weighted Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9145\n",
      "Recall: 0.9145\n",
      "F1 Score: 0.9094697333354964\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each class in the target variable\n",
    "counter = Counter(y_train)\n",
    "\n",
    "# Calculate the number of samples\n",
    "total_samples = len(y_train)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = {cls: float(total_samples / count) for cls, count in counter.items()}\n",
    "\n",
    "# Map the weights to each sample in y_train\n",
    "sample_weights = [class_weights[cls] for cls in y_train]\n",
    "\n",
    "# Initialize XGBoost with multi:softmax objective\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Fit the model, passing in the sample weights\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:17:53.915650Z",
     "start_time": "2023-10-13T05:17:32.911041Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cost-sensitive Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': {'R': 2.0795453623443604e-05, 'N': 2.2389200977768253e-05, 'O': 0.00013219838820189577, 'L': 0.0006259639897705816, '  ': 0.02223128306578676}, 'R': {'D': 2.0795453623443604e-05, 'N': 2.5062967472140136e-05, 'O': 0.00013487215469626767, 'L': 0.0006286377562649534, '  ': 0.022233956832281132}, 'N': {'D': 2.2389200977768253e-05, 'R': 2.5062967472140136e-05, 'O': 0.00013646590205059232, 'L': 0.0006302315036192781, '  ': 0.022235550579635454}, 'O': {'D': 0.00013219838820189577, 'R': 0.00013487215469626767, 'N': 0.00013646590205059232, 'L': 0.0007400406908434056, '  ': 0.02234535976685958}, 'L': {'D': 0.0006259639897705816, 'R': 0.0006286377562649534, 'N': 0.0006302315036192781, 'O': 0.0007400406908434056, '  ': 0.02283912536842827}, '  ': {'D': 0.02223128306578676, 'R': 0.022233956832281132, 'N': 0.022235550579635454, 'O': 0.02234535976685958, 'L': 0.02283912536842827}}\n"
     ]
    }
   ],
   "source": [
    "def create_cost_matrix(y):\n",
    "    # Count the frequency of each class in the target variable\n",
    "    class_freq = Counter(y)\n",
    "    \n",
    "    # Calculate the inverse frequency\n",
    "    inv_freq = {k: 1.0 / v for k, v in class_freq.items()}\n",
    "    \n",
    "    # Create the cost matrix\n",
    "    cost_matrix = {}\n",
    "    for class1 in class_freq.keys():\n",
    "        cost_matrix[class1] = {}\n",
    "        for class2 in class_freq.keys():\n",
    "            if class1 == class2:\n",
    "                continue\n",
    "            cost_matrix[class1][class2] = inv_freq[class1] + inv_freq[class2]\n",
    "            \n",
    "    return cost_matrix\n",
    "\n",
    "# Assuming y_train contains your training labels, and it's a pandas Series\n",
    "# For demonstration, I'm using the value_counts information you provided\n",
    "y_train_demo = ['D'] * 110365 + ['R'] * 85218 + ['N'] * 75028 + ['O'] * 8121 + ['L'] * 1621 + ['  '] * 45\n",
    "cost_matrix = create_cost_matrix(y_train_demo)\n",
    "\n",
    "print(cost_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:29:52.378410Z",
     "start_time": "2023-10-13T05:29:52.348178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sample_weight\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming y_train is your actual training labels and it's a NumPy array\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# For demonstration, converting y_train_demo to a NumPy array\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m y_train_demo_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43my_train_demo\u001B[49m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Compute sample weights\u001B[39;00m\n\u001B[1;32m     15\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m compute_custom_sample_weight(cost_matrix, y_train_demo_np)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_train_demo' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_custom_sample_weight(cost_matrix, y):\n",
    "    sample_weight = np.zeros(len(y))\n",
    "    \n",
    "    for i, class1 in enumerate(y):\n",
    "        for class2, cost in cost_matrix[class1].items():\n",
    "            sample_weight[i] += (y == class2).sum() * cost\n",
    "            \n",
    "    return sample_weight\n",
    "\n",
    "# Assuming y_train is your actual training labels and it's a NumPy array\n",
    "# For demonstration, converting y_train_demo to a NumPy array\n",
    "y_train_demo_np = np.array(y_train_demo)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weight = compute_custom_sample_weight(cost_matrix, y_train_demo_np)\n",
    "\n",
    "# Fit the model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Rest of the code for predictions and metrics...\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T21:08:39.093153Z",
     "start_time": "2023-10-13T21:08:39.009797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTY_CODE\n",
      "D    110365\n",
      "R     85218\n",
      "N     75028\n",
      "O      8121\n",
      "L      1621\n",
      "         45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(engineered_data['PARTY_CODE'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-13T05:27:05.802624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
