{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T15:40:44.559106Z",
     "start_time": "2023-10-13T15:40:44.550457Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 500K dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "        RECORD_ID ADD_TYPE AFAMPROFLS   AGE        AI_COUNTY_NAME AIRCOND  \\\n0          403390        S             21.0  Fairbanks North Star           \n1           62285        H              NaN             Anchorage           \n2          331355                      91.0       Kenai Peninsula           \n3          206320        H             65.0             Anchorage           \n4          188078        S             76.0                Juneau           \n...           ...      ...        ...   ...                   ...     ...   \n499995     349635        H             20.0                  BIBB           \n499996     420654        S             50.0                COWETA       A   \n499997     131262        S             19.0              ROCKDALE           \n499998     315673        H             21.0                BARROW           \n499999     467477        S             19.0               CHATHAM           \n\n       APP_CHILD APP_MENBIG APP_TODDLR APP_WOMEN  ... VTR_PRI16 VTR_PRI17  \\\n0                                                 ...                       \n1                                                 ...                       \n2                                                 ...                       \n3                                                 ...                       \n4                                                 ...         Y             \n...          ...        ...        ...       ...  ...       ...       ...   \n499995                                            ...                       \n499996                                            ...                       \n499997                                            ...                       \n499998                                            ...                       \n499999                                            ...                       \n\n       VTR_PRI18 VTR_PRI19 VTR_PRI20 VTR_PRI21 VTR_PRI22 WORKWOMAN YEARBUILT  \\\n0                                                                        NaN   \n1                                                                        NaN   \n2                                                                        NaN   \n3                                                      Y                       \n4              Y                                       Y         Y      1985   \n...          ...       ...       ...       ...       ...       ...       ...   \n499995                                                                   NaN   \n499996                                                                  2003   \n499997                                                                         \n499998                                                                   NaN   \n499999                                                                   NaN   \n\n          ZIP  \n0       99705  \n1       99506  \n2       99603  \n3       99567  \n4       99801  \n...       ...  \n499995  31204  \n499996  30263  \n499997  30013  \n499998  30680  \n499999  31407  \n\n[500000 rows x 298 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RECORD_ID</th>\n      <th>ADD_TYPE</th>\n      <th>AFAMPROFLS</th>\n      <th>AGE</th>\n      <th>AI_COUNTY_NAME</th>\n      <th>AIRCOND</th>\n      <th>APP_CHILD</th>\n      <th>APP_MENBIG</th>\n      <th>APP_TODDLR</th>\n      <th>APP_WOMEN</th>\n      <th>...</th>\n      <th>VTR_PRI16</th>\n      <th>VTR_PRI17</th>\n      <th>VTR_PRI18</th>\n      <th>VTR_PRI19</th>\n      <th>VTR_PRI20</th>\n      <th>VTR_PRI21</th>\n      <th>VTR_PRI22</th>\n      <th>WORKWOMAN</th>\n      <th>YEARBUILT</th>\n      <th>ZIP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>403390</td>\n      <td>S</td>\n      <td></td>\n      <td>21.0</td>\n      <td>Fairbanks North Star</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99705</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62285</td>\n      <td>H</td>\n      <td></td>\n      <td>NaN</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99506</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>331355</td>\n      <td></td>\n      <td></td>\n      <td>91.0</td>\n      <td>Kenai Peninsula</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>99603</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>206320</td>\n      <td>H</td>\n      <td></td>\n      <td>65.0</td>\n      <td>Anchorage</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td>99567</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188078</td>\n      <td>S</td>\n      <td></td>\n      <td>76.0</td>\n      <td>Juneau</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>Y</td>\n      <td></td>\n      <td>Y</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1985</td>\n      <td>99801</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499995</th>\n      <td>349635</td>\n      <td>H</td>\n      <td></td>\n      <td>20.0</td>\n      <td>BIBB</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31204</td>\n    </tr>\n    <tr>\n      <th>499996</th>\n      <td>420654</td>\n      <td>S</td>\n      <td></td>\n      <td>50.0</td>\n      <td>COWETA</td>\n      <td>A</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>2003</td>\n      <td>30263</td>\n    </tr>\n    <tr>\n      <th>499997</th>\n      <td>131262</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>ROCKDALE</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>30013</td>\n    </tr>\n    <tr>\n      <th>499998</th>\n      <td>315673</td>\n      <td>H</td>\n      <td></td>\n      <td>21.0</td>\n      <td>BARROW</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>30680</td>\n    </tr>\n    <tr>\n      <th>499999</th>\n      <td>467477</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>CHATHAM</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>NaN</td>\n      <td>31407</td>\n    </tr>\n  </tbody>\n</table>\n<p>500000 rows × 298 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "data_500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:15.060846Z",
     "start_time": "2023-10-13T15:40:44.938501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "remove_columns = [\n",
    "    'PRFL_LGBT_SUPPORT',\n",
    "    'PRFL_LIBERAL_NEWS',\n",
    "    'PRFL_MARIJUANA_REFORM',\n",
    "    'PRFL_BIDEN_SUPPORT',\n",
    "    'PRFL_BORDER_SECURITY',\n",
    "    'PRFL_CONSERVATIVE_NEWS',\n",
    "    'PRFL_IMMIGRATION_REFORM',\n",
    "    'PRFL_OBAMA',\n",
    "    'PRFL_PERSUADABLE_VOTER',\n",
    "    'PRFL_POLITICAL_IDEOLOGY',\n",
    "    'PRFL_SANDERS_SUPPORT',\n",
    "    'PRFL_TRUMP_SUPPORT',\n",
    "    'ZIP',\n",
    "    \n",
    "    'VTR_GEN00', 'VTR_GEN01', 'VTR_GEN02', 'VTR_GEN03', 'VTR_GEN04', 'VTR_GEN05', 'VTR_GEN06', 'VTR_GEN07', 'VTR_GEN08', 'VTR_GEN09', 'VTR_GEN10', 'VTR_GEN11', 'VTR_GEN12', 'VTR_GEN13', 'VTR_GEN14', 'VTR_GEN15', 'VTR_GEN16', 'VTR_GEN17', 'VTR_GEN18', 'VTR_GEN19', 'VTR_GEN20', 'VTR_GEN21', 'VTR_GEN22', 'VTR_OTH00', 'VTR_OTH01', 'VTR_OTH02', 'VTR_OTH03', 'VTR_OTH04', 'VTR_OTH05', 'VTR_OTH06', 'VTR_OTH07', 'VTR_OTH08', 'VTR_OTH09', 'VTR_OTH10', 'VTR_OTH11', 'VTR_OTH12', 'VTR_OTH13', 'VTR_OTH14', 'VTR_OTH15', 'VTR_OTH16', 'VTR_OTH17', 'VTR_OTH18', 'VTR_OTH19', 'VTR_OTH20', 'VTR_OTH21', 'VTR_OTH22', 'VTR_PPP00', 'VTR_PPP04', 'VTR_PPP08', 'VTR_PPP12', 'VTR_PPP16', 'VTR_PPP20', 'VTR_PRI00', 'VTR_PRI01', 'VTR_PRI02', 'VTR_PRI03', 'VTR_PRI04', 'VTR_PRI05', 'VTR_PRI06', 'VTR_PRI07', 'VTR_PRI08', 'VTR_PRI09', 'VTR_PRI10', 'VTR_PRI11', 'VTR_PRI12', 'VTR_PRI13', 'VTR_PRI14', 'VTR_PRI15', 'VTR_PRI16', 'VTR_PRI17', 'VTR_PRI18', 'VTR_PRI19', 'VTR_PRI20', 'VTR_PRI21', 'VTR_PRI22',\n",
    "    \n",
    "        \n",
    "      'PRFL_CHOICELIFE', 'TOD_PRES_D_2016_PREC', 'TOD_PRES_O_2016',\n",
    "    'TOD_PRES_R_2016', 'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'VP_PPP',\n",
    "    'AGE', 'CNSUS_PCTW',\n",
    "    \n",
    "    'PARTY_MIX', 'PRFL_MINWAGE', 'PRFL_FENCE_SITTER', \n",
    "]\n",
    "# Drop the list of columns from the dataset\n",
    "data_500.drop(columns=remove_columns, errors='ignore', inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:17.786218Z",
     "start_time": "2023-10-13T15:41:17.752325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_party_create_infer(data_500):\n",
    "    #Replace blanks and spaces with NaN\n",
    "    corrected_data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "    corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "    # Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "    states_to_update = ['DC', 'LA', 'RI']\n",
    "    corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "\n",
    "    # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "    corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "    \n",
    "    print(\"N's after corrections\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['N'])]))\n",
    "    \n",
    "    print(\"U's after corrections\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['U'])]))\n",
    "    \n",
    "    print(\"inferred Rs:\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['S'])]))\n",
    "\n",
    "    print(\"inferred Ds:\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['E'])]))\n",
    "\n",
    "    # Define function to infer party based on given conditions\n",
    "    def set_inferred_party(row):\n",
    "        not_in_list = ['D', 'R', 'M', 'P', 'X', 'Z']\n",
    "        rpx = ['R', 'P', 'X']\n",
    "        dmz = ['D', 'M', 'Z']\n",
    "        if row['PARTY_CODE'] not in ['D', 'R','E','S']:\n",
    "            if (\n",
    "               (row['FUND_POLIT'] == 'R' or row['DON_POLCONS'] or row['PRFL_HEALTHCARE_REFORM'] == '2' or\n",
    "                row['PRFL_2NDAMEND'] == 'Y' or row['PRFL_CHOICELIFE'] == '1') and row['FUND_POLIT'] != 'D' and\n",
    "                not row['DON_POLLIB'] and all(row[col] not in not_in_list for col in\n",
    "                ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] + ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]])\n",
    "           ) or (\n",
    "               sum(1 for col in ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] + ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]]\n",
    "                if row[col] in rpx) > sum(1 for col in ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] +\n",
    "                ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]] if row[col] in dmz)\n",
    "           ):\n",
    "                return 'S'\n",
    "            elif (\n",
    "               (row['FUND_POLIT'] == 'D' or row['DON_POLLIB'] or row['PRFL_HEALTHCARE_REFORM'] == '1' or\n",
    "                row['PRFL_CHOICELIFE'] == '2') and row['FUND_POLIT'] != 'R' and not row['DON_POLCONS'] and\n",
    "                all(row[col] not in not_in_list for col in ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] +\n",
    "                ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]])\n",
    "           ) or (\n",
    "               sum(1 for col in ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] + ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]]\n",
    "                if row[col] in rpx) < sum(1 for col in ['VTR_PRI' + \"{:02}\".format(i) for i in range(22, 2, -1)] +\n",
    "                ['VTR_PPP' + \"{:02}\".format(i) for i in [20, 16, 12, 8, 4, 0]] if row[col] in dmz)\n",
    "           ):\n",
    "                return 'E'\n",
    "        return row['PARTY_CODE']\n",
    "\n",
    "    # Applying the function\n",
    "    corrected_data['PARTY_CODE'] = corrected_data.apply(set_inferred_party, axis=1)\n",
    "   \n",
    "    print(\"new inferred Rs:\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['S'])]))\n",
    "\n",
    "    print(\"new inferred Ds:\")\n",
    "    print(len(corrected_data.loc[corrected_data['PARTY_CODE'].isin(['E'])]))\n",
    "          \n",
    "\n",
    "    # Create party_mapping using defaultdict.Lambda sets default (all other non-D and non-R party registrations to NaN.\n",
    "    infer_party_mapping = defaultdict(lambda: float('nan'))\n",
    "    infer_party_mapping.update({\n",
    "        'D': 'D',  # Registered Democrats are mapped to Democrat\n",
    "        'E': 'D',  # E (Inferred Democrats) are mapped to Democrat\n",
    "        'R': 'R',  # Registered Republicans are mapped to Republican \n",
    "        'S': 'R'   # S (Inferred Republicans) are mapped to Republican\n",
    "    })\n",
    "    # Map PARTY_CODE to INFER_PARTY\n",
    "    corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(infer_party_mapping)\n",
    "\n",
    "    # Define the mapping for PARTY_CODE modifications\n",
    "    party_code_mapping = {\n",
    "        'E': float('nan'),\n",
    "        'S': float('nan'),\n",
    "        'U': float('nan'),\n",
    "        'A': 'O',\n",
    "        'B': 'O',\n",
    "        'C': 'O',\n",
    "        'F': 'O',\n",
    "        'G': 'O',\n",
    "        'H': 'O',\n",
    "        'I': 'O',\n",
    "        'J': 'O',\n",
    "        'K': 'O',\n",
    "        'L': 'L',\n",
    "        'P': 'O',\n",
    "        'Q': 'O',\n",
    "        'T': 'O',\n",
    "        'V': 'O',\n",
    "        'W': 'O',\n",
    "        'Y': 'O',\n",
    "        'Z': 'O'\n",
    "    }\n",
    "    # Apply the mapping to the PARTY_CODE column\n",
    "    corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "\n",
    "    print(\"Final PARTY_CODE values:\")\n",
    "    print(corrected_data['PARTY_CODE'].unique())\n",
    "    print('Final INFER_PARTY values:')\n",
    "    print(corrected_data['INFER_PARTY'].unique())\n",
    "    print('INFER_PARTY Ds and Rs:')\n",
    "    print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "    print('INFER_PARTY NaNs:')\n",
    "    print(corrected_data['INFER_PARTY'].isna().sum())\n",
    "    \n",
    "    \n",
    "    voter_columns = [col for col in corrected_data.columns if col.startswith(\"VTR\")]\n",
    "\n",
    "    #Create column with total number of votes in voter_columns per row\n",
    "    corrected_data['VTR_TOTAL_VOTES'] = corrected_data[voter_columns].notnull().sum(axis=1)\n",
    "\n",
    "    #Sum Democrat and Republican totals\n",
    "    corrected_data['VTR_TOTAL_DVOTES'] = corrected_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "    corrected_data['VTR_TOTAL_RVOTES'] = corrected_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "\n",
    "    #Create conditions that evaluate whether someone has cast at least two more votes for Democrats or Republicans: \n",
    "    conditions = [\n",
    "        corrected_data['VTR_TOTAL_DVOTES'] - corrected_data['VTR_TOTAL_RVOTES'] >= 2,\n",
    "        corrected_data['VTR_TOTAL_RVOTES'] - corrected_data['VTR_TOTAL_DVOTES'] >= 2\n",
    "    ]\n",
    "\n",
    "    choices = ['D', 'R']\n",
    "\n",
    "    corrected_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "    corrected_data['VTR_INFER_PARTY'].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "    # Assign a 'D' or an 'R' to INFER_PARTY if either condition is true:\n",
    "    corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'D') , 'INFER_PARTY'] = 'D'\n",
    "    corrected_data.loc[(corrected_data['VTR_INFER_PARTY'] == 'R') , 'INFER_PARTY'] = 'R'\n",
    "\n",
    "    print('Final INFER_PARTY Ds and Rs:')\n",
    "    print(len(corrected_data.loc[corrected_data['INFER_PARTY'].isin(['D', 'R'])]))\n",
    "    print('Final INFER_PARTY NaNs:')\n",
    "    print(corrected_data['INFER_PARTY'].isna().sum())\n",
    "    \n",
    "    \n",
    "    return (corrected_data)\n",
    "clean_party_create_infer(data_500)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'R' 'O' 'D' 'A' 'F' 'P' 'G' 'L' 'U' 'W' 'B' 'I' 'Y' 'V' 'H' ' ' 'S'\n",
      " 'E' 'Q' 'Z']\n",
      "unique_infer_party:\n",
      "[nan 'R' 'D']\n",
      "unique_party_code_after_modifications:\n",
      "['N' 'R' 'O' 'D' 'L' nan ' ']\n"
     ]
    }
   ],
   "source": [
    "# # Load the dataset again\n",
    "# #data = pd.read_csv('data/surveydata.csv')\n",
    "# \n",
    "# corrected_data = data_500.copy()\n",
    "# \n",
    "# # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'SD'\n",
    "# corrected_data.loc[(corrected_data['STATE'] == 'SD') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "# \n",
    "# # Update the 'U' values in PARTY_CODE to 'N' for rows where STATE is in ['DC', 'LA', 'RI']\n",
    "# states_to_update = ['DC', 'LA', 'RI']\n",
    "# corrected_data.loc[(corrected_data['STATE'].isin(states_to_update)) & (corrected_data['PARTY_CODE'] == 'U'), 'PARTY_CODE'] = 'N'\n",
    "# \n",
    "# # Update the 'I' values in PARTY_CODE to 'N' for rows where STATE is 'OK'\n",
    "# corrected_data.loc[(corrected_data['STATE'] == 'OK') & (corrected_data['PARTY_CODE'] == 'I'), 'PARTY_CODE'] = 'N'\n",
    "# \n",
    "# \n",
    "# #categorical_columns\n",
    "# # Create a mapping dictionary for PARTY_CODE to INFER_PARTY values\n",
    "# party_mapping = {\n",
    "#     'D': 'D',\n",
    "#     'E': 'D',\n",
    "#     'R': 'R',\n",
    "#     'S': 'R',\n",
    "#     'N': float('nan'),\n",
    "#     'U': float('nan'),\n",
    "#     'A': float('nan'),\n",
    "#     'B': float('nan'),\n",
    "#     'C': float('nan'),\n",
    "#     'F': float('nan'),\n",
    "#     'G': float('nan'),\n",
    "#     'H': float('nan'),\n",
    "#     'I': float('nan'),\n",
    "#     'J': float('nan'),\n",
    "#     'K': float('nan'),\n",
    "#     'L': float('nan'),\n",
    "#     'P': float('nan'),\n",
    "#     'Q': float('nan'),\n",
    "#     'T': float('nan'),\n",
    "#     'V': float('nan'),\n",
    "#     'W': float('nan'),\n",
    "#     'Y': float('nan'),\n",
    "#     'Z': float('nan'),\n",
    "#     'O': float('nan'),\n",
    "# }\n",
    "# \n",
    "# # Create the INFER_PARTY column using the mapping\n",
    "# corrected_data['INFER_PARTY'] = corrected_data['PARTY_CODE'].map(party_mapping)\n",
    "# \n",
    "# print(corrected_data['PARTY_CODE'].unique())\n",
    "# \n",
    "# \n",
    "# # Display the unique values in the INFER_PARTY column to ensure correctness\n",
    "# unique_infer_party = corrected_data['INFER_PARTY'].unique()\n",
    "# \n",
    "# print('unique_infer_party:')\n",
    "# \n",
    "# print(unique_infer_party)\n",
    "# \n",
    "# # Define the mapping for PARTY_CODE modifications\n",
    "# party_code_mapping = {\n",
    "#     'E': float('nan'),\n",
    "#     'S': float('nan'),\n",
    "#     'U': float('nan'),\n",
    "#     'A': 'O',\n",
    "#     'B': 'O',\n",
    "#     'C': 'O',\n",
    "#     'F': 'O',\n",
    "#     'G': 'O',\n",
    "#     'H': 'O',\n",
    "#     'I': 'O',\n",
    "#     'J': 'O',\n",
    "#     'K': 'O',\n",
    "#     'L': 'L',\n",
    "#     'P': 'O',\n",
    "#     'Q': 'O',\n",
    "#     'T': 'O',\n",
    "#     'V': 'O',\n",
    "#     'W': 'O',\n",
    "#     'Y': 'O',\n",
    "#     'Z': 'O'\n",
    "# }\n",
    "# \n",
    "# # Apply the mapping to the PARTY_CODE column\n",
    "# corrected_data['PARTY_CODE'] = corrected_data['PARTY_CODE'].replace(party_code_mapping)\n",
    "# \n",
    "# # Check the unique values of PARTY_CODE after the modifications\n",
    "# unique_party_code_after_modifications = corrected_data['PARTY_CODE'].unique()\n",
    "# \n",
    "# print(\"unique_party_code_after_modifications:\")\n",
    "# \n",
    "# print(unique_party_code_after_modifications)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:20.901894Z",
     "start_time": "2023-10-13T15:41:17.784532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['N', 'R', 'O', 'D', 'A', 'F', 'P', 'G', 'L', 'U', 'W', 'B', 'I',\n       'Y', 'V', 'H', ' ', 'S', 'E', 'Q', 'Z'], dtype=object)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500['PARTY_CODE'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:20.936509Z",
     "start_time": "2023-10-13T15:41:20.920746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:21.097008Z",
     "start_time": "2023-10-13T15:41:20.930861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154352\n",
      "154352\n"
     ]
    }
   ],
   "source": [
    "engineered_data = corrected_data.copy()\n",
    "voter_columns = [col for col in engineered_data.columns if col.startswith(\"VTR\")]\n",
    "#Create column with total number of votes in voter_columns per row\n",
    "engineered_data['VTR_TOTAL_VOTES'] = engineered_data[voter_columns].notnull().sum(axis=1)\n",
    "#Sum Democrat and Republican totals\n",
    "engineered_data['VTR_TOTAL_DVOTES'] = engineered_data[voter_columns].isin(['D', 'M', 'Z']).sum(axis=1)\n",
    "engineered_data['VTR_TOTAL_RVOTES'] = engineered_data[voter_columns].isin(['R', 'P', 'X']).sum(axis=1)\n",
    "#Create conditions that evaluate whether more votes have been cast for Democrats or Republicans, and assign 'D' and 'R' to new column based on conditions\n",
    "conditions = [\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] > engineered_data['VTR_TOTAL_RVOTES'],\n",
    "    engineered_data['VTR_TOTAL_DVOTES'] < engineered_data['VTR_TOTAL_RVOTES']\n",
    "]\n",
    "choices = ['D', 'R']\n",
    "engineered_data['VTR_INFER_PARTY'] = np.select(conditions, choices, default=np.nan)\n",
    "#Create conditions that evaluate whether a voter is a swing voter or not\n",
    "conditions_swing = [\n",
    "    (engineered_data['VTR_TOTAL_DVOTES'] > 2) & (engineered_data['VTR_TOTAL_RVOTES'] > 2),\n",
    "    ((engineered_data['VTR_TOTAL_DVOTES'] > 1) & (engineered_data['VTR_TOTAL_RVOTES'] == 0)) | ((engineered_data['VTR_TOTAL_RVOTES'] > 1) & (engineered_data['VTR_TOTAL_DVOTES'] == 0))\n",
    "]\n",
    "choices_swing = ['Y', 'N']\n",
    "engineered_data['VTR_INFER_SWING'] = np.select(conditions_swing, choices_swing, default=np.nan)\n",
    "#sampledf = engineered_data[['PARTY_CODE','INFER_PARTY','VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY','VTR_INFER_SWING']]\n",
    "#print(sampledf.head(50))\n",
    "#Add values to INFER_PARTY and correct any other INFER_PARTY values that don't meet the conditions above:\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'D'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'N'), 'INFER_PARTY'] = 'R'\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'D') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_PARTY'] == 'R') & (engineered_data['VTR_INFER_SWING'] == 'Y'), 'INFER_PARTY'] =  float('nan')\n",
    "#Also delete any 'D' or 'R' INFER_PARTY labels for anyone deemed a \"swing voter\" based on criteria above of voting for both parties at least 3 times each:\n",
    "engineered_data.loc[(engineered_data['VTR_INFER_SWING'] == 'Y') , 'INFER_PARTY'] = float('nan')\n",
    "print(sum(engineered_data['INFER_PARTY'].isna())) #291 NaNs for INFER_PARTY before\n",
    "#Drop auxiliary columns used for math, keeping 'VTR_TOTAL_VOTES', 'VTR_INFER_SWING', and the updated 'INFER_PARTY' columns as features:\n",
    "engineered_data = engineered_data.drop(columns=['VTR_TOTAL_DVOTES','VTR_TOTAL_RVOTES','VTR_INFER_PARTY'])\n",
    "# Drop the list of columns from the dataset\n",
    "#engineered_data_cleaned = engineered_data.drop(columns=remove_columns, errors='ignore')\n",
    "\n",
    "# features_to_remove = [\n",
    "#     'PRFL_CHOICELIFE', 'TOD_PRES_D_2016_PREC', 'TOD_PRES_O_2016',\n",
    "#     'TOD_PRES_R_2016', 'TOD_PRES_R_2016_PREC', 'TOD_PRES_R_2020_PREC', 'VP_PPP',\n",
    "#     'AGE', 'CNSUS_PCTW'\n",
    "# ]\n",
    "# Assuming 'engineered_data' is your DataFrame, remove the less important features\n",
    "#engineered_data = engineered_data.drop(columns=features_to_remove, errors='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:28.380839Z",
     "start_time": "2023-10-13T15:41:20.986080Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# voter_columns = [col for col in engineered_data.columns if col.startswith(\"VTR\")]\n",
    "# # Function to infer party\n",
    "# def infer_party(row):\n",
    "#     # Check for VTR_PPP20 value first\n",
    "#     if row['VTR_PPP20'] in ['D', 'M', 'Z']:\n",
    "#         return 'D'\n",
    "#     elif row['VTR_PPP20'] in ['R', 'P', 'X']:\n",
    "#         return 'R'\n",
    "#     # If VTR_PPP20 condition is not met, continue with existing logic\n",
    "#     last_votes = [vote for vote in row[voter_columns] if vote in ['D', 'M', 'Z', 'R', 'P', 'X']][-2:]\n",
    "#     if len(last_votes) == 2:\n",
    "#         if all(vote in ['D', 'M', 'Z'] for vote in last_votes):\n",
    "#             return 'D'\n",
    "#         elif all(vote in ['R', 'P', 'X'] for vote in last_votes):\n",
    "#             return 'R'\n",
    "#     return np.nan\n",
    "# # Create new series with inferred parties\n",
    "# inferred_parties = engineered_data.apply(infer_party, axis=1)\n",
    "# print(sum(engineered_data['INFER_PARTY'].isna()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:28.386655Z",
     "start_time": "2023-10-13T15:41:28.381596Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "        RECORD_ID ADD_TYPE AFAMPROFLS   AGE AI_COUNTY_NAME AIRCOND APP_CHILD  \\\n11558      138598        S             39.0         PLACER                     \n135845     264469        S             40.0        BALDWIN                     \n330455     289286        S             40.0         MEDINA                     \n216587     148015        S             44.0      WASHTENAW                     \n257848     398789        S             44.0    MECKLENBURG                     \n...           ...      ...        ...   ...            ...     ...       ...   \n62262      263044        S             74.0         SUSSEX       A             \n111662     337263        H             46.0           COOK                     \n41375      452029        S             27.0    LOS ANGELES                     \n82407      450742        S             34.0       COLUMBIA                     \n320467     298730        S             19.0       COLUMBIA                     \n\n       APP_MENBIG APP_TODDLR APP_WOMEN  ... TRAIL_CNT TRAVEL VOTER_CNT  \\\n11558                                   ...         1               14   \n135845                                  ...         2               8    \n330455                                  ...         1               1    \n216587                                  ...         1               4    \n257848                                  ...         1               4    \n...           ...        ...       ...  ...       ...    ...       ...   \n62262                                   ...         1               21   \n111662                                  ...         1               8    \n41375                                   ...         1               2    \n82407                                   ...         2               0    \n320467                                  ...         4      Y        2    \n\n       VOTER_TRLR VP_GEN VP_OTH  VP_PPP VP_PRI WORKWOMAN YEARBUILT  \n11558           1  50.00    0.0  100.00    0.0         Y            \n135845          1  40.00    0.0    0.00   10.0                2014  \n330455          0   0.00    0.0    0.00    0.0                      \n216587          0  20.00    0.0    0.00    0.0                      \n257848          0  28.57    0.0   50.00    0.0                 NaN  \n...           ...    ...    ...     ...    ...       ...       ...  \n62262           1  60.00   10.0   33.33   50.0         Y      1999  \n111662          0  20.00    0.0    0.00    0.0                 NaN  \n41375           0  33.33    0.0  100.00    0.0                 NaN  \n82407           1   0.00    0.0    0.00    0.0                      \n320467          3  50.00   50.0    0.00    0.0                1916  \n\n[500 rows x 210 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RECORD_ID</th>\n      <th>ADD_TYPE</th>\n      <th>AFAMPROFLS</th>\n      <th>AGE</th>\n      <th>AI_COUNTY_NAME</th>\n      <th>AIRCOND</th>\n      <th>APP_CHILD</th>\n      <th>APP_MENBIG</th>\n      <th>APP_TODDLR</th>\n      <th>APP_WOMEN</th>\n      <th>...</th>\n      <th>TRAIL_CNT</th>\n      <th>TRAVEL</th>\n      <th>VOTER_CNT</th>\n      <th>VOTER_TRLR</th>\n      <th>VP_GEN</th>\n      <th>VP_OTH</th>\n      <th>VP_PPP</th>\n      <th>VP_PRI</th>\n      <th>WORKWOMAN</th>\n      <th>YEARBUILT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11558</th>\n      <td>138598</td>\n      <td>S</td>\n      <td></td>\n      <td>39.0</td>\n      <td>PLACER</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>14</td>\n      <td>1</td>\n      <td>50.00</td>\n      <td>0.0</td>\n      <td>100.00</td>\n      <td>0.0</td>\n      <td>Y</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>135845</th>\n      <td>264469</td>\n      <td>S</td>\n      <td></td>\n      <td>40.0</td>\n      <td>BALDWIN</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>2</td>\n      <td></td>\n      <td>8</td>\n      <td>1</td>\n      <td>40.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>10.0</td>\n      <td></td>\n      <td>2014</td>\n    </tr>\n    <tr>\n      <th>330455</th>\n      <td>289286</td>\n      <td>S</td>\n      <td></td>\n      <td>40.0</td>\n      <td>MEDINA</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>216587</th>\n      <td>148015</td>\n      <td>S</td>\n      <td></td>\n      <td>44.0</td>\n      <td>WASHTENAW</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>4</td>\n      <td>0</td>\n      <td>20.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>257848</th>\n      <td>398789</td>\n      <td>S</td>\n      <td></td>\n      <td>44.0</td>\n      <td>MECKLENBURG</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>4</td>\n      <td>0</td>\n      <td>28.57</td>\n      <td>0.0</td>\n      <td>50.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>62262</th>\n      <td>263044</td>\n      <td>S</td>\n      <td></td>\n      <td>74.0</td>\n      <td>SUSSEX</td>\n      <td>A</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>21</td>\n      <td>1</td>\n      <td>60.00</td>\n      <td>10.0</td>\n      <td>33.33</td>\n      <td>50.0</td>\n      <td>Y</td>\n      <td>1999</td>\n    </tr>\n    <tr>\n      <th>111662</th>\n      <td>337263</td>\n      <td>H</td>\n      <td></td>\n      <td>46.0</td>\n      <td>COOK</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>8</td>\n      <td>0</td>\n      <td>20.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>41375</th>\n      <td>452029</td>\n      <td>S</td>\n      <td></td>\n      <td>27.0</td>\n      <td>LOS ANGELES</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>2</td>\n      <td>0</td>\n      <td>33.33</td>\n      <td>0.0</td>\n      <td>100.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>82407</th>\n      <td>450742</td>\n      <td>S</td>\n      <td></td>\n      <td>34.0</td>\n      <td>COLUMBIA</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>2</td>\n      <td></td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>320467</th>\n      <td>298730</td>\n      <td>S</td>\n      <td></td>\n      <td>19.0</td>\n      <td>COLUMBIA</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>...</td>\n      <td>4</td>\n      <td>Y</td>\n      <td>2</td>\n      <td>3</td>\n      <td>50.00</td>\n      <td>50.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td></td>\n      <td>1916</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 210 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_500.sample(n=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:41:28.523321Z",
     "start_time": "2023-10-13T15:41:28.385264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with 'Y' or 'Unknown': ['AFAMPROFLS', 'APP_CHILD', 'APP_MENBIG', 'APP_TODDLR', 'APP_WOMEN', 'APP_WOMPET', 'APP_WOMPLS', 'APP_YNGMEN', 'ARTS', 'AUTOACCES', 'AUTOWORK', 'BOATING', 'BROADERLIV', 'CARDUSER', 'CATOWNER', 'CH_0002FEM', 'CH_0002MAL', 'CH_0002UNK', 'CH_0305FEM', 'CH_0305MAL', 'CH_0305UNK', 'CH_0610FEM', 'CH_0610MAL', 'CH_0610UNK', 'CH_1115FEM', 'CH_1115MAL', 'CH_1115UNK', 'CH_1617FEM', 'CH_1617MAL', 'CH_1617UNK', 'CHRISTFAM', 'COL_ANTIQ', 'COL_ARTS', 'COL_COIN', 'COL_SPORT', 'COL_STAMP', 'COMPHOMOFC', 'COMPUTERS', 'COOK_GEN', 'CURRAFFAIR', 'DEPTSTCRD', 'DIETING', 'DIYLIV', 'DOGOWNER', 'DON_ANML', 'DON_ARTCUL', 'DON_CHARIT', 'DON_CHILD', 'DON_ENVIR', 'DON_ENVWLD', 'DON_HEALTH', 'DON_INTAID', 'DON_OTHER', 'DON_POLCONS', 'DON_POLIT', 'DON_POLLIB', 'DON_RELIG', 'DON_VET', 'DONATION', 'EDU_ONLINE', 'EQUESTRIAN', 'EXER_GROUP', 'GAMING', 'GARDENER', 'GOLF', 'GRANDCHLD', 'HEALTHBEAU', 'HEATHMED', 'HH_SENIOR', 'HH_VETERAN', 'HH_YOUNGAD', 'HIGHBROW', 'HIGHENDAPP', 'HISTMIL', 'HITECHLIV', 'HOME_DECOR', 'HOMEOFFICE', 'HUNTING', 'HUNTSHOOT', 'INV_ACTIVE', 'MAIL_DONOR', 'MAILORDBUY', 'MAILORDRSP', 'MOTORCYCLE', 'NASCAR', 'PARENTING', 'PETS', 'PRESENCHLD', 'PRFL_2NDAMEND', 'PRFL_ACTIVE_MIL', 'PRFL_AMZN_PRIME', 'PRFL_ANML_RIGHTS', 'PRFL_BLM_SUPPORT', 'PRFL_CLINTON_SUPPORT', 'PRFL_EDUCATION', 'PRFL_ENVIRONMENT', 'PRFL_EVANGELICAL', 'PRFL_FENCE_SITTER', 'PRFL_GUN_CONTROL', 'PRFL_HEALTHCARE', 'PRFL_INFLUENCER', 'PRFL_INSURANCE', 'PRFL_LABOR', 'PRFL_METOO_SUPPORT', 'PRFL_MIL_SUPPORT', 'PRFL_TAXES', 'PRFL_TEACHERS_UNION', 'PRFL_VETERAN', 'RD_FINNEWS', 'RD_GEN', 'RD_RELIG', 'RD_SCIFI', 'RELIGINSP', 'SCISPACE', 'SCUBADIVER', 'SELFIMP', 'SINGPARENT', 'SMOKING', 'SPEC_AUTO', 'SPEC_BASE', 'SPEC_BASK', 'SPEC_FOOT', 'SPEC_HOCK', 'SPEC_SOCC', 'SPORTLEIS', 'SWEEPSTAKE', 'TELECOM', 'TENNIS', 'THEATER', 'TRAVEL', 'WORKWOMAN'] 131\n",
      "Columns with more than two categories: ['ADD_TYPE', 'AI_COUNTY_NAME', 'AIRCOND', 'ASSMLCODE', 'BUS_OWNER', 'CENSUS_ST', 'CNS_MEDINC', 'CONG_DIST', 'COUNTY_ST', 'COUNTY_TYPE', 'CRD_RANGE', 'CREDRATE', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE', 'ETHNICCONF', 'ETHNICGRP', 'FUND_POLIT', 'GENDER_MIX', 'GENERATION', 'HH_NUMGEN', 'HH_SIZE', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT', 'INCOMESTHH', 'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'NETWORTH', 'NUMCHILD', 'OCCDETAIL', 'OCCUPATION', 'PARTY_CODE', 'PERSONS_HH', 'POOL', 'PRFL_TEAPARTY', 'RELIGION', 'SEX', 'ST_LO_HOUS', 'ST_UP_HOUS', 'STATE', 'STATUS', 'TOD_PRES_DIFF_2016', 'TOD_PRES_DIFF_2016_PREC', 'TOD_PRES_DIFF_2020_PREC', 'VOTER_CNT', 'VOTER_TRLR', 'YEARBUILT'] 49\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "#data_500 = pd.read_csv(\"/Users/nirugidla/Documents/GitHub/milestone2_MADS/data_500k.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning - Column Names\n",
    "data_500.columns = data_500.columns.str.strip()\n",
    "\n",
    "# Data Cleaning - Drop ZIP\n",
    "#data_500.drop('ZIP', axis=1, inplace=True)\n",
    "# Drop the list of columns from the dataset\n",
    "data_500_cleaned = data_500.drop(columns=remove_columns, errors='ignore')\n",
    "\n",
    "# Data Cleaning - Drop Duplicates\n",
    "data_500.drop_duplicates(inplace=True)\n",
    "\n",
    "# Data Cleaning - Object Columns\n",
    "for col in data_500.columns:\n",
    "    if data_500[col].dtype == 'object':\n",
    "        data_500[col] = data_500[col].str.strip()\n",
    "\n",
    "# Data Cleaning - Empty Strings\n",
    "data_500.replace('', 'Unknown', inplace=True)\n",
    "\n",
    "# Data Cleaning - NaN for Object Types\n",
    "data_500.loc[:, data_500.dtypes == 'object'] = data_500.loc[:, data_500.dtypes == 'object'].fillna('Unknown')\n",
    "\n",
    "# Data Cleaning - Drop Columns and Rows with All NaNs\n",
    "data_500.dropna(axis=1, how='all', inplace=True)\n",
    "data_500.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = data_500.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = data_500.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Data Cleaning - Removing Non-Numeric Columns with More Than 90% Missing Data\n",
    "missing_data_percentage = data_500.isnull().mean() * 100\n",
    "non_numeric_cols_to_remove = missing_data_percentage[non_numeric_cols]\n",
    "non_numeric_cols_to_remove = non_numeric_cols_to_remove[non_numeric_cols_to_remove > 90].index.tolist()\n",
    "data_500_reduced = data_500.drop(columns=non_numeric_cols_to_remove)\n",
    "\n",
    "# Update the list of non-numeric columns after removal\n",
    "non_numeric_cols = data_500_reduced.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Identifying Specific Types of Non-Numeric Columns\n",
    "cols_with_Y_or_Unknown = [col for col in non_numeric_cols if set(data_500_reduced[col].unique()) <= {'Y', 'Unknown'}]\n",
    "cols_with_more_than_two_categories = [col for col in non_numeric_cols if len(data_500_reduced[col].unique()) > 2]\n",
    "\n",
    "# Print identified columns\n",
    "print(\"Columns with 'Y' or 'Unknown':\", cols_with_Y_or_Unknown, len(cols_with_Y_or_Unknown))\n",
    "print(\"Columns with more than two categories:\", cols_with_more_than_two_categories, len(cols_with_more_than_two_categories))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:42:18.047043Z",
     "start_time": "2023-10-13T15:41:29.574200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Just the columns with Yes or Unknown non_numeric_cols "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(500000, 131)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced_with_Y_or_Unknown = data_500[cols_with_Y_or_Unknown].copy()\n",
    "data_reduced_with_Y_or_Unknown.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:42:19.042343Z",
     "start_time": "2023-10-13T15:42:19.021301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.4291\n"
     ]
    }
   ],
   "source": [
    "# Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "data_reduced_with_Y_or_Unknown = engineered_data[cols_with_Y_or_Unknown].copy()\n",
    "\n",
    "# Adding the target column to this data\n",
    "data_reduced_with_Y_or_Unknown['PARTY_CODE'] = engineered_data['PARTY_CODE']\n",
    "\n",
    "# Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['PARTY_CODE'].notna()]\n",
    "\n",
    "# Sample 100,000 rows from the data\n",
    "data_sample = data_reduced_with_Y_or_Unknown.sample(n=100000, random_state=42)\n",
    "data_sample = data_sample.drop(columns='INFER_PARTY', errors='ignore')\n",
    "\n",
    "# Count instances of each class in 'PARTY_CODE' again to filter out classes with fewer than 2 instances\n",
    "class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "\n",
    "# Label-encode 'PARTY_CODE' column\n",
    "le = LabelEncoder()\n",
    "data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "\n",
    "# One-Hot Encoding\n",
    "data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "\n",
    "# Splitting the Data into Training and Test Sets\n",
    "X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "y = data_one_hot['PARTY_CODE']\n",
    "\n",
    "# Perform train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and Train XGBoost Classifier\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions and Evaluate the Model\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Model Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:42:28.065291Z",
     "start_time": "2023-10-13T15:42:19.733649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.63435\n"
     ]
    }
   ],
   "source": [
    "# Assuming cols_with_Y_or_Unknown has been identified and engineered_data has been cleaned\n",
    "data_reduced_with_Y_or_Unknown = engineered_data[cols_with_Y_or_Unknown].copy()\n",
    "\n",
    "# Adding the target column to this data\n",
    "data_reduced_with_Y_or_Unknown['INFER_PARTY'] = engineered_data['INFER_PARTY']\n",
    "\n",
    "# Remove rows where 'INFER_PARTY' is missing, as it's our target variable\n",
    "data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['INFER_PARTY'].notna()]\n",
    "\n",
    "# Sample 100,000 rows from the data\n",
    "data_sample = data_reduced_with_Y_or_Unknown.sample(n=100000, random_state=42)\n",
    "\n",
    "# Count instances of each class in 'INFER_PARTY' again to filter out classes with fewer than 2 instances\n",
    "class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "\n",
    "# Check for sufficient number of valid classes\n",
    "if len(valid_classes) < 2:\n",
    "    raise ValueError(\"Insufficient number of valid classes for multi-class classification.\")\n",
    "\n",
    "data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "\n",
    "# Label-encode 'INFER_PARTY' column\n",
    "le = LabelEncoder()\n",
    "data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "\n",
    "# One-Hot Encoding\n",
    "data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "\n",
    "# Splitting the Data into Training and Test Sets\n",
    "X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "y = data_one_hot['INFER_PARTY']\n",
    "\n",
    "# Perform train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and Train XGBoost Classifier\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42, num_class=len(valid_classes))\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions and Evaluate the Model\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Model Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:42:32.302956Z",
     "start_time": "2023-10-13T15:42:28.794360Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.6145  0.599599  0.6145   \n1    2    0.6055  0.593710  0.6055   \n\n                                      Top_N_Features  \n0  [PRFL_2NDAMEND_Y, DON_POLCONS_Y, PETS_Y, DON_P...  \n1  [PRFL_2NDAMEND_Y, DON_POLCONS_Y, DON_POLLIB_Y,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.6145</td>\n      <td>0.599599</td>\n      <td>0.6145</td>\n      <td>[PRFL_2NDAMEND_Y, DON_POLCONS_Y, PETS_Y, DON_P...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.6055</td>\n      <td>0.593710</td>\n      <td>0.6055</td>\n      <td>[PRFL_2NDAMEND_Y, DON_POLCONS_Y, DON_POLLIB_Y,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_xgboost(sample_size, num_runs, top_N_features):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "        data_reduced_with_Y_or_Unknown = engineered_data[cols_with_Y_or_Unknown].copy()\n",
    "        data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown.drop(columns='PARTY_CODE', errors='ignore')\n",
    "\n",
    "        # Adding the target column to this data\n",
    "        data_reduced_with_Y_or_Unknown['INFER_PARTY'] = engineered_data['INFER_PARTY']\n",
    "\n",
    "        # Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "        data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['INFER_PARTY'].notna()]\n",
    "\n",
    "        # Sample data\n",
    "        data_sample = data_reduced_with_Y_or_Unknown.sample(n=sample_size, random_state=run)\n",
    "\n",
    "        # Dynamic Class Handling\n",
    "        class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 1].index.tolist()\n",
    "        data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "\n",
    "        # Label-encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "\n",
    "        # One-Hot Encoding\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "\n",
    "        # Splitting the Data into Training and Test Sets\n",
    "        X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "        y = data_one_hot['INFER_PARTY']\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Initialize and Train XGBoost Classifier\n",
    "        xgb = XGBClassifier(objective='multi:softmax', num_class=len(valid_classes), random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        # Make Predictions and Evaluate the Model\n",
    "        y_pred = xgb.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Get top N features\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        # Append to results list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example of how to call this function\n",
    "result = run_xgboost(sample_size=10000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:49:23.547829Z",
     "start_time": "2023-10-13T15:49:13.185377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.6145  0.599599  0.6145   \n1    2    0.6055  0.593710  0.6055   \n\n                                      Top_N_Features  \n0  [PRFL_2NDAMEND_Y, DON_POLCONS_Y, PETS_Y, DON_P...  \n1  [PRFL_2NDAMEND_Y, DON_POLCONS_Y, DON_POLLIB_Y,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.6145</td>\n      <td>0.599599</td>\n      <td>0.6145</td>\n      <td>[PRFL_2NDAMEND_Y, DON_POLCONS_Y, PETS_Y, DON_P...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.6055</td>\n      <td>0.593710</td>\n      <td>0.6055</td>\n      <td>[PRFL_2NDAMEND_Y, DON_POLCONS_Y, DON_POLLIB_Y,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_xgboost(sample_size, num_runs, top_N_features):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "        data_reduced_with_Y_or_Unknown = engineered_data[cols_with_Y_or_Unknown].copy()\n",
    "\n",
    "        # Adding the target column to this data\n",
    "        data_reduced_with_Y_or_Unknown['INFER_PARTY'] = engineered_data['INFER_PARTY']\n",
    "\n",
    "        # Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "        data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['INFER_PARTY'].notna()]\n",
    "\n",
    "        # Sample data\n",
    "        data_sample = data_reduced_with_Y_or_Unknown.sample(n=sample_size, random_state=run)\n",
    "\n",
    "        # Dynamic Class Handling\n",
    "        class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 1].index.tolist()\n",
    "        \n",
    "         # Check if we have more than one class, else skip this run\n",
    "        if len(valid_classes) > 1:\n",
    "            data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "\n",
    "            # Label-encode 'PARTY_CODE' column\n",
    "            le = LabelEncoder()\n",
    "            data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "    \n",
    "            # One-Hot Encoding\n",
    "            data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "    \n",
    "            # Splitting the Data into Training and Test Sets\n",
    "            X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "            y = data_one_hot['INFER_PARTY']\n",
    "    \n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "    \n",
    "            # Initialize and Train XGBoost Classifier\n",
    "            xgb = XGBClassifier(objective='multi:softmax', num_class=len(valid_classes), random_state=42)\n",
    "            xgb.fit(X_train, y_train)\n",
    "    \n",
    "            # Make Predictions and Evaluate the Model\n",
    "            y_pred = xgb.predict(X_test)\n",
    "    \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "            # Get top N features\n",
    "            feature_importances = xgb.feature_importances_\n",
    "            sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "            top_features = X.columns[sorted_idx].tolist()\n",
    "    \n",
    "            # Append to results list\n",
    "            results_list.append({\n",
    "                'Run': run,\n",
    "                'Accuracy': accuracy,\n",
    "                'F1_Score': f1,\n",
    "                'Recall': recall,\n",
    "                'Top_N_Features': top_features\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping run {run} due to insufficient classes.\")\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example of how to call this function\n",
    "result = run_xgboost(sample_size=10000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:06:42.547769Z",
     "start_time": "2023-10-10T03:06:32.349680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1     0.448  0.422753   0.448   \n1    2     0.426  0.398875   0.426   \n\n                                      Top_N_Features  \n0  [PRFL_BLM_SUPPORT_Y, PRFL_GUN_CONTROL_Y, PRFL_...  \n1  [PRFL_BLM_SUPPORT_Y, PRFL_GUN_CONTROL_Y, MAILO...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.448</td>\n      <td>0.422753</td>\n      <td>0.448</td>\n      <td>[PRFL_BLM_SUPPORT_Y, PRFL_GUN_CONTROL_Y, PRFL_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.426</td>\n      <td>0.398875</td>\n      <td>0.426</td>\n      <td>[PRFL_BLM_SUPPORT_Y, PRFL_GUN_CONTROL_Y, MAILO...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_xgboost(sample_size, num_runs, top_N_features):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "        data_reduced_with_Y_or_Unknown = data_500[cols_with_Y_or_Unknown].copy()\n",
    "\n",
    "        # Adding the target column to this data\n",
    "        data_reduced_with_Y_or_Unknown['PARTY_CODE'] = engineered_data['PARTY_CODE']\n",
    "\n",
    "        # Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "        data_reduced_with_Y_or_Unknown = data_reduced_with_Y_or_Unknown[data_reduced_with_Y_or_Unknown['PARTY_CODE'].notna()]\n",
    "\n",
    "        # Sample data\n",
    "        data_sample = data_reduced_with_Y_or_Unknown.sample(n=sample_size, random_state=run)\n",
    "\n",
    "        # Dynamic Class Handling\n",
    "        class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "        data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "\n",
    "        # Label-encode 'PARTY_CODE' column\n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "\n",
    "        # One-Hot Encoding\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_with_Y_or_Unknown, drop_first=True)\n",
    "\n",
    "        # Splitting the Data into Training and Test Sets\n",
    "        X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "        y = data_one_hot['PARTY_CODE']\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Initialize and Train XGBoost Classifier\n",
    "        xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        # Make Predictions and Evaluate the Model\n",
    "        y_pred = xgb.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Get top N features\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        # Append to results list\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results list\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example of how to call this function\n",
    "result = run_xgboost(sample_size=10000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:06:56.117745Z",
     "start_time": "2023-10-10T03:06:51.142649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(engineered_data['INFER_PARTY'].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:07:04.967269Z",
     "start_time": "2023-10-10T03:07:04.952542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRFL_BLM_SUPPORT_Y', 'PRFL_GUN_CONTROL_Y', 'PRFL_2NDAMEND_Y', 'RD_SCIFI_Y', 'PRFL_METOO_SUPPORT_Y', 'PRFL_AMZN_PRIME_Y', 'RELIGINSP_Y', 'CH_1617FEM_Y', 'DON_POLLIB_Y', 'CH_1617MAL_Y']\n",
      "+++++\n",
      "['PRFL_BLM_SUPPORT_Y', 'PRFL_GUN_CONTROL_Y', 'MAILORDRSP_Y', 'RD_RELIG_Y', 'COL_STAMP_Y', 'PRFL_METOO_SUPPORT_Y', 'PRFL_2NDAMEND_Y', 'MOTORCYCLE_Y', 'CH_1617FEM_Y', 'DON_POLLIB_Y']\n",
      "+++++\n"
     ]
    }
   ],
   "source": [
    "for run in result['Top_N_Features']:\n",
    "    print(run)\n",
    "    print(\"+++++\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:07:05.600508Z",
     "start_time": "2023-10-10T03:07:05.597132Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Unique Features: ['RD_SCIFI_Y', 'PRFL_AMZN_PRIME_Y', 'RELIGINSP_Y', 'CH_1617MAL_Y', 'MAILORDRSP_Y', 'RD_RELIG_Y', 'COL_STAMP_Y', 'MOTORCYCLE_Y']\n",
      "Most Common Features: ['PRFL_BLM_SUPPORT_Y', 'PRFL_GUN_CONTROL_Y', 'PRFL_2NDAMEND_Y', 'PRFL_METOO_SUPPORT_Y', 'CH_1617FEM_Y', 'DON_POLLIB_Y', 'RD_SCIFI_Y', 'PRFL_AMZN_PRIME_Y', 'RELIGINSP_Y', 'CH_1617MAL_Y', 'MAILORDRSP_Y', 'RD_RELIG_Y', 'COL_STAMP_Y', 'MOTORCYCLE_Y']\n"
     ]
    }
   ],
   "source": [
    "# Assuming result is your DataFrame and 'Top_N_Features' is the column with the lists of top features\n",
    "all_features = [feature for sublist in result['Top_N_Features'].tolist() for feature in sublist]\n",
    "\n",
    "# Count the frequency of each feature\n",
    "feature_counts = Counter(all_features)\n",
    "\n",
    "# Find the most unique features (those that appear only once across all runs)\n",
    "most_unique_features = [feature for feature, count in feature_counts.items() if count == 1]\n",
    "\n",
    "# Find the most common features (those that appear the most across all runs)\n",
    "most_common_features = [feature for feature, count in feature_counts.most_common()]\n",
    "\n",
    "print(\"Most Unique Features:\", most_unique_features)\n",
    "print(\"Most Common Features:\", most_common_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:07:13.099267Z",
     "start_time": "2023-10-10T03:07:13.076529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Just the non_numeric_cols with more than 2 values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(500000, 49)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced_with_more_than_two_categories = data_500[cols_with_more_than_two_categories].copy()\n",
    "data_reduced_with_more_than_two_categories.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:07:18.648084Z",
     "start_time": "2023-10-10T03:07:18.349191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after label encoding: Index(['ADD_TYPE', 'AI_COUNTY_NAME', 'AIRCOND', 'ASSMLCODE', 'BUS_OWNER',\n",
      "       'CENSUS_ST', 'CNS_MEDINC', 'CONG_DIST', 'COUNTY_ST', 'COUNTY_TYPE',\n",
      "       'CRD_RANGE', 'CREDRATE', 'EDUCATION', 'ETHNIC_INFER', 'ETHNICCODE',\n",
      "       'ETHNICCONF', 'ETHNICGRP', 'FUND_POLIT', 'GENDER_MIX', 'GENERATION',\n",
      "       'HH_NUMGEN', 'HH_SIZE', 'HOMEMKTVAL', 'HOMEOWNER', 'HOMEOWNRNT',\n",
      "       'INCOMESTHH', 'LANGUAGE', 'LENGTH_RES', 'LIFESTAGE_CLUSTER', 'NETWORTH',\n",
      "       'NUMCHILD', 'OCCDETAIL', 'OCCUPATION', 'PARTY_CODE', 'PERSONS_HH',\n",
      "       'POOL', 'PRFL_TEAPARTY', 'RELIGION', 'SEX', 'ST_LO_HOUS', 'ST_UP_HOUS',\n",
      "       'STATE', 'STATUS', 'TOD_PRES_DIFF_2016', 'TOD_PRES_DIFF_2016_PREC',\n",
      "       'TOD_PRES_DIFF_2020_PREC', 'VOTER_CNT', 'VOTER_TRLR', 'YEARBUILT'],\n",
      "      dtype='object')\n",
      "XGBoost Model Accuracy: 0.5749\n"
     ]
    }
   ],
   "source": [
    "# Assuming cols_with_Y_or_Unknown has been identified and data_500 has been cleaned\n",
    "data_reduced_with_more_than_two_categories = data_500[cols_with_more_than_two_categories].copy()\n",
    "\n",
    "# Adding the target column to this data\n",
    "data_reduced_with_more_than_two_categories['PARTY_CODE'] = data_500['PARTY_CODE']\n",
    "\n",
    "# Remove rows where 'PARTY_CODE' is missing, as it's our target variable\n",
    "data_reduced_with_more_than_two_categories = data_reduced_with_more_than_two_categories[data_reduced_with_more_than_two_categories['PARTY_CODE'].notna()]\n",
    "\n",
    "# Sample 100,000 rows from the data\n",
    "data_sample = data_reduced_with_more_than_two_categories.sample(n=100000, random_state=42)\n",
    "\n",
    "# Count instances of each class in 'PARTY_CODE' again to filter out classes with fewer than 2 instances\n",
    "class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "\n",
    "# Label-encode 'PARTY_CODE' column\n",
    "le = LabelEncoder()\n",
    "data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "print(\"Columns after label encoding:\", data_sample.columns)\n",
    "\n",
    "\n",
    "# Identify the columns in `cols_with_more_than_two_categories` that are actually present in `data_sample`\n",
    "# Identify the columns in `cols_with_more_than_two_categories` that are actually present in `data_sample`\n",
    "cols_to_encode = [col for col in cols_with_more_than_two_categories if col in data_sample.columns and col != 'PARTY_CODE']\n",
    "\n",
    "# One-Hot Encoding\n",
    "data_one_hot = pd.get_dummies(data_sample, columns=cols_to_encode, drop_first=True)\n",
    "#print(\"Columns after one-hot encoding:\", data_one_hot.columns)\n",
    "#print(\"Columns before dropping PARTY_CODE:\", data_one_hot.columns)\n",
    "\n",
    "# Splitting the Data into Training and Test Sets\n",
    "X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "y = data_one_hot['PARTY_CODE']\n",
    "\n",
    "# Perform train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Sanitize column names to remove characters not allowed by XGBoost\n",
    "X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "\n",
    "# Initialize and Train XGBoost Classifier\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions and Evaluate the Model\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Model Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:11:58.026078Z",
     "start_time": "2023-10-10T03:07:19.877388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:11:58.028782Z",
     "start_time": "2023-10-10T03:11:58.025671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:11:58.031608Z",
     "start_time": "2023-10-10T03:11:58.028700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 1...\n",
      "Starting run 2...\n",
      "Elapsed time: 384.54 seconds\n"
     ]
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_xgboost(data, cols_with_more_than_two_categories, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting run {run}...\")\n",
    "        # Replicate the original code inside the loop\n",
    "        data_reduced_with_more_than_two_categories = data[cols_with_more_than_two_categories].copy()\n",
    "        data_reduced_with_more_than_two_categories['PARTY_CODE'] = data['PARTY_CODE']\n",
    "        data_reduced_with_more_than_two_categories = data_reduced_with_more_than_two_categories[data_reduced_with_more_than_two_categories['PARTY_CODE'].notna()]\n",
    "        data_sample = data_reduced_with_more_than_two_categories.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "        data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "        \n",
    "        cols_to_encode = [col for col in cols_with_more_than_two_categories if col in data_sample.columns and col != 'PARTY_CODE']\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_to_encode, drop_first=True)\n",
    "        \n",
    "        X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "        y = data_one_hot['PARTY_CODE']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        \n",
    "        xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 and cols_with_more_than_two_categories are already defined\n",
    "result = run_xgboost(data_500, cols_with_more_than_two_categories, sample_size=50000, num_runs=2, top_N_features=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T03:18:22.582257Z",
     "start_time": "2023-10-10T03:11:58.038239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#result['Top_N_Features'][0]\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-10T03:01:11.095302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Unique Features: []\n",
      "Most Common Features: ['CENSUS_ST_48', 'PRFL_POLITICAL_IDEOLOGY_L', 'CENSUS_ST_13', 'PRFL_POLITICAL_IDEOLOGY_Unknown', 'TOD_PRES_DIFF_2020_PREC_Unknown']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming result is your DataFrame and 'Top_N_Features' is the column with the lists of top features\n",
    "all_features = [feature for sublist in result['Top_N_Features'].tolist() for feature in sublist]\n",
    "\n",
    "# Count the frequency of each feature\n",
    "feature_counts = Counter(all_features)\n",
    "\n",
    "# Find the most unique features (those that appear only once across all runs)\n",
    "most_unique_features = [feature for feature, count in feature_counts.items() if count == 1]\n",
    "\n",
    "# Find the most common features (those that appear the most across all runs)\n",
    "most_common_features = [feature for feature, count in feature_counts.most_common()]\n",
    "\n",
    "print(\"Most Unique Features:\", most_unique_features)\n",
    "print(\"Most Common Features:\", most_common_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T17:16:37.965906Z",
     "start_time": "2023-10-07T17:16:37.959075Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### modeling for 'PRFL_POLITICAL_IDEOLOGY'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 1...\n",
      "Starting run 2...\n",
      "Elapsed time: 79.81 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Run  Accuracy  F1_Score  Recall  \\\n0    1    0.9364  0.932831  0.9364   \n1    2    0.9364  0.932831  0.9364   \n\n                                      Top_N_Features  \n0  [PARTY_CODE_S, PARTY_CODE_U, PARTY_CODE_N, PAR...  \n1  [PARTY_CODE_S, PARTY_CODE_U, PARTY_CODE_N, PAR...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n      <th>Recall</th>\n      <th>Top_N_Features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.9364</td>\n      <td>0.932831</td>\n      <td>0.9364</td>\n      <td>[PARTY_CODE_S, PARTY_CODE_U, PARTY_CODE_N, PAR...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.9364</td>\n      <td>0.932831</td>\n      <td>0.9364</td>\n      <td>[PARTY_CODE_S, PARTY_CODE_U, PARTY_CODE_N, PAR...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@timer_decorator\n",
    "def run_xgboost(data, cols_with_more_than_two_categories, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "    \n",
    "    # Remove duplicates from cols_with_more_than_two_categories\n",
    "    cols_with_more_than_two_categories = list(set(cols_with_more_than_two_categories))\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting run {run}...\")\n",
    "        \n",
    "        data_reduced_with_more_than_two_categories = data[cols_with_more_than_two_categories].copy()\n",
    "        \n",
    "        # Change target variable to 'PRFL_POLITICAL_IDEOLOGY'\n",
    "        data_reduced_with_more_than_two_categories['PRFL_POLITICAL_IDEOLOGY'] = data['PRFL_POLITICAL_IDEOLOGY']\n",
    "        \n",
    "        # Filter out rows where 'PRFL_POLITICAL_IDEOLOGY' is missing\n",
    "        data_reduced_with_more_than_two_categories = data_reduced_with_more_than_two_categories[data_reduced_with_more_than_two_categories['PRFL_POLITICAL_IDEOLOGY'].notna()]\n",
    "\n",
    "        data_sample = data_reduced_with_more_than_two_categories.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        class_counts = data_sample['PRFL_POLITICAL_IDEOLOGY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "        data_sample = data_sample[data_sample['PRFL_POLITICAL_IDEOLOGY'].isin(valid_classes)]\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        data_sample['PRFL_POLITICAL_IDEOLOGY'] = le.fit_transform(data_sample['PRFL_POLITICAL_IDEOLOGY'].astype(str))\n",
    "        \n",
    "        cols_to_encode = [col for col in cols_with_more_than_two_categories if col in data_sample.columns and col != 'PRFL_POLITICAL_IDEOLOGY']\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_to_encode, drop_first=True)\n",
    "        \n",
    "        # Check for duplicate columns before training\n",
    "        duplicate_columns = data_one_hot.columns[data_one_hot.columns.duplicated()]\n",
    "        if len(duplicate_columns) > 0:\n",
    "            print(f\"Warning: Duplicate columns found: {duplicate_columns}\")\n",
    "\n",
    "        # Check for invalid data types\n",
    "        invalid_dtypes = data_one_hot.select_dtypes(exclude=['int', 'float', 'bool', 'category']).columns\n",
    "        if len(invalid_dtypes) > 0:\n",
    "            print(f\"Warning: Invalid data types found: {invalid_dtypes}\")\n",
    "\n",
    "        # Drop 'PRFL_POLITICAL_IDEOLOGY' to prepare data for training\n",
    "        X = data_one_hot.drop('PRFL_POLITICAL_IDEOLOGY', axis=1)\n",
    "        y = data_one_hot['PRFL_POLITICAL_IDEOLOGY']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        \n",
    "        xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 and cols_with_more_than_two_categories are already defined\n",
    "# Include 'PARTY_CODE' in the cols_with_more_than_two_categories list\n",
    "if 'PARTY_CODE' not in cols_with_more_than_two_categories:\n",
    "    cols_with_more_than_two_categories.append('PARTY_CODE')\n",
    "\n",
    "result = run_xgboost(data_500, cols_with_more_than_two_categories, sample_size=5000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T20:46:30.538504Z",
     "start_time": "2023-10-08T20:45:10.710852Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training on all features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@timer_decorator\n",
    "def run_xgboost(data, cols_with_more_than_two_categories, sample_size=100000, num_runs=1, top_N_features=10):\n",
    "    results_list = []\n",
    "    \n",
    "    # Remove duplicates from cols_with_more_than_two_categories\n",
    "    cols_with_more_than_two_categories = list(set(cols_with_more_than_two_categories))\n",
    "\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Starting run {run}...\")\n",
    "        \n",
    "        data_reduced_with_more_than_two_categories = data[cols_with_more_than_two_categories].copy()\n",
    "        \n",
    "        # Change target variable to 'PRFL_POLITICAL_IDEOLOGY'\n",
    "        data_reduced_with_more_than_two_categories['PRFL_POLITICAL_IDEOLOGY'] = data['PRFL_POLITICAL_IDEOLOGY']\n",
    "        \n",
    "        # Filter out rows where 'PRFL_POLITICAL_IDEOLOGY' is missing\n",
    "        data_reduced_with_more_than_two_categories = data_reduced_with_more_than_two_categories[data_reduced_with_more_than_two_categories['PRFL_POLITICAL_IDEOLOGY'].notna()]\n",
    "\n",
    "        data_sample = data_reduced_with_more_than_two_categories.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        class_counts = data_sample['PRFL_POLITICAL_IDEOLOGY'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "        data_sample = data_sample[data_sample['PRFL_POLITICAL_IDEOLOGY'].isin(valid_classes)]\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        data_sample['PRFL_POLITICAL_IDEOLOGY'] = le.fit_transform(data_sample['PRFL_POLITICAL_IDEOLOGY'].astype(str))\n",
    "        \n",
    "        cols_to_encode = [col for col in cols_with_more_than_two_categories if col in data_sample.columns and col != 'PRFL_POLITICAL_IDEOLOGY']\n",
    "        data_one_hot = pd.get_dummies(data_sample, columns=cols_to_encode, drop_first=True)\n",
    "        \n",
    "        # Check for duplicate columns before training\n",
    "        duplicate_columns = data_one_hot.columns[data_one_hot.columns.duplicated()]\n",
    "        if len(duplicate_columns) > 0:\n",
    "            print(f\"Warning: Duplicate columns found: {duplicate_columns}\")\n",
    "\n",
    "        # Check for invalid data types\n",
    "        invalid_dtypes = data_one_hot.select_dtypes(exclude=['int', 'float', 'bool', 'category']).columns\n",
    "        if len(invalid_dtypes) > 0:\n",
    "            print(f\"Warning: Invalid data types found: {invalid_dtypes}\")\n",
    "\n",
    "        # Drop 'PRFL_POLITICAL_IDEOLOGY' to prepare data for training\n",
    "        X = data_one_hot.drop('PRFL_POLITICAL_IDEOLOGY', axis=1)\n",
    "        y = data_one_hot['PRFL_POLITICAL_IDEOLOGY']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "        \n",
    "        xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        feature_importances = xgb.feature_importances_\n",
    "        sorted_idx = feature_importances.argsort()[::-1][:top_N_features]\n",
    "        top_features = X.columns[sorted_idx].tolist()\n",
    "\n",
    "        results_list.append({\n",
    "            'Run': run,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1,\n",
    "            'Recall': recall,\n",
    "            'Top_N_Features': top_features\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Assuming data_500 and cols_with_more_than_two_categories are already defined\n",
    "# Include 'PARTY_CODE' in the cols_with_more_than_two_categories list\n",
    "if 'PARTY_CODE' not in cols_with_more_than_two_categories:\n",
    "    cols_with_more_than_two_categories.append('PARTY_CODE')\n",
    "\n",
    "result = run_xgboost(data_500, cols_with_more_than_two_categories, sample_size=50000, num_runs=2, top_N_features=10)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Columns - Dealing with Imbalanced Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 'PARTY_CODE' as target variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['INFER_PARTY'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 42\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X_train, X_test, y_train, y_test\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Assuming engineered_data is already defined\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengineered_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[27], line 29\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[0;34m(engineered_data, sample_size)\u001B[0m\n\u001B[1;32m     25\u001B[0m data_one_hot \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(data_sample, columns\u001B[38;5;241m=\u001B[39mnon_numeric_cols, drop_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Split into features and labels\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m#X = data_one_hot.drop('PARTY_CODE', axis=1)\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mdata_one_hot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPARTY_CODE\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mINFER_PARTY\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m y \u001B[38;5;241m=\u001B[39m data_one_hot[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPARTY_CODE\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Handle special characters in column names that XGBoost doesn't like\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/frame.py:5258\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5111\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5112\u001B[0m     labels: IndexLabel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5119\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5120\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5122\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[1;32m   5123\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5256\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[1;32m   5257\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 5258\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5260\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5264\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5265\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5266\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py:4549\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4547\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   4548\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4549\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   4552\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/generic.py:4591\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4589\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m   4590\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4591\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4592\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[1;32m   4594\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[1;32m   4595\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/indexes/base.py:6699\u001B[0m, in \u001B[0;36mIndex.drop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   6697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m   6698\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 6699\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(labels[mask])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6700\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[1;32m   6701\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['INFER_PARTY'] not found in axis\""
     ]
    }
   ],
   "source": [
    "def preprocess_data(engineered_data, sample_size=10000):\n",
    "    # Copy the data\n",
    "    data_with_all_columns = engineered_data.copy()\n",
    "    \n",
    "    # Remove rows where the target column 'PARTY_CODE' is NaN\n",
    "    data_with_all_columns = data_with_all_columns[data_with_all_columns['PARTY_CODE'].notna()]\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data_with_all_columns.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Keep only the classes that have at least 2 samples\n",
    "    class_counts = data_sample['PARTY_CODE'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "    data_sample = data_sample[data_sample['PARTY_CODE'].isin(valid_classes)]\n",
    "    \n",
    "    # Label encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    data_sample['PARTY_CODE'] = le.fit_transform(data_sample['PARTY_CODE'].astype(str))\n",
    "    \n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns.difference(['PARTY_CODE'])\n",
    "    non_numeric_cols = data_sample.select_dtypes(exclude=['int64', 'float64']).columns.difference(['PARTY_CODE'])\n",
    "    \n",
    "    # One-hot encode non-numeric columns\n",
    "    data_one_hot = pd.get_dummies(data_sample, columns=non_numeric_cols, drop_first=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    #X = data_one_hot.drop('PARTY_CODE', axis=1)\n",
    "    X = data_one_hot.drop(columns=['PARTY_CODE', 'INFER_PARTY'])\n",
    "    y = data_one_hot['PARTY_CODE']\n",
    "    \n",
    "    # Handle special characters in column names that XGBoost doesn't like\n",
    "    X.columns = X.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Usage\n",
    "# Assuming engineered_data is already defined\n",
    "X_train, X_test, y_train, y_test = preprocess_data(engineered_data, sample_size=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:22:59.292030Z",
     "start_time": "2023-10-13T06:22:55.723574Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 'INFER_PARTY' as target variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(engineered_data, sample_size=10000):\n",
    "    # Copy the data\n",
    "    data_with_all_columns = engineered_data.copy()\n",
    "    \n",
    "    # Remove rows where the target column 'INFER_PARTY' is NaN\n",
    "    data_with_all_columns = data_with_all_columns[data_with_all_columns['INFER_PARTY'].notna()]\n",
    "    \n",
    "    # Sample the data\n",
    "    data_sample = data_with_all_columns.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Keep only the classes that have at least 2 samples\n",
    "    class_counts = data_sample['INFER_PARTY'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index.tolist()\n",
    "    data_sample = data_sample[data_sample['INFER_PARTY'].isin(valid_classes)]\n",
    "    \n",
    "    # Label encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    data_sample['INFER_PARTY'] = le.fit_transform(data_sample['INFER_PARTY'].astype(str))\n",
    "    \n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_cols = data_sample.select_dtypes(include=['int64', 'float64']).columns.difference(['INFER_PARTY'])\n",
    "    non_numeric_cols = data_sample.select_dtypes(exclude=['int64', 'float64']).columns.difference(['INFER_PARTY'])\n",
    "    \n",
    "    # One-hot encode non-numeric columns\n",
    "    data_one_hot = pd.get_dummies(data_sample, columns=non_numeric_cols, drop_first=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = data_one_hot.drop('INFER_PARTY', axis=1)\n",
    "    y = data_one_hot['INFER_PARTY']\n",
    "    \n",
    "    # Handle special characters in column names that XGBoost doesn't like\n",
    "    X.columns = X.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Usage\n",
    "# Assuming engineered_data is already defined\n",
    "X_train, X_test, y_train, y_test = preprocess_data(engineered_data, sample_size=10000)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resampling Techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.925\n",
      "Recall: 0.925\n",
      "F1 Score: 0.9128656777493607\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Determine the smallest class count\n",
    "min_class_count = np.min(np.bincount(y_train))\n",
    "\n",
    "# Apply SMOTE to the imputed training data\n",
    "# Set n_neighbors to min_class_count - 1 or a default small number\n",
    "n_neighbors = min(min_class_count - 1, 5)\n",
    "smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "X_res, y_res = smote.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "# Make predictions on the imputed test set\n",
    "y_pred = xgb.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:02:38.164714Z",
     "start_time": "2023-10-13T05:00:22.327485Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.413\n",
      "Recall: 0.413\n",
      "F1 Score: 0.484446515639784\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Apply RandomUnderSampler to the imputed training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "# Make predictions on the imputed test set\n",
    "y_pred = xgb.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:13:47.516746Z",
     "start_time": "2023-10-13T05:13:44.438174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Weighted Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9145\n",
      "Recall: 0.9145\n",
      "F1 Score: 0.9094697333354964\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each class in the target variable\n",
    "counter = Counter(y_train)\n",
    "\n",
    "# Calculate the number of samples\n",
    "total_samples = len(y_train)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = {cls: float(total_samples / count) for cls, count in counter.items()}\n",
    "\n",
    "# Map the weights to each sample in y_train\n",
    "sample_weights = [class_weights[cls] for cls in y_train]\n",
    "\n",
    "# Initialize XGBoost with multi:softmax objective\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Fit the model, passing in the sample weights\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:17:53.915650Z",
     "start_time": "2023-10-13T05:17:32.911041Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cost-sensitive Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': {'R': 2.0795453623443604e-05, 'N': 2.2389200977768253e-05, 'O': 0.00013219838820189577, 'L': 0.0006259639897705816, '  ': 0.02223128306578676}, 'R': {'D': 2.0795453623443604e-05, 'N': 2.5062967472140136e-05, 'O': 0.00013487215469626767, 'L': 0.0006286377562649534, '  ': 0.022233956832281132}, 'N': {'D': 2.2389200977768253e-05, 'R': 2.5062967472140136e-05, 'O': 0.00013646590205059232, 'L': 0.0006302315036192781, '  ': 0.022235550579635454}, 'O': {'D': 0.00013219838820189577, 'R': 0.00013487215469626767, 'N': 0.00013646590205059232, 'L': 0.0007400406908434056, '  ': 0.02234535976685958}, 'L': {'D': 0.0006259639897705816, 'R': 0.0006286377562649534, 'N': 0.0006302315036192781, 'O': 0.0007400406908434056, '  ': 0.02283912536842827}, '  ': {'D': 0.02223128306578676, 'R': 0.022233956832281132, 'N': 0.022235550579635454, 'O': 0.02234535976685958, 'L': 0.02283912536842827}}\n"
     ]
    }
   ],
   "source": [
    "def create_cost_matrix(y):\n",
    "    # Count the frequency of each class in the target variable\n",
    "    class_freq = Counter(y)\n",
    "    \n",
    "    # Calculate the inverse frequency\n",
    "    inv_freq = {k: 1.0 / v for k, v in class_freq.items()}\n",
    "    \n",
    "    # Create the cost matrix\n",
    "    cost_matrix = {}\n",
    "    for class1 in class_freq.keys():\n",
    "        cost_matrix[class1] = {}\n",
    "        for class2 in class_freq.keys():\n",
    "            if class1 == class2:\n",
    "                continue\n",
    "            cost_matrix[class1][class2] = inv_freq[class1] + inv_freq[class2]\n",
    "            \n",
    "    return cost_matrix\n",
    "\n",
    "# Assuming y_train contains your training labels, and it's a pandas Series\n",
    "# For demonstration, I'm using the value_counts information you provided\n",
    "y_train_demo = ['D'] * 110365 + ['R'] * 85218 + ['N'] * 75028 + ['O'] * 8121 + ['L'] * 1621 + ['  '] * 45\n",
    "cost_matrix = create_cost_matrix(y_train_demo)\n",
    "\n",
    "print(cost_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:29:52.378410Z",
     "start_time": "2023-10-13T05:29:52.348178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m y_train_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(y_train) \n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Compute sample weights\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_custom_sample_weight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcost_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_np\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[1;32m     20\u001B[0m xgb \u001B[38;5;241m=\u001B[39m XGBClassifier(objective\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmulti:softmax\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "Cell \u001B[0;32mIn[28], line 5\u001B[0m, in \u001B[0;36mcompute_custom_sample_weight\u001B[0;34m(cost_matrix, y)\u001B[0m\n\u001B[1;32m      2\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(y))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, class1 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(y):\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m class2, cost \u001B[38;5;129;01min\u001B[39;00m \u001B[43mcost_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclass1\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      6\u001B[0m         sample_weight[i] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (y \u001B[38;5;241m==\u001B[39m class2)\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m*\u001B[39m cost\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sample_weight\n",
      "\u001B[0;31mKeyError\u001B[0m: 5"
     ]
    }
   ],
   "source": [
    "def compute_custom_sample_weight(cost_matrix, y):\n",
    "    sample_weight = np.zeros(len(y))\n",
    "    \n",
    "    for i, class1 in enumerate(y):\n",
    "        for class2, cost in cost_matrix[class1].items():\n",
    "            sample_weight[i] += (y == class2).sum() * cost\n",
    "            \n",
    "    return sample_weight\n",
    "\n",
    "# Assuming y_train is your actual training labels and it's a NumPy array\n",
    "# For demonstration, converting y_train_demo to a NumPy array\n",
    "#y_train_demo_np = np.array(y_train_demo)\n",
    "# This should be your actual y_train, converted to a NumPy array\n",
    "y_train_np = np.array(y_train) \n",
    "\n",
    "# Compute sample weights\n",
    "sample_weight = compute_custom_sample_weight(cost_matrix, y_train_np)\n",
    "\n",
    "# Fit the model\n",
    "xgb = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Rest of the code for predictions and metrics...\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:25:11.465458Z",
     "start_time": "2023-10-13T06:25:11.448216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTY_CODE\n",
      "D    110365\n",
      "R     85218\n",
      "N     75028\n",
      "O      8121\n",
      "L      1621\n",
      "         45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(engineered_data['PARTY_CODE'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-13T05:27:05.802624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
